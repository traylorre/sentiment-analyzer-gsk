name: "Deploy Pipeline"

# Consolidated deployment pipeline using job dependencies (needs:)
# instead of multiple workflow_run triggers.
#
# ARCHITECTURAL NOTE:
# Simplified 2-environment flow (preprod + prod). Dev environment removed to reduce
# complexity and cost. Mocked tests provide fast feedback, preprod validates real AWS.
#
# Flow:
#   push to main ‚Üí build ‚Üí test (mocked) ‚Üí deploy-preprod ‚Üí test-preprod ‚Üí deploy-prod ‚Üí canary
#
# Benefits:
# - No workflow_run nesting limits
# - Shared artifacts between jobs
# - Single workflow run view
# - Automatic progression through stages
# - Environment-based approval gates
# - Lower AWS costs (no dev environment)
# - Clearer separation: mocked tests vs real AWS tests

on:
  push:
    branches:
      - main
    # REMOVED paths-ignore - it was blocking critical deploys!
    # Problem: When PR #94 changed deploy.yml + tests/conftest.py, it should have
    # triggered a deploy, but the workflow was skipped. This left Lambda running
    # old code with pydantic binary incompatibility (HTTP 502).
    #
    # Better approach: Deploy on ALL pushes to main. Terraform and the build job
    # are idempotent - if nothing changed, they complete quickly. The cost of
    # running an extra 5-minute workflow is far less than the cost of missing
    # a critical fix.
    #
    # If build times become a problem, consider:
    # - Using paths filter on the build job ONLY (not the whole workflow)
    # - Caching more aggressively
    # - Using Turborepo-style content hashing

  # Manual trigger for testing/rollbacks
  workflow_dispatch:

permissions:
  contents: read
  actions: write
  id-token: write

# Prevent concurrent deployments
concurrency:
  group: deploy-pipeline
  cancel-in-progress: false

jobs:
  # ========================================================================
  # JOB 1: Build Lambda Packages
  # ========================================================================
  build:
    name: Build Lambda Packages
    runs-on: ubuntu-latest

    outputs:
      artifact-sha: ${{ steps.package.outputs.sha }}
      artifact-name: ${{ steps.package.outputs.name }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.13
        uses: actions/setup-python@v6
        with:
          python-version: '3.13'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Free up disk space
        run: |
          echo "üßπ Freeing up disk space on GitHub runner..."
          df -h

          # Remove user-owned caches only (NO SUDO - security requirement)
          # Build needs ~600MB, runner has ~2GB available
          # KEEP ~/.cache/pip for faster subsequent builds (saves ~30-50% on pip install)
          rm -rf ~/.npm
          docker system prune -af --volumes

          echo "‚úÖ Disk space after cleanup:"
          df -h

      - name: Package Lambda Functions
        id: package
        run: |
          SHA="${GITHUB_SHA:0:7}"
          echo "sha=${SHA}" >> $GITHUB_OUTPUT
          echo "name=lambda-packages-${SHA}" >> $GITHUB_OUTPUT

          echo "üì¶ Building Lambda packages for commit: ${SHA}"
          mkdir -p packages

          # Validation function to test package imports
          validate_lambda_package() {
            local package_name=$1
            local handler_file=$2

            echo "üîç Validating ${package_name} package structure..."
            cd packages/${package_name}-build

            # Verify handler file exists and has no syntax errors
            if [ ! -f "${handler_file}" ]; then
              echo "‚ùå VALIDATION FAILED: ${handler_file} not found in ${package_name} package"
              exit 1
            fi

            # Check for basic Python syntax errors
            if ! python3 -m py_compile "${handler_file}" 2>/dev/null; then
              echo "‚ùå VALIDATION FAILED: Syntax error in ${handler_file}"
              exit 1
            fi

            echo "‚úÖ ${package_name} package structure validated"
            cd ../..
          }

          # ============================================================
          # Ingestion Lambda - Lightweight dependencies only
          # ============================================================
          echo ""
          echo "üì¶ Packaging Ingestion Lambda (boto3, requests, pydantic, logging)..."

          # Install only ingestion dependencies
          # Use platform-specific flags for Lambda Python 3.13 compatibility
          # CRITICAL: pydantic-core has Rust binaries that MUST match Lambda runtime
          pip install \
            boto3==1.41.0 \
            requests==2.32.5 \
            httpx==0.28.1 \
            pydantic==2.12.4 \
            python-json-logger==4.0.0 \
            aws-xray-sdk==2.14.0 \
            -t packages/ingestion-deps/ \
            --platform manylinux2014_x86_64 \
            --implementation cp \
            --python-version 313 \
            --only-binary=:all: \
            --no-cache-dir \
            --disable-pip-version-check \
            --quiet

          # Build ingestion package
          # NOTE: handler.py must be at ROOT for Lambda handler config "handler.lambda_handler"
          # The src/ directory structure is needed for internal imports (from src.lambdas.ingestion...)
          mkdir -p packages/ingestion-build/src/lambdas/ingestion packages/ingestion-build/src/lib
          cp -r packages/ingestion-deps/* packages/ingestion-build/
          cp -r src/lambdas/ingestion/* packages/ingestion-build/          # handler.py at ROOT
          cp -r src/lambdas/ingestion/* packages/ingestion-build/src/lambdas/ingestion/  # also in src/ for imports
          cp -r src/lambdas/shared packages/ingestion-build/src/lambdas/
          cp -r src/lib/* packages/ingestion-build/src/lib/
          cd packages/ingestion-build
          zip -r ../ingestion-${SHA}.zip . \
            -x "*.pyc" "__pycache__/*" "*.pytest_cache/*" "tests/*" -q
          cd ../..

          # Validate package structure before zipping
          validate_lambda_package "ingestion" "handler.py"

          # Show size and cleanup
          INGESTION_SIZE=$(du -h packages/ingestion-${SHA}.zip | cut -f1)
          echo "‚úÖ Ingestion Lambda: ${INGESTION_SIZE}"
          rm -rf packages/ingestion-deps packages/ingestion-build

          # ============================================================
          # Dashboard Lambda - Now uses container deployment (Feature 1036)
          # ============================================================
          echo ""
          echo "‚ÑπÔ∏è Dashboard Lambda uses container deployment - see build-dashboard-image-preprod job"
          echo "   Container deployment fixes pydantic binary incompatibility that caused HTTP 502 errors"

          # ============================================================
          # Analysis Lambda - Minimal dependencies (model loads from S3)
          # ============================================================
          echo ""
          echo "üì¶ Packaging Analysis Lambda (minimal - model in S3)..."

          # Install minimal analysis dependencies (NO torch/transformers)
          # NOTE: Analysis Lambda will use container images (see PR #58)
          # This ZIP packaging remains temporarily for backwards compatibility
          # CRITICAL: pydantic-core has Rust binaries that MUST match Lambda runtime
          pip install \
            boto3==1.41.0 \
            pydantic==2.12.4 \
            python-json-logger==4.0.0 \
            aws-xray-sdk==2.14.0 \
            -t packages/analysis-deps/ \
            --platform manylinux2014_x86_64 \
            --implementation cp \
            --python-version 313 \
            --only-binary=:all: \
            --no-cache-dir \
            --disable-pip-version-check \
            --quiet

          # Build analysis package
          # NOTE: handler.py must be at ROOT for Lambda handler config "handler.lambda_handler"
          # The src/ directory structure is needed for internal imports (from src.lambdas.analysis...)
          mkdir -p packages/analysis-build/src/lambdas/analysis packages/analysis-build/src/lib
          cp -r packages/analysis-deps/* packages/analysis-build/
          cp -r src/lambdas/analysis/* packages/analysis-build/          # handler.py at ROOT
          cp -r src/lambdas/analysis/* packages/analysis-build/src/lambdas/analysis/  # also in src/ for imports
          cp -r src/lambdas/shared packages/analysis-build/src/lambdas/
          cp -r src/lib/* packages/analysis-build/src/lib/
          cd packages/analysis-build
          zip -r ../analysis-${SHA}.zip . \
            -x "*.pyc" "__pycache__/*" "*.pytest_cache/*" "tests/*" -q
          cd ../..

          # Validate package structure before cleanup
          validate_lambda_package "analysis" "handler.py"

          # Show size and cleanup
          ANALYSIS_SIZE=$(du -h packages/analysis-${SHA}.zip | cut -f1)
          echo "‚úÖ Analysis Lambda: ${ANALYSIS_SIZE}"
          rm -rf packages/analysis-deps packages/analysis-build

          # ============================================================
          # Metrics Lambda - Minimal dependencies (TD-011)
          # ============================================================
          echo ""
          echo "üì¶ Packaging Metrics Lambda (minimal - boto3 only)..."

          # Install minimal metrics dependencies
          # Note: Metrics Lambda doesn't use pydantic, but we include platform flags
          # for consistency and future-proofing if dependencies are added
          pip install \
            boto3==1.41.0 \
            python-json-logger==4.0.0 \
            -t packages/metrics-deps/ \
            --platform manylinux2014_x86_64 \
            --implementation cp \
            --python-version 313 \
            --only-binary=:all: \
            --no-cache-dir \
            --disable-pip-version-check \
            --quiet

          # Build metrics package
          # NOTE: handler.py must be at ROOT for Lambda handler config "handler.lambda_handler"
          # The src/ directory structure is needed for internal imports (from src.lib.metrics...)
          mkdir -p packages/metrics-build/src/lambdas/metrics packages/metrics-build/src/lib
          cp -r packages/metrics-deps/* packages/metrics-build/
          cp -r src/lambdas/metrics/* packages/metrics-build/          # handler.py at ROOT
          cp -r src/lambdas/metrics/* packages/metrics-build/src/lambdas/metrics/  # also in src/ for imports
          cp -r src/lambdas/shared packages/metrics-build/src/lambdas/
          cp -r src/lib/* packages/metrics-build/src/lib/
          cd packages/metrics-build
          zip -r ../metrics-${SHA}.zip . \
            -x "*.pyc" "__pycache__/*" "*.pytest_cache/*" "tests/*" -q
          cd ../..

          # Validate package structure before cleanup
          validate_lambda_package "metrics" "handler.py"

          # Show size and cleanup
          METRICS_SIZE=$(du -h packages/metrics-${SHA}.zip | cut -f1)
          echo "‚úÖ Metrics Lambda: ${METRICS_SIZE}"
          rm -rf packages/metrics-deps packages/metrics-build

          # ============================================================
          # Notification Lambda - SendGrid email notifications (Feature 006)
          # ============================================================
          echo ""
          echo "üì¶ Packaging Notification Lambda (boto3, sendgrid, pydantic)..."

          # Install notification dependencies in two steps:
          # 1. Binary packages with platform-specific flags (pydantic-core needs correct binary)
          # 2. SendGrid separately (starkbank-ecdsa is pure Python, no binary available for py313)
          pip install \
            boto3==1.41.0 \
            pydantic==2.12.4 \
            python-json-logger==4.0.0 \
            aws-xray-sdk==2.14.0 \
            -t packages/notification-deps/ \
            --platform manylinux2014_x86_64 \
            --implementation cp \
            --python-version 313 \
            --only-binary=:all: \
            --no-cache-dir \
            --disable-pip-version-check \
            --quiet

          # Install sendgrid without binary restriction (starkbank-ecdsa is pure Python)
          pip install \
            sendgrid==6.11.0 \
            -t packages/notification-deps/ \
            --no-cache-dir \
            --disable-pip-version-check \
            --quiet

          # Build notification package
          # NOTE: handler.py must be at ROOT for Lambda handler config "handler.lambda_handler"
          # The src/ directory structure is needed for internal imports (from src.lambdas.notification...)
          mkdir -p packages/notification-build/src/lambdas/notification packages/notification-build/src/lib
          cp -r packages/notification-deps/* packages/notification-build/
          cp -r src/lambdas/notification/* packages/notification-build/          # handler.py at ROOT
          cp -r src/lambdas/notification/* packages/notification-build/src/lambdas/notification/  # also in src/ for imports
          cp -r src/lambdas/shared packages/notification-build/src/lambdas/
          cp -r src/lib/* packages/notification-build/src/lib/
          cd packages/notification-build
          zip -r ../notification-${SHA}.zip . \
            -x "*.pyc" "__pycache__/*" "*.pytest_cache/*" "tests/*" -q
          cd ../..

          # Validate package structure before cleanup
          validate_lambda_package "notification" "handler.py"

          # Show size and cleanup
          NOTIFICATION_SIZE=$(du -h packages/notification-${SHA}.zip | cut -f1)
          echo "‚úÖ Notification Lambda: ${NOTIFICATION_SIZE}"
          rm -rf packages/notification-deps packages/notification-build

          # ============================================================
          # Summary
          # ============================================================
          echo ""
          echo "üìä Package Summary:"
          echo "  Ingestion:     ${INGESTION_SIZE}"
          echo "  Dashboard:     ${DASHBOARD_SIZE}"
          echo "  Analysis:      ${ANALYSIS_SIZE}"
          echo "  Metrics:       ${METRICS_SIZE}"
          echo "  Notification:  ${NOTIFICATION_SIZE}"
          echo ""
          ls -lh packages/*.zip
          echo ""
          echo "‚úÖ All Lambda packages built successfully!"
          echo "üöÄ Build time optimized: Per-Lambda dependencies (not bundling 3.9GB!)"

      - name: Upload Lambda Packages as Artifacts
        uses: actions/upload-artifact@v6
        with:
          name: ${{ steps.package.outputs.name }}
          path: packages/
          retention-days: 90
          if-no-files-found: error

      - name: Create Build Manifest
        run: |
          SHA="${GITHUB_SHA:0:7}"
          cat > build-manifest.json <<EOF
          {
            "git_sha": "${GITHUB_SHA}",
            "git_sha_short": "${SHA}",
            "git_ref": "${GITHUB_REF}",
            "build_timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "actor": "${GITHUB_ACTOR}",
            "workflow_run_id": "${GITHUB_RUN_ID}",
            "workflow_run_number": "${GITHUB_RUN_NUMBER}"
          }
          EOF
          cat build-manifest.json

      - name: Upload Build Manifest
        uses: actions/upload-artifact@v6
        with:
          name: build-manifest-${{ steps.package.outputs.sha }}
          path: build-manifest.json
          retention-days: 90

  # ========================================================================
  # JOB 2: Unit Tests (Mocked AWS)
  # ========================================================================
  test:
    name: Unit Tests (Mocked AWS)
    needs: build
    runs-on: ubuntu-latest

    permissions:
      contents: read
      actions: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.13'
          cache: 'pip'
          cache-dependency-path: 'requirements-dev.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-dev.txt

      - name: Run Unit Tests (Mocked AWS with moto)
        id: unit-tests
        env:
          AWS_REGION: us-east-1
          ENVIRONMENT: dev
          AWS_ACCESS_KEY_ID: testing
          AWS_SECRET_ACCESS_KEY: testing  # pragma: allowlist secret - fake credential for moto mocks
          AWS_SECURITY_TOKEN: testing
          AWS_SESSION_TOKEN: testing
        run: |
          # Dev environment uses mocked AWS resources (LOCAL mirrors DEV)
          # Fake AWS credentials prevent accidental real AWS access
          set +e
          pytest tests/unit/ -v --tb=short -m "not preprod"
          TEST_EXIT_CODE=$?
          set -e

          if [ $TEST_EXIT_CODE -eq 0 ]; then
            echo "‚úÖ All unit tests passed"
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Unit tests failed with exit code $TEST_EXIT_CODE"
            echo "passed=false" >> $GITHUB_OUTPUT
          fi
          # Exit successfully to allow result check step
          exit 0

      - name: Check Unit Test Results
        if: steps.unit-tests.outputs.passed != 'true'
        run: |
          echo "::error::Unit tests failed - cannot proceed with deployment"
          exit 1

      - name: Validate Build Artifacts
        uses: actions/download-artifact@v7
        with:
          name: ${{ needs.build.outputs.artifact-name }}
          path: packages/

      - name: Check Package Sizes
        run: |
          echo "üì¶ Validating Lambda package sizes..."
          ls -lh packages/*.zip

          # Warn if packages are too large
          for pkg in packages/*.zip; do
            SIZE=$(stat -f%z "$pkg" 2>/dev/null || stat -c%s "$pkg")
            SIZE_MB=$((SIZE / 1024 / 1024))
            echo "  $(basename $pkg): ${SIZE_MB}MB"
            if [ $SIZE_MB -gt 250 ]; then
              echo "‚ö†Ô∏è  WARNING: $(basename $pkg) is ${SIZE_MB}MB (Lambda limit: 250MB unzipped)"
            fi
          done

          echo "‚úÖ Package validation complete"

  # ========================================================================
  # JOB 3: Build SSE Lambda Image (Preprod)
  # ========================================================================
  build-sse-image-preprod:
    name: Build SSE Lambda Image (Preprod)
    runs-on: ubuntu-latest
    needs: [build, test]
    environment: preprod

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Configure AWS Credentials (Preprod)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and Push SSE Lambda Image
        uses: docker/build-push-action@v6
        with:
          # Build context changed to src/ to include src/lib/timeseries
          # Required for: from src.lib.timeseries import Resolution
          context: src
          file: src/lambdas/sse_streaming/Dockerfile
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/preprod-sse-streaming-lambda:latest
            ${{ steps.login-ecr.outputs.registry }}/preprod-sse-streaming-lambda:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          provenance: false

      # Smoke test: Verify Python imports work inside the Docker image
      # Catches missing __init__.py files and import errors before deployment
      # See: fix(128) - ModuleNotFoundError for src.lambdas.shared imports
      - name: Smoke Test SSE Lambda Imports
        run: |
          echo "üß™ Running Docker import smoke test..."
          IMAGE="${{ steps.login-ecr.outputs.registry }}/preprod-sse-streaming-lambda:${{ github.sha }}"

          # Run Python import check inside the container
          # Note: PYTHONPATH is set here to match the Lambda environment variable
          # configured in Terraform. Docker ENV alone doesn't work because Lambda
          # Web Adapter subprocess doesn't inherit container environment reliably.
          # Feature 1043: USERS_TABLE for config queries, SENTIMENTS_TABLE for sentiment data
          docker run --rm -e PYTHONPATH=/app/packages:/app -e USERS_TABLE=preprod-sentiment-users -e SENTIMENTS_TABLE=preprod-sentiment-items --entrypoint python "$IMAGE" -c "
          import sys
          print('Testing imports...')

          # These imports must work for the Lambda to start
          from handler import app
          from config import config_lookup_service
          from stream import stream_generator
          from src.lambdas.shared.logging_utils import sanitize_for_log
          from src.lambdas.shared.models.configuration import Configuration
          # Timeseries models (Feature 1009) - added after ModuleNotFoundError fix
          from src.lib.timeseries import Resolution
          from timeseries_models import PartialBucketEvent

          print('‚úÖ All imports successful')
          sys.exit(0)
          "

          if [ $? -eq 0 ]; then
            echo "‚úÖ SSE Lambda import smoke test passed"
          else
            echo "‚ùå SSE Lambda import smoke test FAILED"
            echo "The Docker image has import errors that will cause Runtime.ExitError"
            exit 1
          fi

  # ========================================================================
  # JOB 3.5: Build Analysis Lambda Image (Preprod)
  # ADR-005 Phase 2: Container deployment for PyTorch + transformers
  # ========================================================================
  build-analysis-image-preprod:
    name: Build Analysis Lambda Image (Preprod)
    runs-on: ubuntu-latest
    needs: [build, test]
    environment: preprod

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Configure AWS Credentials (Preprod)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and Push Analysis Lambda Image
        uses: docker/build-push-action@v6
        with:
          context: src
          file: src/lambdas/analysis/Dockerfile
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/preprod-analysis-lambda:latest
            ${{ steps.login-ecr.outputs.registry }}/preprod-analysis-lambda:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          provenance: false

      # Smoke test: Verify Python imports work inside the Docker image
      # Catches missing dependencies and import errors before deployment
      - name: Smoke Test Analysis Lambda Imports
        run: |
          echo "üß™ Running Docker import smoke test..."
          IMAGE="${{ steps.login-ecr.outputs.registry }}/preprod-analysis-lambda:${{ github.sha }}"

          # Run Python import check inside the container
          docker run --rm --entrypoint python "$IMAGE" -c "
          import sys
          print('Testing imports...')

          # Core imports - must work for Lambda to start
          from handler import lambda_handler
          from sentiment import analyze_sentiment, load_model

          # Shared module imports
          from src.lambdas.shared.logging_utils import sanitize_for_log

          # ML library imports - the whole point of container deployment
          import torch
          import transformers
          print(f'torch version: {torch.__version__}')
          print(f'transformers version: {transformers.__version__}')

          print('‚úÖ All imports successful')
          sys.exit(0)
          "

          if [ $? -eq 0 ]; then
            echo "‚úÖ Analysis Lambda import smoke test passed"
          else
            echo "‚ùå Analysis Lambda import smoke test FAILED"
            echo "The Docker image has import errors that will cause Runtime.ExitError"
            exit 1
          fi

  # ========================================================================
  # JOB 3.6: Build Dashboard Lambda Image (Preprod)
  # Feature 1036: Container deployment to fix pydantic binary incompatibility
  # ========================================================================
  build-dashboard-image-preprod:
    name: Build Dashboard Lambda Image (Preprod)
    runs-on: ubuntu-latest
    needs: [build, test]
    environment: preprod

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Configure AWS Credentials (Preprod)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and Push Dashboard Lambda Image
        uses: docker/build-push-action@v6
        with:
          context: src
          file: src/lambdas/dashboard/Dockerfile
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/preprod-dashboard-lambda:latest
            ${{ steps.login-ecr.outputs.registry }}/preprod-dashboard-lambda:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          provenance: false

      # Smoke test: Verify Python imports work inside the Docker image
      # Catches missing dependencies and import errors before deployment
      - name: Smoke Test Dashboard Lambda Imports
        run: |
          echo "üß™ Running Docker import smoke test..."
          IMAGE="${{ steps.login-ecr.outputs.registry }}/preprod-dashboard-lambda:${{ github.sha }}"

          # Run Python import check inside the container
          # Note: Must pass env vars that modules expect at import time (chaos.py, handler.py)
          # Feature 1043: USERS_TABLE and SENTIMENTS_TABLE are required at import time
          docker run --rm -e ENVIRONMENT=preprod -e USERS_TABLE=preprod-sentiment-users -e SENTIMENTS_TABLE=preprod-sentiment-items --entrypoint python "$IMAGE" -c "
          import sys
          print('Testing imports...')

          # Core imports - must work for Lambda to start
          from handler import lambda_handler

          # FastAPI and Mangum - web framework
          from fastapi import FastAPI
          from mangum import Mangum

          # Pydantic - the whole point of container deployment
          # This import would fail with ZIP deployment due to binary incompatibility
          import pydantic
          from pydantic import BaseModel
          print(f'pydantic version: {pydantic.__version__}')

          # Shared module imports
          from src.lambdas.shared.logging_utils import sanitize_for_log

          print('‚úÖ All imports successful')
          sys.exit(0)
          "

          if [ $? -eq 0 ]; then
            echo "‚úÖ Dashboard Lambda import smoke test passed"
          else
            echo "‚ùå Dashboard Lambda import smoke test FAILED"
            echo "The Docker image has import errors that will cause Runtime.ExitError"
            exit 1
          fi

  # ========================================================================
  # JOB 4: Deploy to Preprod
  # ========================================================================
  deploy-preprod:
    name: Deploy to Preprod
    needs: [build, test, build-sse-image-preprod, build-analysis-image-preprod, build-dashboard-image-preprod]
    runs-on: ubuntu-latest
    environment:
      name: preprod
      url: ${{ steps.outputs.outputs.dashboard_url }}

    outputs:
      dashboard_url: ${{ steps.outputs.outputs.dashboard_url }}
      sns_topic_arn: ${{ steps.outputs.outputs.sns_topic_arn }}
      sse_lambda_url: ${{ steps.outputs.outputs.sse_lambda_url }}

    defaults:
      run:
        working-directory: infrastructure/terraform

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Download Lambda Packages
        uses: actions/download-artifact@v7
        with:
          name: ${{ needs.build.outputs.artifact-name }}
          path: packages/

      - name: Configure AWS Credentials (Preprod)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Upload Lambda Packages to S3 (Preprod)
        run: |
          SHA="${{ needs.build.outputs.artifact-sha }}"
          BUCKET="preprod-sentiment-lambda-deployments"

          echo "üì§ Uploading Lambda packages to S3 (preprod)..."

          aws s3 cp ../../packages/ingestion-${SHA}.zip \
            s3://${BUCKET}/ingestion/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)"

          aws s3 cp ../../packages/analysis-${SHA}.zip \
            s3://${BUCKET}/analysis/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)"

          # Dashboard Lambda now uses container deployment (Feature 1036)
          # No ZIP upload needed - see build-dashboard-image-preprod job

          aws s3 cp ../../packages/metrics-${SHA}.zip \
            s3://${BUCKET}/metrics/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)"

          aws s3 cp ../../packages/notification-${SHA}.zip \
            s3://${BUCKET}/notification/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)"

          echo "‚úÖ Lambda packages uploaded to S3"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Init (Preprod)
        run: |
          terraform init \
            -backend-config=backend-preprod.hcl \
            -backend-config="region=${{ vars.AWS_REGION }}" \
            -reconfigure

      - name: Check for Stale Terraform Locks (Preprod)
        run: |
          echo "üîç Checking for Terraform state locks..."

          # Check for S3 native lock file (.tflock)
          BUCKET="sentiment-analyzer-terraform-state-218795110243"
          LOCK_KEY="preprod/terraform.tfstate.tflock"

          LOCK_EXISTS=$(aws s3api head-object \
            --bucket "$BUCKET" \
            --key "$LOCK_KEY" \
            --region ${{ vars.AWS_REGION }} \
            2>/dev/null || echo "")

          if [ -z "$LOCK_EXISTS" ]; then
            echo "‚úÖ No lock file found - proceeding with deployment"
            exit 0
          fi

          echo "‚ö†Ô∏è  LOCK DETECTED - Manual intervention may be required"
          echo ""
          echo "This workflow uses concurrency controls (cancel-in-progress: false)"
          echo "so only ONE deployment should run at a time."
          echo ""
          echo "If you see a lock here, it likely means:"
          echo "1. A previous run was canceled/failed and left an orphaned lock"
          echo "2. Someone is running Terraform manually (check with team)"
          echo ""
          echo "Lock file details:"
          echo "  Location: s3://$BUCKET/$LOCK_KEY"

          # Get lock file metadata
          LAST_MODIFIED=$(echo "$LOCK_EXISTS" | jq -r '.LastModified // "unknown"')
          echo "  Last Modified: $LAST_MODIFIED"

          # Download and display lock contents (contains lock ID and info)
          aws s3 cp "s3://$BUCKET/$LOCK_KEY" - 2>/dev/null | head -c 500 || echo "  (Unable to read lock contents)"
          echo ""
          echo ""
          echo "üõ†Ô∏è  TO MANUALLY UNLOCK:"
          echo "If you're certain no other deployment is running, unlock with:"
          echo ""
          echo "  cd infrastructure/terraform"
          echo "  terraform force-unlock <LOCK_ID>"
          echo ""
          echo "Or delete the lock file via AWS CLI:"
          echo "  aws s3 rm s3://$BUCKET/$LOCK_KEY"
          echo ""
          echo "‚è≥ Proceeding with deployment..."
          echo "   Terraform will wait for lock release (timeout: 5 minutes)"
          echo ""

      - name: Terraform Refresh (Preprod)
        run: |
          echo "üîÑ Syncing Terraform state with AWS..."
          # Refresh ensures state matches actual AWS resources
          # This fixes drift where state has origin_read_timeout=30 but AWS has 300
          terraform refresh \
            -var-file=preprod.tfvars \
            -var="model_version=${{ needs.build.outputs.artifact-sha }}" \
            -var="lambda_package_version=${{ needs.build.outputs.artifact-sha }}" \
            -lock-timeout=5m
          echo "‚úÖ State refresh complete"

      - name: Fix CloudFront Invalid Timeout (AWS CLI)
        run: |
          echo "üîç Checking CloudFront distribution for invalid timeout values..."

          # Get the CloudFront distribution ID from Terraform outputs
          DIST_ID=$(terraform output -raw cloudfront_distribution_id 2>/dev/null || echo "")

          if [ -z "$DIST_ID" ]; then
            echo "‚ö†Ô∏è No CloudFront distribution found - skipping timeout check"
            exit 0
          fi

          echo "Distribution ID: $DIST_ID"

          # Get current distribution config
          aws cloudfront get-distribution-config --id "$DIST_ID" > /tmp/cf-config.json
          ETAG=$(jq -r '.ETag' /tmp/cf-config.json)

          # Check if any origin has invalid timeout (>180)
          INVALID_TIMEOUT=$(jq -r '.DistributionConfig.Origins.Items[]? |
            select(.CustomOriginConfig.OriginReadTimeout > 180) |
            "\(.Id): \(.CustomOriginConfig.OriginReadTimeout)s"' /tmp/cf-config.json)

          if [ -z "$INVALID_TIMEOUT" ]; then
            echo "‚úÖ All origin timeouts are valid (<=180s)"
            exit 0
          fi

          echo "‚ö†Ô∏è Found invalid timeout values:"
          echo "$INVALID_TIMEOUT"
          echo ""
          echo "üîß Fixing timeout values via AWS CLI..."

          # Update config: set all origin_read_timeout > 180 to 180
          jq '.DistributionConfig.Origins.Items = [.DistributionConfig.Origins.Items[] |
            if .CustomOriginConfig.OriginReadTimeout > 180
            then .CustomOriginConfig.OriginReadTimeout = 180
            else . end]' /tmp/cf-config.json > /tmp/cf-config-fixed.json

          # Also fix keepalive timeout if > 60
          jq '.DistributionConfig.Origins.Items = [.DistributionConfig.Origins.Items[] |
            if .CustomOriginConfig.OriginKeepaliveTimeout > 60
            then .CustomOriginConfig.OriginKeepaliveTimeout = 60
            else . end]' /tmp/cf-config-fixed.json > /tmp/cf-config-final.json

          # Extract just the DistributionConfig for the update
          jq '.DistributionConfig' /tmp/cf-config-final.json > /tmp/cf-update.json

          # Update the distribution
          aws cloudfront update-distribution \
            --id "$DIST_ID" \
            --if-match "$ETAG" \
            --distribution-config file:///tmp/cf-update.json

          echo "‚úÖ CloudFront distribution updated with valid timeout values"
          echo ""
          echo "‚è≥ Waiting 30s for CloudFront to propagate changes..."
          sleep 30

      - name: Reset CloudFront Terraform State
        run: |
          echo "üîÑ Resetting CloudFront resource in Terraform state..."
          echo "This fixes state drift that causes InvalidOriginReadTimeout errors"
          echo ""

          DIST_ID=$(terraform output -raw cloudfront_distribution_id 2>/dev/null || echo "")

          if [ -z "$DIST_ID" ]; then
            echo "‚ö†Ô∏è No CloudFront distribution found - skipping state reset"
            exit 0
          fi

          echo "Distribution ID: $DIST_ID"
          echo ""

          # Remove CloudFront distribution from state
          echo "üì§ Removing CloudFront distribution from Terraform state..."
          terraform state rm module.cloudfront.aws_cloudfront_distribution.dashboard || {
            echo "‚ö†Ô∏è Resource not in state (may have already been removed)"
          }

          # Re-import the CloudFront distribution
          echo "üì• Re-importing CloudFront distribution into Terraform state..."
          terraform import \
            -var-file=preprod.tfvars \
            -var="model_version=${{ needs.build.outputs.artifact-sha }}" \
            -var="lambda_package_version=${{ needs.build.outputs.artifact-sha }}" \
            module.cloudfront.aws_cloudfront_distribution.dashboard "$DIST_ID"

          echo ""
          echo "‚úÖ CloudFront state reset complete - state now matches AWS"

      - name: Terraform Plan (Preprod)
        id: plan
        timeout-minutes: 10
        run: |
          terraform plan \
            -var-file=preprod.tfvars \
            -var="model_version=${{ needs.build.outputs.artifact-sha }}" \
            -var="lambda_package_version=${{ needs.build.outputs.artifact-sha }}" \
            -no-color \
            -lock-timeout=5m \
            -out=preprod.tfplan | tee plan.txt

          if grep -q "No changes" plan.txt; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Apply (Preprod)
        if: steps.plan.outputs.has_changes == 'true'
        timeout-minutes: 15
        run: |
          terraform apply -auto-approve -lock-timeout=5m preprod.tfplan
          echo "‚úÖ Preprod infrastructure deployed"

      # Force SSE Lambda to use the new Docker image
      # Terraform doesn't detect :latest tag changes because the image_uri is unchanged.
      # We use the immutable SHA tag pushed by build-sse-image-preprod job.
      - name: Force SSE Lambda Image Update
        run: |
          echo "üîÑ Forcing SSE Lambda to use new Docker image..."

          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          REGION="${{ vars.AWS_REGION }}"
          IMAGE_URI="${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/preprod-sse-streaming-lambda:${{ github.sha }}"

          echo "Image URI: ${IMAGE_URI}"

          aws lambda update-function-code \
            --function-name preprod-sentiment-sse-streaming \
            --image-uri "${IMAGE_URI}"

          # Wait for update to complete
          echo "‚è≥ Waiting for Lambda update to complete..."
          aws lambda wait function-updated \
            --function-name preprod-sentiment-sse-streaming

          echo "‚úÖ SSE Lambda updated to use new image"

      # Force Analysis Lambda to use the new Docker image
      # ADR-005 Phase 2: Container deployment for PyTorch + transformers
      - name: Force Analysis Lambda Image Update
        run: |
          echo "üîÑ Forcing Analysis Lambda to use new Docker image..."

          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          REGION="${{ vars.AWS_REGION }}"
          IMAGE_URI="${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/preprod-analysis-lambda:${{ github.sha }}"

          echo "Image URI: ${IMAGE_URI}"

          aws lambda update-function-code \
            --function-name preprod-sentiment-analysis \
            --image-uri "${IMAGE_URI}"

          # Wait for update to complete
          echo "‚è≥ Waiting for Lambda update to complete..."
          aws lambda wait function-updated \
            --function-name preprod-sentiment-analysis

          echo "‚úÖ Analysis Lambda updated to use new image"

      # Force Dashboard Lambda to use the new Docker image
      # Feature 1036: Container deployment to fix pydantic binary incompatibility
      - name: Force Dashboard Lambda Image Update
        run: |
          echo "üîÑ Forcing Dashboard Lambda to use new Docker image..."

          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          REGION="${{ vars.AWS_REGION }}"
          IMAGE_URI="${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/preprod-dashboard-lambda:${{ github.sha }}"

          echo "Image URI: ${IMAGE_URI}"

          aws lambda update-function-code \
            --function-name preprod-sentiment-dashboard \
            --image-uri "${IMAGE_URI}"

          # Wait for update to complete
          echo "‚è≥ Waiting for Lambda update to complete..."
          aws lambda wait function-updated \
            --function-name preprod-sentiment-dashboard

          echo "‚úÖ Dashboard Lambda updated to use new image"

      - name: Get Preprod Outputs
        id: outputs
        run: |
          DASHBOARD_URL=$(terraform output -raw dashboard_function_url 2>/dev/null || echo "")
          API_URL=$(terraform output -raw dashboard_api_url 2>/dev/null || echo "")
          SNS_TOPIC_ARN=$(terraform output -raw sns_topic_arn 2>/dev/null || echo "")
          SSE_LAMBDA_URL=$(terraform output -raw sse_lambda_function_url 2>/dev/null || echo "")
          DASHBOARD_S3_BUCKET=$(terraform output -raw dashboard_s3_bucket 2>/dev/null || echo "")
          CLOUDFRONT_DISTRIBUTION_ID=$(terraform output -raw cloudfront_distribution_id 2>/dev/null || echo "")

          echo "dashboard_url=${DASHBOARD_URL}" >> $GITHUB_OUTPUT
          echo "api_url=${API_URL}" >> $GITHUB_OUTPUT
          echo "sns_topic_arn=${SNS_TOPIC_ARN}" >> $GITHUB_OUTPUT
          echo "sse_lambda_url=${SSE_LAMBDA_URL}" >> $GITHUB_OUTPUT
          echo "dashboard_s3_bucket=${DASHBOARD_S3_BUCKET}" >> $GITHUB_OUTPUT
          echo "cloudfront_distribution_id=${CLOUDFRONT_DISTRIBUTION_ID}" >> $GITHUB_OUTPUT

      - name: Deploy Dashboard to S3 (Preprod)
        run: |
          BUCKET="${{ steps.outputs.outputs.dashboard_s3_bucket }}"
          DISTRIBUTION_ID="${{ steps.outputs.outputs.cloudfront_distribution_id }}"

          if [ -z "$BUCKET" ]; then
            echo "‚ö†Ô∏è Dashboard S3 bucket not found - skipping dashboard deploy"
            exit 0
          fi

          echo "üì§ Deploying dashboard to S3: ${BUCKET}"

          # Sync dashboard files to S3 with appropriate cache headers
          # HTML files: no-cache (always fetch fresh)
          # JS/CSS files: cache for 1 year (versioned by content hash)
          aws s3 sync ../../src/dashboard/ s3://${BUCKET}/ \
            --exclude ".gitkeep" \
            --exclude "README.md" \
            --cache-control "public, max-age=31536000" \
            --delete

          # Override cache for HTML files (no cache)
          aws s3 cp ../../src/dashboard/index.html s3://${BUCKET}/index.html \
            --cache-control "no-cache, no-store, must-revalidate" \
            --content-type "text/html"

          aws s3 cp ../../src/dashboard/chaos.html s3://${BUCKET}/chaos.html \
            --cache-control "no-cache, no-store, must-revalidate" \
            --content-type "text/html"

          echo "‚úÖ Dashboard deployed to S3"

          # Invalidate CloudFront cache for immediate update
          if [ -n "$DISTRIBUTION_ID" ]; then
            echo "üîÑ Invalidating CloudFront cache..."
            aws cloudfront create-invalidation \
              --distribution-id "$DISTRIBUTION_ID" \
              --paths "/*" \
              --query 'Invalidation.Id' \
              --output text
            echo "‚úÖ CloudFront invalidation created"
          fi

      - name: Smoke Test (Post-Deployment)
        id: smoke_test
        timeout-minutes: 2
        run: |
          # Use API Gateway URL for health check (CloudFront only routes /api/* to API Gateway)
          API_URL="${{ steps.outputs.outputs.api_url }}"

          if [ -z "$API_URL" ]; then
            echo "‚ö†Ô∏è API URL not found - skipping smoke test"
            exit 0
          fi

          echo "üß™ Running post-deployment smoke test..."
          echo "API URL: $API_URL"

          # Test 1: Health endpoint (unauthenticated)
          echo ""
          echo "Test 1: Health endpoint..."
          HEALTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" --max-time 30 "${API_URL}/health")

          if [ "$HEALTH_STATUS" != "200" ]; then
            echo "‚ùå SMOKE TEST FAILED: Health endpoint returned HTTP $HEALTH_STATUS"
            echo "This indicates a Lambda cold start failure or packaging issue."
            echo "Check CloudWatch logs: aws logs tail /aws/lambda/preprod-sentiment-dashboard"
            exit 1
          fi

          echo "‚úÖ Health endpoint: HTTP 200"

          # Test 2: Verify Lambda cold start succeeded (no ImportModuleError)
          echo ""
          echo "Test 2: Checking Lambda logs for import errors..."
          sleep 5  # Wait for CloudWatch logs to propagate

          IMPORT_ERRORS=$(aws logs tail /aws/lambda/preprod-sentiment-dashboard --since 2m --format short 2>/dev/null | \
            grep -i "ImportModuleError\|No module named\|cannot import" || echo "")

          if [ -n "$IMPORT_ERRORS" ]; then
            echo "‚ùå SMOKE TEST FAILED: ImportModuleError detected in Lambda logs"
            echo "$IMPORT_ERRORS"
            echo ""
            echo "This indicates a packaging/binary compatibility issue."
            echo "See docs/security/PREPROD_HTTP_502_ROOT_CAUSE.md for troubleshooting."
            exit 1
          fi

          echo "‚úÖ No import errors in Lambda logs"

          # Test 3: Verify response body is valid JSON
          echo ""
          echo "Test 3: Validating response format..."
          HEALTH_RESPONSE=$(curl -s --max-time 30 "${API_URL}/health")

          if ! echo "$HEALTH_RESPONSE" | jq . >/dev/null 2>&1; then
            echo "‚ùå SMOKE TEST FAILED: Health endpoint returned invalid JSON"
            echo "Response: $HEALTH_RESPONSE"
            exit 1
          fi

          echo "‚úÖ Response is valid JSON"
          echo ""
          echo "üéâ All smoke tests passed! Deployment is healthy."

      - name: Update Deployment Metadata (Preprod)
        if: success()
        run: |
          ENV="preprod"
          BUCKET="preprod-sentiment-lambda-deployments"
          DASHBOARD_URL="${{ steps.outputs.outputs.dashboard_url }}"
          TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          SHA="${GITHUB_SHA:0:7}"

          echo "üìù Updating deployment metadata for ${ENV}..."

          # Download existing metadata or create empty
          aws s3 cp s3://${BUCKET}/deployment-metadata.json /tmp/metadata.json 2>/dev/null || echo '{}' > /tmp/metadata.json

          # Update metadata for this environment
          jq --arg env "$ENV" \
             --arg url "$DASHBOARD_URL" \
             --arg ts "$TIMESTAMP" \
             --arg sha "$SHA" \
             '.[$env] = {"dashboard_url": $url, "last_deployed": $ts, "git_sha": $sha}' \
             /tmp/metadata.json > /tmp/metadata-new.json

          # Upload to S3
          aws s3 cp /tmp/metadata-new.json s3://${BUCKET}/deployment-metadata.json \
            --content-type "application/json"

          echo "‚úÖ Metadata updated: $(cat /tmp/metadata-new.json)"

      - name: Record Deployment
        if: always()
        run: |
          echo "üìù Recording deployment metadata..."
          echo "Git SHA: ${{ github.sha }}"
          echo "Deployed at: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "Dashboard URL: ${{ steps.outputs.outputs.dashboard_url }}"
          echo "Smoke test result: ${{ steps.smoke_test.outcome }}"

  # ========================================================================
  # JOB 4: Test Preprod (Real AWS)
  # ========================================================================
  test-preprod:
    name: Preprod Integration Tests
    needs: [build, deploy-preprod]
    runs-on: ubuntu-latest
    environment: preprod

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.13'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-dev.txt

      - name: Install Playwright browsers
        run: playwright install chromium --with-deps

      - name: Configure AWS Credentials (Preprod)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Get Preprod Infrastructure Outputs
        id: infra
        run: |
          cd infrastructure/terraform
          terraform init \
            -backend-config=backend-preprod.hcl \
            -backend-config="region=${{ vars.AWS_REGION }}" \
            -reconfigure

          SNS_TOPIC_ARN=$(terraform output -raw sns_topic_arn 2>/dev/null || echo "")
          echo "sns_topic_arn=${SNS_TOPIC_ARN}" >> $GITHUB_OUTPUT

      - name: Warm Up Lambdas for Metrics (TD-001)
        env:
          DASHBOARD_URL: ${{ needs.deploy-preprod.outputs.dashboard_url }}
          SSE_LAMBDA_URL: ${{ needs.deploy-preprod.outputs.sse_lambda_url }}
        run: |
          echo "üî• Warming up Lambdas to generate CloudWatch metrics..."
          echo "This ensures observability tests have real metrics to validate."
          echo ""

          # Feature 1047: Use X-User-ID for session auth (consistent with all endpoints)
          # API key auth was removed in Feature 1039 - all endpoints now use session auth
          # Feature 1049: X-User-ID must be a valid UUID format
          WARMUP_USER_ID="00000000-0000-0000-0000-000000000001"

          # Invoke dashboard Lambda multiple times to generate metrics
          echo "1. Invoking Dashboard Lambda /health endpoint..."
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" "${DASHBOARD_URL}/health" || echo "000")
          echo "  HTTP $HTTP_CODE"
          if [ "$HTTP_CODE" != "200" ]; then
            echo "  ‚ö†Ô∏è Health endpoint returned $HTTP_CODE (continuing, may be cold start)"
          fi

          # Feature 1047: Use X-User-ID header for session auth
          echo "2. Invoking /api/v2/metrics endpoint (session auth)..."
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            -H "X-User-ID: ${WARMUP_USER_ID}" \
            "${DASHBOARD_URL}/api/v2/metrics" || echo "000")
          echo "  HTTP $HTTP_CODE"
          if [ "$HTTP_CODE" != "200" ]; then
            echo "‚ùå ERROR: /api/v2/metrics returned HTTP $HTTP_CODE (expected 200)"
            echo "Check extract_auth_context() in auth_middleware.py and Lambda logs"
            exit 1
          fi

          # All endpoints now use X-User-ID header for session auth
          echo "3. Invoking /api/v2/tickers/AAPL/sentiment/history endpoint (warmup only)..."
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            -H "X-User-ID: ${WARMUP_USER_ID}" \
            "${DASHBOARD_URL}/api/v2/tickers/AAPL/sentiment/history" || echo "000")
          echo "  HTTP $HTTP_CODE (non-critical warmup, any response is acceptable)"

          # Warm up SSE Lambda (Feature 140: SSE tests were failing due to cold start)
          # SSE Lambda uses Docker image with Lambda Web Adapter, needs warmup
          # Fix(141): Add retry with backoff to handle cold start delays (10-20s for Docker Lambdas)
          echo ""
          echo "4. Invoking SSE Lambda /health endpoint (with retry)..."
          for i in 1 2 3 4 5; do
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 --max-time 30 "${SSE_LAMBDA_URL}/health" || echo "000")
            echo "  Attempt $i: HTTP $HTTP_CODE"
            if [ "$HTTP_CODE" = "200" ]; then
              echo "  ‚úÖ SSE Lambda /health responded successfully"
              break
            fi
            if [ $i -lt 5 ]; then
              SLEEP_TIME=$((2 ** (i - 1)))
              echo "  Retrying in ${SLEEP_TIME}s..."
              sleep $SLEEP_TIME
            else
              echo "  ‚ö†Ô∏è SSE Lambda /health did not return 200 after 5 attempts (continuing anyway)"
            fi
          done

          echo "5. Invoking SSE Lambda /api/v2/stream/status endpoint (with retry)..."
          for i in 1 2 3 4 5; do
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 --max-time 30 "${SSE_LAMBDA_URL}/api/v2/stream/status" || echo "000")
            echo "  Attempt $i: HTTP $HTTP_CODE"
            if [ "$HTTP_CODE" = "200" ]; then
              echo "  ‚úÖ SSE Lambda /api/v2/stream/status responded successfully"
              break
            fi
            if [ $i -lt 5 ]; then
              SLEEP_TIME=$((2 ** (i - 1)))
              echo "  Retrying in ${SLEEP_TIME}s..."
              sleep $SLEEP_TIME
            else
              echo "  ‚ö†Ô∏è SSE Lambda /api/v2/stream/status did not return 200 after 5 attempts (continuing anyway)"
            fi
          done

          # Warm up Ingestion Lambda (Feature 142: Reduce cold start delays)
          # Uses aws lambda invoke since no HTTP endpoint
          echo ""
          echo "6. Invoking Ingestion Lambda for warmup..."
          INGESTION_FUNCTION="preprod-sentiment-ingestion"
          # Send a warmup event that will be recognized and short-circuited
          WARMUP_PAYLOAD='{"warmup": true}'
          for i in 1 2 3; do
            RESULT=$(aws lambda invoke \
              --function-name "$INGESTION_FUNCTION" \
              --invocation-type RequestResponse \
              --payload "$WARMUP_PAYLOAD" \
              --cli-binary-format raw-in-base64-out \
              /tmp/ingestion-response.json 2>&1) || true
            if echo "$RESULT" | grep -q "200"; then
              echo "  ‚úÖ Ingestion Lambda warmed up successfully"
              break
            fi
            if [ $i -lt 3 ]; then
              echo "  Attempt $i: Retrying in 5s..."
              sleep 5
            else
              echo "  ‚ö†Ô∏è Ingestion Lambda warmup did not complete (continuing anyway)"
            fi
          done

          # Warm up Analysis Lambda (Feature 142: ML model loading)
          # Analysis Lambda has heavy ML dependencies, warmup helps with model loading
          echo ""
          echo "7. Invoking Analysis Lambda for warmup..."
          ANALYSIS_FUNCTION="preprod-sentiment-analysis"
          for i in 1 2 3; do
            RESULT=$(aws lambda invoke \
              --function-name "$ANALYSIS_FUNCTION" \
              --invocation-type RequestResponse \
              --payload "$WARMUP_PAYLOAD" \
              --cli-binary-format raw-in-base64-out \
              /tmp/analysis-response.json 2>&1) || true
            if echo "$RESULT" | grep -q "200"; then
              echo "  ‚úÖ Analysis Lambda warmed up successfully"
              break
            fi
            if [ $i -lt 3 ]; then
              echo "  Attempt $i: Retrying in 5s..."
              sleep 5
            else
              echo "  ‚ö†Ô∏è Analysis Lambda warmup did not complete (continuing anyway)"
            fi
          done

          # Wait for CloudWatch metrics to propagate
          echo ""
          echo "‚è≥ Waiting 60s for CloudWatch metrics to propagate..."
          sleep 60

          echo "‚úÖ Lambda warmup complete - metrics should now be available"

      - name: Run Preprod Integration Tests
        env:
          AWS_REGION: ${{ vars.AWS_REGION }}
          ENVIRONMENT: preprod
          # Legacy table for news/sentiment data (v1 API)
          DYNAMODB_TABLE: preprod-sentiment-items
          # Feature 006 users table for user configs (v2 API, used by dashboard handler)
          DATABASE_TABLE: preprod-sentiment-users
          SNS_TOPIC_ARN: ${{ steps.infra.outputs.sns_topic_arn }}
          DASHBOARD_API_KEY: ${{ secrets.DASHBOARD_API_KEY }}
          API_KEY: ${{ secrets.DASHBOARD_API_KEY }}
          WATCH_TAGS: AI,climate,economy
          MODEL_VERSION: v${{ needs.build.outputs.artifact-sha }}
          # Required for canary tests (test_canary_preprod.py)
          PREPROD_DASHBOARD_URL: ${{ needs.deploy-preprod.outputs.dashboard_url }}
          PREPROD_DASHBOARD_API_KEY: ${{ secrets.DASHBOARD_API_KEY }}
          # Required for E2E tests (tests/e2e/helpers/api_client.py uses PREPROD_API_URL)
          PREPROD_API_URL: ${{ needs.deploy-preprod.outputs.dashboard_url }}
          # Required for SSE E2E tests - routes /api/v2/stream* to SSE Lambda (RESPONSE_STREAM mode)
          # See: specs/082-fix-sse-e2e-timeouts/spec.md
          SSE_LAMBDA_URL: ${{ needs.deploy-preprod.outputs.sse_lambda_url }}
          # Required for E2E Lambda invocation tests (test_e2e_lambda_invocation_preprod.py)
          DASHBOARD_FUNCTION_URL: ${{ needs.deploy-preprod.outputs.dashboard_url }}
        id: integration-tests
        run: |
          # Run tests marked as "preprod" (auto-marked by conftest.py for *_preprod.py files)
          # These tests use REAL AWS resources, not moto mocks
          # Timeout: 720s (12 min) - provides buffer for Playwright tests and Lambda cold starts
          # Increased from 480s due to Dashboard Lambda container deployment cold starts
          # See: specs/1041-increase-integ-test-timeout/spec.md
          set +e  # Don't exit on error
          timeout 720 pytest tests/ \
            -m "preprod" \
            -v \
            --tb=short \
            --junitxml=preprod-test-results.xml
          TEST_EXIT_CODE=$?
          set -e

          if [ $TEST_EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Integration tests passed"
            echo "passed=true" >> $GITHUB_OUTPUT
          elif [ $TEST_EXIT_CODE -eq 124 ]; then
            echo "‚ö†Ô∏è Integration tests timed out (720s limit)"
            echo "passed=false" >> $GITHUB_OUTPUT
            echo "reason=timeout" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Integration tests failed with exit code $TEST_EXIT_CODE"
            echo "passed=false" >> $GITHUB_OUTPUT
            echo "reason=failed" >> $GITHUB_OUTPUT
          fi

          # Always exit 0 to allow artifact upload, but track status
          exit 0

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: preprod-test-results-${{ needs.build.outputs.artifact-sha }}
          path: preprod-test-results.xml
          retention-days: 30

      - name: Check Integration Test Results
        if: steps.integration-tests.outputs.passed != 'true'
        run: |
          echo "::error::Integration tests did not pass: ${{ steps.integration-tests.outputs.reason }}"
          echo ""
          echo "Check the preprod-test-results artifact for details."
          echo ""
          echo "Common causes:"
          echo "  - Timeout: Lambda cold start taking too long, increase warmup time"
          echo "  - Failed: Check test output above for specific failures"
          echo "  - Auth: Endpoints use session auth (X-User-ID header), not API key (Feature 1039)"
          exit 1

      - name: Create Validation Metadata
        if: steps.integration-tests.outputs.passed == 'true'
        run: |
          SHA="${{ needs.build.outputs.artifact-sha }}"
          cat > validated.json <<EOF
          {
            "artifact_name": "lambda-packages-${SHA}",
            "validation_timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "preprod_tests_passed": true,
            "validated_by_workflow": "${GITHUB_RUN_ID}",
            "git_sha": "${GITHUB_SHA}",
            "ready_for_production": true
          }
          EOF
          cat validated.json
          echo "‚úÖ Integration tests passed - ready for production!"

      - name: Upload Validation Metadata
        if: success()
        uses: actions/upload-artifact@v6
        with:
          name: validation-${{ needs.build.outputs.artifact-sha }}
          path: validated.json
          retention-days: 90

  # ========================================================================
  # JOB 5: Build SSE Lambda Image (Production)
  # TEMPORARILY DISABLED: Production deployment skipped until ready for prod release
  # To re-enable: Remove `if: false` from jobs 5-7 and restore summary needs
  # See: interview/FUTURE_IMPROVEMENTS.md for tracking
  # ========================================================================
  build-sse-image-prod:
    name: Build SSE Lambda Image (Production)
    if: false  # TEMP: Skip production - re-enable when ready for prod release
    runs-on: ubuntu-latest
    needs: [build, deploy-preprod, test-preprod]
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Configure AWS Credentials (Production)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and Push SSE Lambda Image
        uses: docker/build-push-action@v6
        with:
          # Build context changed to src/ to include src/lib/timeseries
          # Required for: from src.lib.timeseries import Resolution
          context: src
          file: src/lambdas/sse_streaming/Dockerfile
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/prod-sse-streaming-lambda:latest
            ${{ steps.login-ecr.outputs.registry }}/prod-sse-streaming-lambda:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          provenance: false

      # Smoke test: Verify Python imports work inside the Docker image
      # See: fix(128) - ModuleNotFoundError for src.lambdas.shared imports
      - name: Smoke Test SSE Lambda Imports
        run: |
          echo "üß™ Running Docker import smoke test..."
          IMAGE="${{ steps.login-ecr.outputs.registry }}/prod-sse-streaming-lambda:${{ github.sha }}"

          # Note: PYTHONPATH matches Lambda environment variable in Terraform
          # Feature 1043: USERS_TABLE for config queries, SENTIMENTS_TABLE for sentiment data
          docker run --rm -e PYTHONPATH=/app/packages:/app -e USERS_TABLE=prod-sentiment-users -e SENTIMENTS_TABLE=prod-sentiment-items --entrypoint python "$IMAGE" -c "
          import sys
          from handler import app
          from config import config_lookup_service
          from stream import stream_generator
          from src.lambdas.shared.logging_utils import sanitize_for_log
          from src.lambdas.shared.models.configuration import Configuration
          # Timeseries models (Feature 1009) - added after ModuleNotFoundError fix
          from src.lib.timeseries import Resolution
          from timeseries_models import PartialBucketEvent
          print('‚úÖ All imports successful')
          sys.exit(0)
          "

          if [ $? -eq 0 ]; then
            echo "‚úÖ SSE Lambda import smoke test passed"
          else
            echo "‚ùå SSE Lambda import smoke test FAILED"
            exit 1
          fi

  # ========================================================================
  # JOB 5.5: Build Dashboard Lambda Image (Production)
  # Feature 1036: Container deployment to fix pydantic binary incompatibility
  # ========================================================================
  build-dashboard-image-prod:
    name: Build Dashboard Lambda Image (Production)
    if: false  # TEMP: Skip production (see JOB 5 comment)
    runs-on: ubuntu-latest
    needs: [build, deploy-preprod, test-preprod]
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Configure AWS Credentials (Production)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and Push Dashboard Lambda Image
        uses: docker/build-push-action@v6
        with:
          context: src
          file: src/lambdas/dashboard/Dockerfile
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/prod-dashboard-lambda:latest
            ${{ steps.login-ecr.outputs.registry }}/prod-dashboard-lambda:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          provenance: false

      - name: Smoke Test Dashboard Lambda Imports
        run: |
          echo "üß™ Running Docker import smoke test..."
          IMAGE="${{ steps.login-ecr.outputs.registry }}/prod-dashboard-lambda:${{ github.sha }}"

          # Note: Must pass env vars that modules expect at import time (chaos.py, handler.py)
          # Feature 1043: USERS_TABLE and SENTIMENTS_TABLE are required at import time
          docker run --rm -e ENVIRONMENT=prod -e USERS_TABLE=prod-sentiment-users -e SENTIMENTS_TABLE=prod-sentiment-items --entrypoint python "$IMAGE" -c "
          import sys
          from handler import lambda_handler
          from fastapi import FastAPI
          from mangum import Mangum
          import pydantic
          from pydantic import BaseModel
          print(f'pydantic version: {pydantic.__version__}')
          from src.lambdas.shared.logging_utils import sanitize_for_log
          print('‚úÖ All imports successful')
          sys.exit(0)
          "

          if [ $? -eq 0 ]; then
            echo "‚úÖ Dashboard Lambda import smoke test passed"
          else
            echo "‚ùå Dashboard Lambda import smoke test FAILED"
            exit 1
          fi

  # ========================================================================
  # JOB 6: Deploy to Production
  # TEMPORARILY DISABLED - see JOB 5 comment
  # ========================================================================
  deploy-prod:
    name: Deploy to Production
    if: false  # TEMP: Skip production
    needs: [build, deploy-preprod, test-preprod, build-sse-image-prod, build-dashboard-image-prod]
    runs-on: ubuntu-latest

    # Conditional environment based on PR author
    # - Dependabot ‚Üí production-auto (no approval)
    # - Human ‚Üí production (requires approval)
    environment:
      name: ${{ github.actor == 'dependabot[bot]' && 'production-auto' || 'production' }}
      url: ${{ steps.outputs.outputs.dashboard_url }}

    outputs:
      dashboard_url: ${{ steps.outputs.outputs.dashboard_url }}

    defaults:
      run:
        working-directory: infrastructure/terraform

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Download Lambda Packages
        uses: actions/download-artifact@v7
        with:
          name: ${{ needs.build.outputs.artifact-name }}
          path: packages/

      - name: Configure AWS Credentials (Production)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Upload Lambda Packages to S3 (Production)
        run: |
          SHA="${{ needs.build.outputs.artifact-sha }}"
          BUCKET="prod-sentiment-lambda-deployments"

          echo "üì§ Uploading VALIDATED Lambda packages to S3 (production)..."

          aws s3 cp ../../packages/ingestion-${SHA}.zip \
            s3://${BUCKET}/ingestion/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ),validated-in=preprod"

          aws s3 cp ../../packages/analysis-${SHA}.zip \
            s3://${BUCKET}/analysis/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ),validated-in=preprod"

          # Dashboard Lambda now uses container deployment (Feature 1036)
          # No ZIP upload needed - see build-dashboard-image-prod job

          aws s3 cp ../../packages/metrics-${SHA}.zip \
            s3://${BUCKET}/metrics/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ),validated-in=preprod"

          aws s3 cp ../../packages/notification-${SHA}.zip \
            s3://${BUCKET}/notification/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ),validated-in=preprod"

          echo "‚úÖ Lambda packages uploaded to S3"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Init (Production)
        run: |
          terraform init \
            -backend-config=backend-prod.hcl \
            -backend-config="region=${{ vars.AWS_REGION }}" \
            -reconfigure

      - name: Check for Stale Terraform Locks (Production)
        run: |
          echo "üîç Checking for Terraform state locks..."

          # Check for S3 native lock file (.tflock)
          BUCKET="sentiment-analyzer-terraform-state-218795110243"
          LOCK_KEY="prod/terraform.tfstate.tflock"

          LOCK_EXISTS=$(aws s3api head-object \
            --bucket "$BUCKET" \
            --key "$LOCK_KEY" \
            --region ${{ vars.AWS_REGION }} \
            2>/dev/null || echo "")

          if [ -z "$LOCK_EXISTS" ]; then
            echo "‚úÖ No lock file found - proceeding with deployment"
            exit 0
          fi

          echo "‚ö†Ô∏è  LOCK DETECTED - Manual intervention may be required"
          echo ""
          echo "This workflow uses concurrency controls (cancel-in-progress: false)"
          echo "so only ONE deployment should run at a time."
          echo ""
          echo "If you see a lock here, it likely means:"
          echo "1. A previous run was canceled/failed and left an orphaned lock"
          echo "2. Someone is running Terraform manually (check with team)"
          echo ""
          echo "Lock file details:"
          echo "  Location: s3://$BUCKET/$LOCK_KEY"

          # Get lock file metadata
          LAST_MODIFIED=$(echo "$LOCK_EXISTS" | jq -r '.LastModified // "unknown"')
          echo "  Last Modified: $LAST_MODIFIED"

          # Download and display lock contents (contains lock ID and info)
          aws s3 cp "s3://$BUCKET/$LOCK_KEY" - 2>/dev/null | head -c 500 || echo "  (Unable to read lock contents)"
          echo ""
          echo ""
          echo "üõ†Ô∏è  TO MANUALLY UNLOCK (PRODUCTION):"
          echo "‚ö†Ô∏è  ONLY unlock if you're CERTAIN no other deployment is running!"
          echo ""
          echo "  cd infrastructure/terraform"
          echo "  terraform force-unlock <LOCK_ID>"
          echo ""
          echo "Or delete the lock file via AWS CLI:"
          echo "  aws s3 rm s3://$BUCKET/$LOCK_KEY"
          echo ""
          echo "‚è≥ Proceeding with deployment..."
          echo "   Terraform will wait for lock release (timeout: 5 minutes)"
          echo ""

      - name: Terraform Refresh (Production)
        run: |
          echo "üîÑ Syncing Terraform state with AWS..."
          terraform refresh \
            -var-file=prod.tfvars \
            -var="model_version=${{ needs.build.outputs.artifact-sha }}" \
            -var="lambda_package_version=${{ needs.build.outputs.artifact-sha }}" \
            -lock-timeout=5m
          echo "‚úÖ State refresh complete"

      - name: Fix CloudFront Invalid Timeout (AWS CLI)
        run: |
          echo "üîç Checking CloudFront distribution for invalid timeout values..."

          DIST_ID=$(terraform output -raw cloudfront_distribution_id 2>/dev/null || echo "")

          if [ -z "$DIST_ID" ]; then
            echo "‚ö†Ô∏è No CloudFront distribution found - skipping timeout check"
            exit 0
          fi

          echo "Distribution ID: $DIST_ID"

          aws cloudfront get-distribution-config --id "$DIST_ID" > /tmp/cf-config.json
          ETAG=$(jq -r '.ETag' /tmp/cf-config.json)

          INVALID_TIMEOUT=$(jq -r '.DistributionConfig.Origins.Items[]? |
            select(.CustomOriginConfig.OriginReadTimeout > 180) |
            "\(.Id): \(.CustomOriginConfig.OriginReadTimeout)s"' /tmp/cf-config.json)

          if [ -z "$INVALID_TIMEOUT" ]; then
            echo "‚úÖ All origin timeouts are valid (<=180s)"
            exit 0
          fi

          echo "‚ö†Ô∏è Found invalid timeout values:"
          echo "$INVALID_TIMEOUT"
          echo ""
          echo "üîß Fixing timeout values via AWS CLI..."

          jq '.DistributionConfig.Origins.Items = [.DistributionConfig.Origins.Items[] |
            if .CustomOriginConfig.OriginReadTimeout > 180
            then .CustomOriginConfig.OriginReadTimeout = 180
            else . end]' /tmp/cf-config.json > /tmp/cf-config-fixed.json

          jq '.DistributionConfig.Origins.Items = [.DistributionConfig.Origins.Items[] |
            if .CustomOriginConfig.OriginKeepaliveTimeout > 60
            then .CustomOriginConfig.OriginKeepaliveTimeout = 60
            else . end]' /tmp/cf-config-fixed.json > /tmp/cf-config-final.json

          jq '.DistributionConfig' /tmp/cf-config-final.json > /tmp/cf-update.json

          aws cloudfront update-distribution \
            --id "$DIST_ID" \
            --if-match "$ETAG" \
            --distribution-config file:///tmp/cf-update.json

          echo "‚úÖ CloudFront distribution updated with valid timeout values"
          echo ""
          echo "‚è≥ Waiting 30s for CloudFront to propagate changes..."
          sleep 30

      - name: Reset CloudFront Terraform State
        run: |
          echo "üîÑ Resetting CloudFront resource in Terraform state..."

          DIST_ID=$(terraform output -raw cloudfront_distribution_id 2>/dev/null || echo "")

          if [ -z "$DIST_ID" ]; then
            echo "‚ö†Ô∏è No CloudFront distribution found - skipping state reset"
            exit 0
          fi

          echo "Distribution ID: $DIST_ID"

          terraform state rm module.cloudfront.aws_cloudfront_distribution.dashboard || {
            echo "‚ö†Ô∏è Resource not in state"
          }

          terraform import \
            -var-file=prod.tfvars \
            -var="model_version=${{ needs.build.outputs.artifact-sha }}" \
            -var="lambda_package_version=${{ needs.build.outputs.artifact-sha }}" \
            module.cloudfront.aws_cloudfront_distribution.dashboard "$DIST_ID"

          echo "‚úÖ CloudFront state reset complete"

      - name: Terraform Plan (Production)
        id: plan
        timeout-minutes: 10
        run: |
          terraform plan \
            -var-file=prod.tfvars \
            -var="model_version=${{ needs.build.outputs.artifact-sha }}" \
            -var="lambda_package_version=${{ needs.build.outputs.artifact-sha }}" \
            -no-color \
            -lock-timeout=5m \
            -out=prod.tfplan | tee plan.txt

      - name: Terraform Apply (Production)
        timeout-minutes: 15
        run: |
          echo "üöÄ Deploying to PRODUCTION..."
          terraform apply -auto-approve -lock-timeout=5m prod.tfplan
          echo "‚úÖ Production deployment complete"

      - name: Get Production Outputs
        id: outputs
        run: |
          DASHBOARD_URL=$(terraform output -raw dashboard_function_url 2>/dev/null || echo "")
          API_URL=$(terraform output -raw dashboard_api_url 2>/dev/null || echo "")
          echo "dashboard_url=${DASHBOARD_URL}" >> $GITHUB_OUTPUT
          echo "api_url=${API_URL}" >> $GITHUB_OUTPUT
          echo "Production Dashboard URL: ${DASHBOARD_URL}"
          echo "Production API URL: ${API_URL}"

      - name: Update Deployment Metadata (Production)
        if: success()
        run: |
          ENV="prod"
          BUCKET="preprod-sentiment-lambda-deployments"
          DASHBOARD_URL="${{ steps.outputs.outputs.dashboard_url }}"
          TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          SHA="${GITHUB_SHA:0:7}"

          echo "üìù Updating deployment metadata for ${ENV}..."

          # Download existing metadata or create empty
          aws s3 cp s3://${BUCKET}/deployment-metadata.json /tmp/metadata.json 2>/dev/null || echo '{}' > /tmp/metadata.json

          # Update metadata for this environment
          jq --arg env "$ENV" \
             --arg url "$DASHBOARD_URL" \
             --arg ts "$TIMESTAMP" \
             --arg sha "$SHA" \
             '.[$env] = {"dashboard_url": $url, "last_deployed": $ts, "git_sha": $sha}' \
             /tmp/metadata.json > /tmp/metadata-new.json

          # Upload to S3
          aws s3 cp /tmp/metadata-new.json s3://${BUCKET}/deployment-metadata.json \
            --content-type "application/json"

          echo "‚úÖ Metadata updated: $(cat /tmp/metadata-new.json)"

  # ========================================================================
  # JOB 7: Production Canary Test
  # TEMPORARILY DISABLED - see JOB 5 comment
  # ========================================================================
  canary:
    name: Production Canary Test
    if: false  # TEMP: Skip production
    needs: [build, deploy-prod]
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Run Canary Health Check
        timeout-minutes: 2
        run: |
          # Use API Gateway URL for health check (more reliable than CloudFront for canary)
          API_URL="${{ needs.deploy-prod.outputs.api_url }}"

          echo "üîç Running canary test against: ${API_URL}"

          # Feature 1047: Health endpoint is unauthenticated (no API key needed)
          response=$(curl -f -s -w "\n%{http_code}" \
            "${API_URL}/health" || echo "FAILED")

          http_code=$(echo "$response" | tail -1)
          body=$(echo "$response" | head -n -1)

          echo "HTTP Status: ${http_code}"
          echo "Response: ${body}"

          if [ "${http_code}" != "200" ]; then
            echo "‚ùå Canary FAILED - Health check returned ${http_code}"
            exit 1
          fi

          if ! echo "$body" | grep -q '"status"'; then
            echo "‚ùå Canary FAILED - Response missing 'status' field"
            exit 1
          fi

          echo "‚úÖ Canary test PASSED"

  # ========================================================================
  # JOB 8: Summary
  # TEMPORARILY: Only depends on preprod jobs (production disabled)
  # To restore: Add build-sse-image-prod, deploy-prod, canary to needs
  # ========================================================================
  summary:
    name: Deployment Summary
    needs: [build, test, build-sse-image-preprod, build-analysis-image-preprod, build-dashboard-image-preprod, deploy-preprod, test-preprod]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Deployment Status
        run: |
          echo "## Deployment Pipeline Summary"
          echo ""
          echo "- Build: ${{ needs.build.result }}"
          echo "- Unit Tests (Mocked): ${{ needs.test.result }}"
          echo "- Build SSE Image (Preprod): ${{ needs.build-sse-image-preprod.result }}"
          echo "- Build Analysis Image (Preprod): ${{ needs.build-analysis-image-preprod.result }}"
          echo "- Deploy Preprod: ${{ needs.deploy-preprod.result }}"
          echo "- Test Preprod (Real AWS): ${{ needs.test-preprod.result }}"
          echo "- Production: SKIPPED (temporarily disabled)"
          echo ""
          echo "Artifact SHA: ${{ needs.build.outputs.artifact-sha }}"
          echo "Preprod Dashboard: ${{ needs.deploy-preprod.outputs.dashboard_url }}"
