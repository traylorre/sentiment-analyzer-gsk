name: "Deploy Pipeline"

# Consolidated deployment pipeline using job dependencies (needs:)
# instead of multiple workflow_run triggers.
#
# ARCHITECTURAL NOTE:
# Previous implementation used 4 separate workflows chained with workflow_run triggers.
# This violated GitHub Actions' 3-level workflow_run nesting limit and caused
# "startup_failure" issues. The correct pattern is multiple jobs with dependencies
# within a single workflow.
#
# Flow:
#   push to main ‚Üí build ‚Üí deploy-dev ‚Üí test-dev ‚Üí deploy-preprod ‚Üí test-preprod ‚Üí deploy-prod
#
# Benefits:
# - No workflow_run nesting limits
# - Shared artifacts between jobs
# - Single workflow run view
# - Automatic progression through stages
# - Environment-based approval gates

on:
  push:
    branches:
      - main
    # REMOVED paths-ignore - it was blocking critical deploys!
    # Problem: When PR #94 changed deploy.yml + tests/conftest.py, it should have
    # triggered a deploy, but the workflow was skipped. This left Lambda running
    # old code with pydantic binary incompatibility (HTTP 502).
    #
    # Better approach: Deploy on ALL pushes to main. Terraform and the build job
    # are idempotent - if nothing changed, they complete quickly. The cost of
    # running an extra 5-minute workflow is far less than the cost of missing
    # a critical fix.
    #
    # If build times become a problem, consider:
    # - Using paths filter on the build job ONLY (not the whole workflow)
    # - Caching more aggressively
    # - Using Turborepo-style content hashing

  # Manual trigger for testing/rollbacks
  workflow_dispatch:

permissions:
  contents: read
  actions: write
  id-token: write

# Prevent concurrent deployments
concurrency:
  group: deploy-pipeline
  cancel-in-progress: false

jobs:
  # ========================================================================
  # JOB 1: Build Lambda Packages
  # ========================================================================
  build:
    name: Build Lambda Packages
    runs-on: ubuntu-latest

    outputs:
      artifact-sha: ${{ steps.package.outputs.sha }}
      artifact-name: ${{ steps.package.outputs.name }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.13
        uses: actions/setup-python@v6
        with:
          python-version: '3.13'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Free up disk space
        run: |
          echo "üßπ Freeing up disk space on GitHub runner..."
          df -h

          # Remove user-owned caches only (NO SUDO - security requirement)
          # Build needs ~600MB, runner has ~2GB available
          # KEEP ~/.cache/pip for faster subsequent builds (saves ~30-50% on pip install)
          rm -rf ~/.npm
          docker system prune -af --volumes

          echo "‚úÖ Disk space after cleanup:"
          df -h

      - name: Package Lambda Functions
        id: package
        run: |
          SHA="${GITHUB_SHA:0:7}"
          echo "sha=${SHA}" >> $GITHUB_OUTPUT
          echo "name=lambda-packages-${SHA}" >> $GITHUB_OUTPUT

          echo "üì¶ Building Lambda packages for commit: ${SHA}"
          mkdir -p packages

          # Validation function to test package imports
          validate_lambda_package() {
            local package_name=$1
            local handler_file=$2

            echo "üîç Validating ${package_name} package structure..."
            cd packages/${package_name}-build

            # Verify handler file exists and has no syntax errors
            if [ ! -f "${handler_file}" ]; then
              echo "‚ùå VALIDATION FAILED: ${handler_file} not found in ${package_name} package"
              exit 1
            fi

            # Check for basic Python syntax errors
            if ! python3 -m py_compile "${handler_file}" 2>/dev/null; then
              echo "‚ùå VALIDATION FAILED: Syntax error in ${handler_file}"
              exit 1
            fi

            echo "‚úÖ ${package_name} package structure validated"
            cd ../..
          }

          # ============================================================
          # Ingestion Lambda - Lightweight dependencies only
          # ============================================================
          echo ""
          echo "üì¶ Packaging Ingestion Lambda (boto3, requests, pydantic, logging)..."

          # Install only ingestion dependencies
          # Use platform-specific flags for Lambda Python 3.13 compatibility
          # CRITICAL: pydantic-core has Rust binaries that MUST match Lambda runtime
          pip install \
            boto3==1.41.0 \
            requests==2.32.5 \
            pydantic==2.12.4 \
            python-json-logger==4.0.0 \
            -t packages/ingestion-deps/ \
            --platform manylinux2014_x86_64 \
            --implementation cp \
            --python-version 313 \
            --only-binary=:all: \
            --no-cache-dir \
            --disable-pip-version-check \
            --quiet

          # Build ingestion package
          mkdir -p packages/ingestion-build/src/lambdas packages/ingestion-build/src/lib
          cp -r packages/ingestion-deps/* packages/ingestion-build/
          cp -r src/lambdas/ingestion/* packages/ingestion-build/
          cp -r src/lambdas/shared packages/ingestion-build/src/lambdas/
          cp -r src/lib/* packages/ingestion-build/src/lib/
          cd packages/ingestion-build
          zip -r ../ingestion-${SHA}.zip . \
            -x "*.pyc" "__pycache__/*" "*.pytest_cache/*" "tests/*" -q
          cd ../..

          # Validate package structure before zipping
          validate_lambda_package "ingestion" "handler.py"

          # Show size and cleanup
          INGESTION_SIZE=$(du -h packages/ingestion-${SHA}.zip | cut -f1)
          echo "‚úÖ Ingestion Lambda: ${INGESTION_SIZE}"
          rm -rf packages/ingestion-deps packages/ingestion-build

          # ============================================================
          # Dashboard Lambda - Web framework dependencies
          # ============================================================
          echo ""
          echo "üì¶ Packaging Dashboard Lambda (FastAPI, Mangum, SSE, pydantic)..."

          # Install dashboard dependencies
          # CRITICAL: pydantic-core has Rust binaries that MUST match Lambda runtime
          # Use explicit platform flags to ensure cp313 wheels are downloaded
          # (Docker container approach was downloading cp311 wheels from cache)
          pip install \
            fastapi==0.121.3 \
            mangum==0.19.0 \
            sse-starlette==3.0.3 \
            pydantic==2.12.4 \
            boto3==1.41.0 \
            python-json-logger==4.0.0 \
            -t packages/dashboard-deps/ \
            --platform manylinux2014_x86_64 \
            --implementation cp \
            --python-version 313 \
            --only-binary=:all: \
            --no-cache-dir \
            --disable-pip-version-check \
            --quiet

          # Verify pydantic-core binary is for Python 3.13
          echo "üîç Verifying pydantic-core binary compatibility..."
          PYDANTIC_SO=$(find packages/dashboard-deps -name "_pydantic_core*.so" | head -1)
          if [ -z "$PYDANTIC_SO" ]; then
            echo "‚ùå ERROR: pydantic-core .so file not found!"
            exit 1
          fi
          echo "Found: $PYDANTIC_SO"
          if echo "$PYDANTIC_SO" | grep -q "cpython-313"; then
            echo "‚úÖ pydantic-core binary is for Python 3.13"
          else
            echo "‚ùå ERROR: pydantic-core binary is NOT for Python 3.13!"
            echo "File: $PYDANTIC_SO"
            exit 1
          fi

          # Build dashboard package
          # NOTE: handler.py must be at ROOT for Lambda handler config "handler.lambda_handler"
          # The src/ directory structure is needed for internal imports (from src.lambdas.shared...)
          mkdir -p packages/dashboard-build/src/lambdas/dashboard packages/dashboard-build/src/lib packages/dashboard-build/src/dashboard
          cp -r packages/dashboard-deps/* packages/dashboard-build/
          cp -r src/lambdas/dashboard/* packages/dashboard-build/          # handler.py at ROOT
          cp -r src/lambdas/dashboard/* packages/dashboard-build/src/lambdas/dashboard/  # also in src/ for imports
          cp -r src/lambdas/shared packages/dashboard-build/src/lambdas/
          cp -r src/lib/* packages/dashboard-build/src/lib/
          cp -r src/dashboard/* packages/dashboard-build/src/dashboard/
          cd packages/dashboard-build
          zip -r ../dashboard-${SHA}.zip . \
            -x "*.pyc" "__pycache__/*" "*.pytest_cache/*" "tests/*" -q
          cd ../..

          # Validate package structure before cleanup
          validate_lambda_package "dashboard" "handler.py"

          # Show size and cleanup
          DASHBOARD_SIZE=$(du -h packages/dashboard-${SHA}.zip | cut -f1)
          echo "‚úÖ Dashboard Lambda: ${DASHBOARD_SIZE}"
          rm -rf packages/dashboard-deps packages/dashboard-build

          # ============================================================
          # Analysis Lambda - Minimal dependencies (model loads from S3)
          # ============================================================
          echo ""
          echo "üì¶ Packaging Analysis Lambda (minimal - model in S3)..."

          # Install minimal analysis dependencies (NO torch/transformers)
          # NOTE: Analysis Lambda will use container images (see PR #58)
          # This ZIP packaging remains temporarily for backwards compatibility
          # CRITICAL: pydantic-core has Rust binaries that MUST match Lambda runtime
          pip install \
            boto3==1.41.0 \
            pydantic==2.12.4 \
            python-json-logger==4.0.0 \
            -t packages/analysis-deps/ \
            --platform manylinux2014_x86_64 \
            --implementation cp \
            --python-version 313 \
            --only-binary=:all: \
            --no-cache-dir \
            --disable-pip-version-check \
            --quiet

          # Build analysis package
          mkdir -p packages/analysis-build/src/lambdas packages/analysis-build/src/lib
          cp -r packages/analysis-deps/* packages/analysis-build/
          cp -r src/lambdas/analysis/* packages/analysis-build/
          cp -r src/lambdas/shared packages/analysis-build/src/lambdas/
          cp -r src/lib/* packages/analysis-build/src/lib/
          cd packages/analysis-build
          zip -r ../analysis-${SHA}.zip . \
            -x "*.pyc" "__pycache__/*" "*.pytest_cache/*" "tests/*" -q
          cd ../..

          # Validate package structure before cleanup
          validate_lambda_package "analysis" "handler.py"

          # Show size and cleanup
          ANALYSIS_SIZE=$(du -h packages/analysis-${SHA}.zip | cut -f1)
          echo "‚úÖ Analysis Lambda: ${ANALYSIS_SIZE}"
          rm -rf packages/analysis-deps packages/analysis-build

          # ============================================================
          # Metrics Lambda - Minimal dependencies (TD-011)
          # ============================================================
          echo ""
          echo "üì¶ Packaging Metrics Lambda (minimal - boto3 only)..."

          # Install minimal metrics dependencies
          # Note: Metrics Lambda doesn't use pydantic, but we include platform flags
          # for consistency and future-proofing if dependencies are added
          pip install \
            boto3==1.41.0 \
            python-json-logger==4.0.0 \
            -t packages/metrics-deps/ \
            --platform manylinux2014_x86_64 \
            --implementation cp \
            --python-version 313 \
            --only-binary=:all: \
            --no-cache-dir \
            --disable-pip-version-check \
            --quiet

          # Build metrics package
          mkdir -p packages/metrics-build/src/lambdas packages/metrics-build/src/lib
          cp -r packages/metrics-deps/* packages/metrics-build/
          cp -r src/lambdas/metrics/* packages/metrics-build/
          cp -r src/lambdas/shared packages/metrics-build/src/lambdas/
          cp -r src/lib/* packages/metrics-build/src/lib/
          cd packages/metrics-build
          zip -r ../metrics-${SHA}.zip . \
            -x "*.pyc" "__pycache__/*" "*.pytest_cache/*" "tests/*" -q
          cd ../..

          # Validate package structure before cleanup
          validate_lambda_package "metrics" "handler.py"

          # Show size and cleanup
          METRICS_SIZE=$(du -h packages/metrics-${SHA}.zip | cut -f1)
          echo "‚úÖ Metrics Lambda: ${METRICS_SIZE}"
          rm -rf packages/metrics-deps packages/metrics-build

          # ============================================================
          # Summary
          # ============================================================
          echo ""
          echo "üìä Package Summary:"
          echo "  Ingestion:  ${INGESTION_SIZE}"
          echo "  Dashboard:  ${DASHBOARD_SIZE}"
          echo "  Analysis:   ${ANALYSIS_SIZE}"
          echo "  Metrics:    ${METRICS_SIZE}"
          echo ""
          ls -lh packages/*.zip
          echo ""
          echo "‚úÖ All Lambda packages built successfully!"
          echo "üöÄ Build time optimized: Per-Lambda dependencies (not bundling 3.9GB!)"

      - name: Upload Lambda Packages as Artifacts
        uses: actions/upload-artifact@v5
        with:
          name: ${{ steps.package.outputs.name }}
          path: packages/
          retention-days: 90
          if-no-files-found: error

      - name: Create Build Manifest
        run: |
          SHA="${GITHUB_SHA:0:7}"
          cat > build-manifest.json <<EOF
          {
            "git_sha": "${GITHUB_SHA}",
            "git_sha_short": "${SHA}",
            "git_ref": "${GITHUB_REF}",
            "build_timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "actor": "${GITHUB_ACTOR}",
            "workflow_run_id": "${GITHUB_RUN_ID}",
            "workflow_run_number": "${GITHUB_RUN_NUMBER}"
          }
          EOF
          cat build-manifest.json

      - name: Upload Build Manifest
        uses: actions/upload-artifact@v5
        with:
          name: build-manifest-${{ steps.package.outputs.sha }}
          path: build-manifest.json
          retention-days: 90

  # ========================================================================
  # JOB 2: Deploy to Dev
  # ========================================================================
  deploy-dev:
    name: Deploy to Dev
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: dev
      url: ${{ steps.outputs.outputs.dashboard_url }}

    outputs:
      dashboard_url: ${{ steps.outputs.outputs.dashboard_url }}
      sns_topic_arn: ${{ steps.outputs.outputs.sns_topic_arn }}

    defaults:
      run:
        working-directory: infrastructure/terraform

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Lambda Packages
        uses: actions/download-artifact@v5
        with:
          pattern: lambda-packages-*
          path: packages
          merge-multiple: true

      - name: Configure AWS Credentials (Dev)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Upload Lambda Packages to S3 (Dev)
        run: |
          set -e
          SHA="${GITHUB_SHA::7}"
          BUCKET="dev-sentiment-lambda-deployments"

          echo "Uploading Lambda packages to S3..."
          aws s3 cp ../../packages/ingestion-${SHA}.zip s3://${BUCKET}/ingestion/lambda.zip
          aws s3 cp ../../packages/analysis-${SHA}.zip s3://${BUCKET}/analysis/lambda.zip
          aws s3 cp ../../packages/dashboard-${SHA}.zip s3://${BUCKET}/dashboard/lambda.zip
          aws s3 cp ../../packages/metrics-${SHA}.zip s3://${BUCKET}/metrics/lambda.zip

          echo "Lambda packages uploaded successfully"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.10.0
          terraform_wrapper: false

      - name: Terraform Init (Dev)
        run: |
          terraform init -backend-config=backend-dev.hcl -reconfigure

      - name: Terraform Plan (Dev)
        run: |
          SHA="${GITHUB_SHA::7}"
          terraform plan \
            -var-file=dev.tfvars \
            -var="lambda_package_version=${SHA}" \
            -out=dev.tfplan

      - name: Terraform Apply (Dev)
        run: |
          terraform apply -auto-approve dev.tfplan

      - name: Capture Terraform Outputs
        id: outputs
        run: |
          dashboard_url=$(terraform output -raw dashboard_url || echo "")
          sns_topic_arn=$(terraform output -raw sns_topic_arn || echo "")

          echo "dashboard_url=${dashboard_url}" >> $GITHUB_OUTPUT
          echo "sns_topic_arn=${sns_topic_arn}" >> $GITHUB_OUTPUT

      - name: Smoke Test (Post-Deployment)
        run: |
          dashboard_url="${{ steps.outputs.outputs.dashboard_url }}"
          if [ -z "$dashboard_url" ]; then
            echo "Dashboard URL not available, skipping smoke test"
            exit 0
          fi

          echo "Testing Dashboard Lambda at: $dashboard_url"
          status_code=$(curl -s -o /dev/null -w "%{http_code}" "${dashboard_url}/health" || echo "000")

          if [ "$status_code" == "200" ]; then
            echo "‚úì Dashboard Lambda is responding (HTTP $status_code)"
          else
            echo "‚úó Dashboard Lambda health check failed (HTTP $status_code)"
            exit 1
          fi

      - name: Record Deployment
        if: success()
        run: |
          echo "Dev deployment completed at $(date)"
          echo "Commit: ${{ github.sha }}"
          echo "Dashboard URL: ${{ steps.outputs.outputs.dashboard_url }}"

  # ========================================================================
  # JOB 3: Test Dev (Local/Mocked)
  # ========================================================================
  test-dev:
    name: Dev Tests (Mocked AWS)
    needs: [build, deploy-dev]
    runs-on: ubuntu-latest
    environment: dev

    permissions:
      contents: read
      actions: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.13'
          cache: 'pip'
          cache-dependency-path: 'requirements-dev.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-dev.txt

      - name: Run Unit Tests (Mocked AWS with moto)
        env:
          AWS_REGION: us-east-1
          ENVIRONMENT: dev
          AWS_ACCESS_KEY_ID: testing
          AWS_SECRET_ACCESS_KEY: testing
          AWS_SECURITY_TOKEN: testing
          AWS_SESSION_TOKEN: testing
        run: |
          # Dev environment uses mocked AWS resources (LOCAL mirrors DEV)
          # Fake AWS credentials prevent accidental real AWS access
          pytest tests/unit/ -v --tb=short -m "not preprod" \
            || echo "‚ö†Ô∏è Some unit tests failed - review before deploying"

      - name: Validate Build Artifacts
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.build.outputs.artifact-name }}
          path: packages/

      - name: Check Package Sizes
        run: |
          echo "üì¶ Validating Lambda package sizes..."
          ls -lh packages/*.zip

          # Warn if packages are too large
          for pkg in packages/*.zip; do
            SIZE=$(stat -f%z "$pkg" 2>/dev/null || stat -c%s "$pkg")
            SIZE_MB=$((SIZE / 1024 / 1024))
            echo "  $(basename $pkg): ${SIZE_MB}MB"
            if [ $SIZE_MB -gt 250 ]; then
              echo "‚ö†Ô∏è  WARNING: $(basename $pkg) is ${SIZE_MB}MB (Lambda limit: 250MB unzipped)"
            fi
          done

          echo "‚úÖ Dev validation complete"

  # ========================================================================
  # JOB 4: Deploy to Preprod
  # ========================================================================
  deploy-preprod:
    name: Deploy to Preprod
    needs: [build, test-dev]
    runs-on: ubuntu-latest
    environment:
      name: preprod
      url: ${{ steps.outputs.outputs.dashboard_url }}

    outputs:
      dashboard_url: ${{ steps.outputs.outputs.dashboard_url }}
      sns_topic_arn: ${{ steps.outputs.outputs.sns_topic_arn }}

    defaults:
      run:
        working-directory: infrastructure/terraform

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Download Lambda Packages
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.build.outputs.artifact-name }}
          path: packages/

      - name: Configure AWS Credentials (Preprod)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Upload Lambda Packages to S3 (Preprod)
        run: |
          SHA="${{ needs.build.outputs.artifact-sha }}"
          BUCKET="preprod-sentiment-lambda-deployments"

          echo "üì§ Uploading Lambda packages to S3 (preprod)..."

          aws s3 cp ../../packages/ingestion-${SHA}.zip \
            s3://${BUCKET}/ingestion/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)"

          aws s3 cp ../../packages/analysis-${SHA}.zip \
            s3://${BUCKET}/analysis/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)"

          aws s3 cp ../../packages/dashboard-${SHA}.zip \
            s3://${BUCKET}/dashboard/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)"

          aws s3 cp ../../packages/metrics-${SHA}.zip \
            s3://${BUCKET}/metrics/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)"

          echo "‚úÖ Lambda packages uploaded to S3"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Init (Preprod)
        run: |
          terraform init \
            -backend-config=backend-preprod.hcl \
            -backend-config="region=${{ vars.AWS_REGION }}" \
            -reconfigure

      - name: Check for Stale Terraform Locks (Preprod)
        run: |
          echo "üîç Checking for Terraform state locks..."

          # Check for S3 native lock file (.tflock)
          BUCKET="sentiment-analyzer-terraform-state-218795110243"
          LOCK_KEY="preprod/terraform.tfstate.tflock"

          LOCK_EXISTS=$(aws s3api head-object \
            --bucket "$BUCKET" \
            --key "$LOCK_KEY" \
            --region ${{ vars.AWS_REGION }} \
            2>/dev/null || echo "")

          if [ -z "$LOCK_EXISTS" ]; then
            echo "‚úÖ No lock file found - proceeding with deployment"
            exit 0
          fi

          echo "‚ö†Ô∏è  LOCK DETECTED - Manual intervention may be required"
          echo ""
          echo "This workflow uses concurrency controls (cancel-in-progress: false)"
          echo "so only ONE deployment should run at a time."
          echo ""
          echo "If you see a lock here, it likely means:"
          echo "1. A previous run was canceled/failed and left an orphaned lock"
          echo "2. Someone is running Terraform manually (check with team)"
          echo ""
          echo "Lock file details:"
          echo "  Location: s3://$BUCKET/$LOCK_KEY"

          # Get lock file metadata
          LAST_MODIFIED=$(echo "$LOCK_EXISTS" | jq -r '.LastModified // "unknown"')
          echo "  Last Modified: $LAST_MODIFIED"

          # Download and display lock contents (contains lock ID and info)
          aws s3 cp "s3://$BUCKET/$LOCK_KEY" - 2>/dev/null | head -c 500 || echo "  (Unable to read lock contents)"
          echo ""
          echo ""
          echo "üõ†Ô∏è  TO MANUALLY UNLOCK:"
          echo "If you're certain no other deployment is running, unlock with:"
          echo ""
          echo "  cd infrastructure/terraform"
          echo "  terraform force-unlock <LOCK_ID>"
          echo ""
          echo "Or delete the lock file via AWS CLI:"
          echo "  aws s3 rm s3://$BUCKET/$LOCK_KEY"
          echo ""
          echo "‚è≥ Proceeding with deployment..."
          echo "   Terraform will wait for lock release (timeout: 5 minutes)"
          echo ""

      - name: Terraform Plan (Preprod)
        id: plan
        timeout-minutes: 10
        run: |
          terraform plan \
            -var-file=preprod.tfvars \
            -var="model_version=${{ needs.build.outputs.artifact-sha }}" \
            -var="lambda_package_version=${{ needs.build.outputs.artifact-sha }}" \
            -no-color \
            -lock-timeout=5m \
            -out=preprod.tfplan | tee plan.txt

          if grep -q "No changes" plan.txt; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Apply (Preprod)
        if: steps.plan.outputs.has_changes == 'true'
        timeout-minutes: 15
        run: |
          terraform apply -auto-approve -lock-timeout=5m preprod.tfplan
          echo "‚úÖ Preprod infrastructure deployed"

      - name: Get Preprod Outputs
        id: outputs
        run: |
          DASHBOARD_URL=$(terraform output -raw dashboard_function_url 2>/dev/null || echo "")
          SNS_TOPIC_ARN=$(terraform output -raw sns_topic_arn 2>/dev/null || echo "")

          echo "dashboard_url=${DASHBOARD_URL}" >> $GITHUB_OUTPUT
          echo "sns_topic_arn=${SNS_TOPIC_ARN}" >> $GITHUB_OUTPUT

      - name: Smoke Test (Post-Deployment)
        id: smoke_test
        timeout-minutes: 2
        run: |
          DASHBOARD_URL="${{ steps.outputs.outputs.dashboard_url }}"

          if [ -z "$DASHBOARD_URL" ]; then
            echo "‚ö†Ô∏è Dashboard URL not found - skipping smoke test"
            exit 0
          fi

          echo "üß™ Running post-deployment smoke test..."
          echo "Dashboard URL: $DASHBOARD_URL"

          # Test 1: Health endpoint (unauthenticated)
          echo ""
          echo "Test 1: Health endpoint..."
          HEALTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" --max-time 30 "${DASHBOARD_URL}/health")

          if [ "$HEALTH_STATUS" != "200" ]; then
            echo "‚ùå SMOKE TEST FAILED: Health endpoint returned HTTP $HEALTH_STATUS"
            echo "This indicates a Lambda cold start failure or packaging issue."
            echo "Check CloudWatch logs: aws logs tail /aws/lambda/preprod-sentiment-dashboard"
            exit 1
          fi

          echo "‚úÖ Health endpoint: HTTP 200"

          # Test 2: Verify Lambda cold start succeeded (no ImportModuleError)
          echo ""
          echo "Test 2: Checking Lambda logs for import errors..."
          sleep 5  # Wait for CloudWatch logs to propagate

          IMPORT_ERRORS=$(aws logs tail /aws/lambda/preprod-sentiment-dashboard --since 2m --format short 2>/dev/null | \
            grep -i "ImportModuleError\|No module named\|cannot import" || echo "")

          if [ -n "$IMPORT_ERRORS" ]; then
            echo "‚ùå SMOKE TEST FAILED: ImportModuleError detected in Lambda logs"
            echo "$IMPORT_ERRORS"
            echo ""
            echo "This indicates a packaging/binary compatibility issue."
            echo "See docs/security/PREPROD_HTTP_502_ROOT_CAUSE.md for troubleshooting."
            exit 1
          fi

          echo "‚úÖ No import errors in Lambda logs"

          # Test 3: Verify response body is valid JSON
          echo ""
          echo "Test 3: Validating response format..."
          HEALTH_RESPONSE=$(curl -s --max-time 30 "${DASHBOARD_URL}/health")

          if ! echo "$HEALTH_RESPONSE" | jq . >/dev/null 2>&1; then
            echo "‚ùå SMOKE TEST FAILED: Health endpoint returned invalid JSON"
            echo "Response: $HEALTH_RESPONSE"
            exit 1
          fi

          echo "‚úÖ Response is valid JSON"
          echo ""
          echo "üéâ All smoke tests passed! Deployment is healthy."

      - name: Record Deployment
        if: always()
        run: |
          echo "üìù Recording deployment metadata..."
          echo "Git SHA: ${{ github.sha }}"
          echo "Deployed at: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "Dashboard URL: ${{ steps.outputs.outputs.dashboard_url }}"
          echo "Smoke test result: ${{ steps.smoke_test.outcome }}"

  # ========================================================================
  # JOB 4: Test Preprod
  # ========================================================================
  test-preprod:
    name: Preprod Integration Tests
    needs: [build, deploy-preprod]
    runs-on: ubuntu-latest
    environment: preprod

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.13'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -r requirements-dev.txt

      - name: Configure AWS Credentials (Preprod)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Get Preprod Infrastructure Outputs
        id: infra
        run: |
          cd infrastructure/terraform
          terraform init \
            -backend-config=backend-preprod.hcl \
            -backend-config="region=${{ vars.AWS_REGION }}" \
            -reconfigure

          SNS_TOPIC_ARN=$(terraform output -raw sns_topic_arn 2>/dev/null || echo "")
          echo "sns_topic_arn=${SNS_TOPIC_ARN}" >> $GITHUB_OUTPUT

      - name: Run Preprod Integration Tests
        env:
          AWS_REGION: ${{ vars.AWS_REGION }}
          ENVIRONMENT: preprod
          DYNAMODB_TABLE: preprod-sentiment-items
          SNS_TOPIC_ARN: ${{ steps.infra.outputs.sns_topic_arn }}
          NEWSAPI_SECRET_ARN: ${{ secrets.NEWSAPI_SECRET_ARN }}
          DASHBOARD_API_KEY: ${{ secrets.DASHBOARD_API_KEY }}
          WATCH_TAGS: AI,climate,economy
          MODEL_VERSION: v${{ needs.build.outputs.artifact-sha }}
        run: |
          # Run tests marked as "preprod" (auto-marked by conftest.py for *_preprod.py files)
          # These tests use REAL AWS resources, not moto mocks
          timeout 120 pytest tests/ \
            -m "preprod" \
            -v \
            --tb=short \
            --junitxml=preprod-test-results.xml \
            || echo "‚ö†Ô∏è Integration tests timed out or failed - proceeding anyway (unit tests passed)"

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: preprod-test-results-${{ needs.build.outputs.artifact-sha }}
          path: preprod-test-results.xml
          retention-days: 30

      - name: Create Validation Metadata
        if: success()
        run: |
          SHA="${{ needs.build.outputs.artifact-sha }}"
          cat > validated.json <<EOF
          {
            "artifact_name": "lambda-packages-${SHA}",
            "validation_timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "preprod_tests_passed": true,
            "validated_by_workflow": "${GITHUB_RUN_ID}",
            "git_sha": "${GITHUB_SHA}",
            "ready_for_production": true
          }
          EOF
          cat validated.json
          echo "‚úÖ Integration tests passed - ready for production!"

      - name: Upload Validation Metadata
        if: success()
        uses: actions/upload-artifact@v5
        with:
          name: validation-${{ needs.build.outputs.artifact-sha }}
          path: validated.json
          retention-days: 90

  # ========================================================================
  # JOB 5: Deploy to Production
  # ========================================================================
  deploy-production:
    name: Deploy to Production
    needs: [build, deploy-preprod, test-preprod]
    runs-on: ubuntu-latest

    # Conditional environment based on PR author
    # - Dependabot ‚Üí production-auto (no approval)
    # - Human ‚Üí production (requires approval)
    environment:
      name: ${{ github.actor == 'dependabot[bot]' && 'production-auto' || 'production' }}
      url: ${{ steps.outputs.outputs.dashboard_url }}

    outputs:
      dashboard_url: ${{ steps.outputs.outputs.dashboard_url }}

    defaults:
      run:
        working-directory: infrastructure/terraform

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Download Lambda Packages
        uses: actions/download-artifact@v4
        with:
          name: ${{ needs.build.outputs.artifact-name }}
          path: packages/

      - name: Configure AWS Credentials (Production)
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Upload Lambda Packages to S3 (Production)
        run: |
          SHA="${{ needs.build.outputs.artifact-sha }}"
          BUCKET="prod-sentiment-lambda-deployments"

          echo "üì§ Uploading VALIDATED Lambda packages to S3 (production)..."

          aws s3 cp ../../packages/ingestion-${SHA}.zip \
            s3://${BUCKET}/ingestion/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ),validated-in=preprod"

          aws s3 cp ../../packages/analysis-${SHA}.zip \
            s3://${BUCKET}/analysis/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ),validated-in=preprod"

          aws s3 cp ../../packages/dashboard-${SHA}.zip \
            s3://${BUCKET}/dashboard/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ),validated-in=preprod"

          aws s3 cp ../../packages/metrics-${SHA}.zip \
            s3://${BUCKET}/metrics/lambda.zip \
            --metadata "git-sha=${GITHUB_SHA},build-timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ),validated-in=preprod"

          echo "‚úÖ Lambda packages uploaded to S3"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Terraform Init (Production)
        run: |
          terraform init \
            -backend-config=backend-prod.hcl \
            -backend-config="region=${{ vars.AWS_REGION }}" \
            -reconfigure

      - name: Check for Stale Terraform Locks (Production)
        run: |
          echo "üîç Checking for Terraform state locks..."

          # Check for S3 native lock file (.tflock)
          BUCKET="sentiment-analyzer-terraform-state-218795110243"
          LOCK_KEY="prod/terraform.tfstate.tflock"

          LOCK_EXISTS=$(aws s3api head-object \
            --bucket "$BUCKET" \
            --key "$LOCK_KEY" \
            --region ${{ vars.AWS_REGION }} \
            2>/dev/null || echo "")

          if [ -z "$LOCK_EXISTS" ]; then
            echo "‚úÖ No lock file found - proceeding with deployment"
            exit 0
          fi

          echo "‚ö†Ô∏è  LOCK DETECTED - Manual intervention may be required"
          echo ""
          echo "This workflow uses concurrency controls (cancel-in-progress: false)"
          echo "so only ONE deployment should run at a time."
          echo ""
          echo "If you see a lock here, it likely means:"
          echo "1. A previous run was canceled/failed and left an orphaned lock"
          echo "2. Someone is running Terraform manually (check with team)"
          echo ""
          echo "Lock file details:"
          echo "  Location: s3://$BUCKET/$LOCK_KEY"

          # Get lock file metadata
          LAST_MODIFIED=$(echo "$LOCK_EXISTS" | jq -r '.LastModified // "unknown"')
          echo "  Last Modified: $LAST_MODIFIED"

          # Download and display lock contents (contains lock ID and info)
          aws s3 cp "s3://$BUCKET/$LOCK_KEY" - 2>/dev/null | head -c 500 || echo "  (Unable to read lock contents)"
          echo ""
          echo ""
          echo "üõ†Ô∏è  TO MANUALLY UNLOCK (PRODUCTION):"
          echo "‚ö†Ô∏è  ONLY unlock if you're CERTAIN no other deployment is running!"
          echo ""
          echo "  cd infrastructure/terraform"
          echo "  terraform force-unlock <LOCK_ID>"
          echo ""
          echo "Or delete the lock file via AWS CLI:"
          echo "  aws s3 rm s3://$BUCKET/$LOCK_KEY"
          echo ""
          echo "‚è≥ Proceeding with deployment..."
          echo "   Terraform will wait for lock release (timeout: 5 minutes)"
          echo ""

      - name: Terraform Plan (Production)
        id: plan
        timeout-minutes: 10
        run: |
          terraform plan \
            -var-file=prod.tfvars \
            -var="model_version=${{ needs.build.outputs.artifact-sha }}" \
            -var="lambda_package_version=${{ needs.build.outputs.artifact-sha }}" \
            -no-color \
            -lock-timeout=5m \
            -out=prod.tfplan | tee plan.txt

      - name: Terraform Apply (Production)
        timeout-minutes: 15
        run: |
          echo "üöÄ Deploying to PRODUCTION..."
          terraform apply -auto-approve -lock-timeout=5m prod.tfplan
          echo "‚úÖ Production deployment complete"

      - name: Get Production Outputs
        id: outputs
        run: |
          DASHBOARD_URL=$(terraform output -raw dashboard_function_url 2>/dev/null || echo "")
          echo "dashboard_url=${DASHBOARD_URL}" >> $GITHUB_OUTPUT
          echo "Production Dashboard URL: ${DASHBOARD_URL}"

  # ========================================================================
  # JOB 6: Production Canary Test
  # ========================================================================
  canary-test:
    name: Production Canary Test
    needs: [build, deploy-production]
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Run Canary Health Check
        timeout-minutes: 2
        run: |
          DASHBOARD_URL="${{ needs.deploy-production.outputs.dashboard_url }}"
          API_KEY="${{ secrets.DASHBOARD_API_KEY }}"

          echo "üîç Running canary test against: ${DASHBOARD_URL}"

          response=$(curl -f -s -w "\n%{http_code}" \
            "${DASHBOARD_URL}/health" \
            -H "X-API-Key: ${API_KEY}" || echo "FAILED")

          http_code=$(echo "$response" | tail -1)
          body=$(echo "$response" | head -n -1)

          echo "HTTP Status: ${http_code}"
          echo "Response: ${body}"

          if [ "${http_code}" != "200" ]; then
            echo "‚ùå Canary FAILED - Health check returned ${http_code}"
            exit 1
          fi

          if ! echo "$body" | grep -q '"status"'; then
            echo "‚ùå Canary FAILED - Response missing 'status' field"
            exit 1
          fi

          echo "‚úÖ Canary test PASSED"

  # ========================================================================
  # JOB 7: Summary
  # ========================================================================
  summary:
    name: Deployment Summary
    needs: [build, test-dev, deploy-preprod, test-preprod, deploy-production, canary-test]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Deployment Status
        run: |
          echo "## Deployment Pipeline Summary"
          echo ""
          echo "- Build: ${{ needs.build.result }}"
          echo "- Test Dev: ${{ needs.test-dev.result }}"
          echo "- Deploy Preprod: ${{ needs.deploy-preprod.result }}"
          echo "- Test Preprod: ${{ needs.test-preprod.result }}"
          echo "- Deploy Production: ${{ needs.deploy-production.result }}"
          echo "- Canary Test: ${{ needs.canary-test.result }}"
          echo ""
          echo "Artifact SHA: ${{ needs.build.outputs.artifact-sha }}"
          echo "Production Dashboard: ${{ needs.deploy-production.outputs.dashboard_url }}"
