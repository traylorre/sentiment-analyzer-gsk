# Feature Specification: X-Ray Exclusive Tracing

**Feature Branch**: `1219-xray-exclusive-tracing`
**Created**: 2026-02-14
**Status**: Draft (Round 10)
**Input**: Full X-Ray trace coverage across entire sentiment-analyzer-gsk system. Consolidate all custom logging, custom correlation IDs, and missing trace propagation onto X-Ray exclusively. Single exception: one non-X-Ray canary to validate X-Ray itself is operational.
**Round 2 Emergent Issues**: (1) SSE Lambda RESPONSE_STREAM mode closes X-Ray segment before streaming completes — orphaned subsegments, (2) asyncio.new_event_loop() breaks X-Ray context propagation in async-to-sync bridge, (3) SendGrid uses urllib not httpx — NOT auto-patched by X-Ray SDK, (4) Powertools Tracer vs raw xray_recorder inconsistency across Lambdas causes silent exception loss in traces.
**Round 3 Emergent Issues**: (1) `begin_segment()` is a documented no-op in Lambda — the "Two-Phase Architecture" proposed in Round 2 is **INVALID**, (2) Powertools `@tracer.capture_method` silently mishandles async generators — falls through to sync wrapper, (3) `EventSource` API does not support custom HTTP headers per WHATWG spec — trace header propagation for SSE requires `fetch()` + `ReadableStream`, (4) Clients can force 100% X-Ray sampling via `Sampled=1` header — cost amplification attack vector, (5) X-Ray has no native "guaranteed capture on error" mode — sampling decisions made before request outcome is known, (6) X-Ray silently drops data at 2,600 segments/sec region limit — no alert on data loss, (7) X-Ray SDK `AsyncContext` loses context across event loop boundaries — must use default `threading.local()` context.
**Round 4 Emergent Issues**: (1) Audit reveals 2 Lambda functions (SSE Streaming, Metrics) have zero CloudWatch error alarms — operators never alerted to failures that X-Ray traces could diagnose, (2) 7 custom metrics emitted without alarms (StuckItems, ConnectionAcquireFailures, EventLatencyMs, MetricsLambdaErrors, HighLatencyAlert, PollDurationMs, AnalysisErrors) — failures invisible to operators, (3) X-Ray Groups only operate on already-sampled traces — at production sampling <100%, error monitoring via trace data misses unsampled errors; CloudWatch metrics required for 100% error alarming, (4) ADOT auto-instrumentation (`AWS_LAMBDA_EXEC_WRAPPER`) conflicts with Powertools Tracer — double-patches botocore, duplicates handler wrapping; must use sidecar-only mode, (5) CloudFront removed from architecture (Features 1203-1207) — browser-to-backend trace propagation works without intermediary; re-adding CloudFront would break it (CloudFront treats `X-Amzn-Trace-Id` as restricted header, replaces client value), (6) X-Ray has no native span links — SSE reconnection trace correlation requires annotation-based pattern (`session_id`, `previous_trace_id`), (7) CloudWatch `put_metric_data` failure makes all `treat_missing_data=notBreaching` alarms false-green — canary must verify CloudWatch emission health in addition to X-Ray health; requires separate IAM role and out-of-band alerting.
**Round 5 Emergent Issues**: (1) OTel SDK on SSE Lambda requires explicit configuration of `AwsXRayLambdaPropagator` to read `_X_AMZN_TRACE_ID` environment variable — without this, streaming-phase OTel spans carry different trace IDs than the Lambda runtime's X-Ray facade segment, creating disconnected traces that break SC-010, (2) OTel `TracerProvider` MUST use `AwsXRayIdGenerator` — standard `RandomIdGenerator` produces garbage timestamps in X-Ray trace ID epoch field, causing X-Ray to misindex traces by time window, (3) OTel OTLP exporter endpoint (`localhost:4318`) must be explicitly configured — SDK does not auto-discover the ADOT Extension, (4) `BatchSpanProcessor.force_flush()` MUST be called in the streaming generator's `finally` block — without it, spans buffered between batch intervals are lost when the Lambda execution environment freezes, (5) X-Ray segment document size limit is 64KB per emitted document — large metadata payloads are silently rejected by `PutTraceSegments` API, (6) OTel sampler must be `parentbased_always_on` to honor Lambda runtime's sampling decision — using `always_on` creates orphaned spans for unsampled (`Sampled=0`) invocations, (7) 4 specific OTel Python packages required but not enumerated in spec or requirements files.
**Round 6 Emergent Issues**: (1) OTel trace context MUST be extracted per-invocation (not at module level) — on warm invocations `_X_AMZN_TRACE_ID` changes, but module-level `propagate.extract()` returns the stale first invocation's context, linking ALL subsequent invocations' spans to the wrong trace, (2) Powertools Tracer's `auto_patch=True` globally patches boto3 at import time — during SSE streaming phase, every auto-patched boto3 call creates BOTH an X-Ray subsegment (via X-Ray daemon, unreliable after handler returns) AND an OTel span (via ADOT, reliable), producing duplicate entries in X-Ray, (3) SSE Lambda MUST use `auto_patch=False` to prevent dual-emission — handler-phase AWS SDK calls traced via explicit `@tracer.capture_method` instead of global patching, (4) Existing CloudWatch alarm thresholds not reviewed — audit Section 6.1 identifies `analysis_latency_high` at 25s (42% of 60s timeout) as too generous; Task 17 adds new alarms at 80-90% but doesn't recalibrate existing alarms.
**Round 7 Emergent Issues**: (1) ADOT Lambda Extension has two layer types — collector-only (`aws-otel-collector-*`, ~35MB) and Python SDK (`aws-otel-python-*`, ~80MB); sidecar-only mode MUST use collector-only layer to avoid unused auto-instrumentation code and ~45MB memory waste, (2) OTel Python SDK `BatchSpanProcessor` silently catches ALL exporter exceptions and `force_flush()` returns `True` regardless of export outcome — this is by OTel specification design (exporters must not crash applications) and CANNOT be overridden; ADOT Extension health is monitored exclusively via the canary, (3) `TracerProvider` MUST be module-level singleton — creating per-invocation leaks `BatchSpanProcessor` daemon threads (~1MB each) causing monotonic memory growth until OOM; FR-059's per-invocation requirement applies ONLY to `propagate.extract()`, NOT to `TracerProvider`, (4) OTel span attributes MUST use semantic conventions that ADOT maps to native X-Ray fields (`db.system`, `rpc.service`, `exception.type`) — non-standard names become non-indexed metadata invisible in X-Ray console, (5) ADOT Extension layer ARN MUST be version-pinned in Terraform — unpinned layers can silently introduce breaking collector schema changes on `terraform apply`, (6) SSE Lambda MUST NOT set `OTEL_PROPAGATORS` env var — `set_global_textmap()` unconditionally overwrites it, creating misleading visible-but-inactive configuration.
**Round 9 Emergent Issues**: (1) ADOT Extension has KNOWN ~30% span drop rate when collector context is canceled before HTTP exports complete (GitHub aws-otel-lambda#886, opentelemetry-lambda#224) — force_flush() handles SDK→Extension hop but Extension→X-Ray backend is a KNOWN race condition mitigated by the decouple processor; spec must acknowledge two-hop flush architecture, (2) Spec Round 5 assumption states "default batch timeout of 200ms" — INCORRECT; actual BSP schedule_delay_millis default is 5000ms (5 seconds); FR-073 overrides to 1000ms so runtime is correct but assumption is WRONG, (3) FR-060 does not explicitly prohibit OTel BotocoreInstrumentor().instrument() — future implementer could enable it alongside manual spans recreating double-instrumentation; must be explicitly prohibited, (4) TracerProvider(shutdown_on_exit=True) default registers atexit handler that races with ConcurrentMultiSpanProcessor thread pool on environment recycling (opentelemetry-python#4461); FR-065 must specify shutdown_on_exit=False, (5) X-Ray trace propagation delay is "generally 30 seconds" but non-deterministic; FR-019 canary must use retry-with-backoff (30s then 60s) not single-shot query; BatchGetTraces cannot distinguish "not yet indexed" from "trace lost", (6) FR-049 does not classify put_metric_data errors as transient vs permanent — IAM revocation (the exact failure FR-051 detects) would be retried instead of immediately escalated via FR-050; error taxonomy required, (7) FR-033 assumes SSE Lambda server checks Last-Event-ID for stream resumption — if server ignores it, client-side propagation is hollow; server-side support must be specified, (8) FR-048 requires client to read response X-Amzn-Trace-Id but CORS ExposeHeaders does not include it — hard prerequisite for reconnection trace correlation, (9) ReadableStreamDefaultReader.closed is canonical way to distinguish graceful close from network error — FR-033 reconnection should use different strategies per close type.
**Round 10 Emergent Issues**: (1) Lambda streaming continues after client disconnect — AWS docs confirm streamed responses are NOT interrupted when client disconnects; Python custom runtimes have no standard disconnect detection mechanism (unlike Node.js StreamifyResponse metadata.onAbort); BrokenPipeError on write is the only signal; SSE Lambda continues DynamoDB polling, span creation, and force_flush() for phantom streams, (2) BatchSpanProcessor max_queue_size=512 (FR-073) is INSUFFICIENT — 15-second streaming Lambda at ~15 polls/sec with ~3 spans/poll produces ~675 spans; exceeds 512 queue capacity; BSP silently drops spans via queue.put_nowait() Full exception caught as warning; recalculation required, (3) X-Ray IsPartial flag not checked by canary — X-Ray marks traces as partial (IsPartial=true in TraceSummary) when not all segments received; canary (FR-078) checks trace existence but not completeness; partial traces with missing ADOT streaming spans appear "found," (4) Lambda Function URL streaming breaks silently in VPC — responses buffered instead of streamed; SSE Lambda MUST NOT be deployed in VPC, (5) SSE retry: field not emitted by server — SSE spec (WHATWG HTML Living Standard §9.2) defines server-side reconnection control; without it clients use implementation-defined defaults; fetch()+ReadableStream does NOT auto-parse retry: like EventSource, (6) OTel streaming semantic conventions are still undefined (marked "TODO" in OTel semconv) — project's span-per-poll-cycle convention is project-specific with no standard to align to, (7) Decouple processor auto-configuration only applies to Lambda Layer deployments, NOT container-based custom configs — must be explicitly included in custom collector YAML, (8) AwsXRayLambdaPropagator extracts context even when Sampled=0 (opentelemetry-lambda#1782) — if Active Tracing accidentally disabled, ALL spans silently suppressed, (9) X-Ray centralized sampling fallback to TraceIDRatioBased (1 trace/sec) when sampling API unreachable — dev/preprod 100% sampling silently degrades.
**Round 8 Emergent Issues**: (1) SSE Lambda is **container-based** (ECR image with `python:3.13-slim` + custom bootstrap) — Lambda container images CANNOT use Lambda Layers (AWS docs: "You cannot add Lambda layers to container images"); FR-062 "collector-only layer" and FR-063 "version-pinned layer ARN" are **INVALID as written** — ADOT must be embedded in Docker image via multi-stage build from `public.ecr.aws/aws-observability/aws-otel-lambda-extension-amd64`; FR-062/FR-063 REWRITTEN, FR-069/FR-070 added for container-based deployment, (2) OTel `AwsLambdaResourceDetector` does NOT set `service.name` — defaults to `unknown_service`, which X-Ray uses as the service map node name, making the SSE Lambda indistinguishable from any other unconfigured service; `OTEL_SERVICE_NAME` env var is MANDATORY (FR-071), (3) `AwsLambdaResourceDetector` not specified in TracerProvider configuration — missing `cloud.provider`, `cloud.region`, `faas.name` resource attributes on all spans; FR-072 added, (4) `BatchSpanProcessor` default `schedule_delay_millis=5000` and `max_queue_size=2048` are designed for long-running servers, not Lambda — Lambda-tuned configuration required (FR-073), (5) Round 7 assumption about "zero Lambda layers" and "250MB deployment limit" based on WRONG deployment model — SSE Lambda is container-based with 10GB image limit, not ZIP with layer support; assumption INVALIDATED.

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Operator Traces a Request End-to-End in X-Ray Console (Priority: P1)

An operator investigating a slow or failed request opens the AWS X-Ray console and sees a complete service map and trace from browser through API Gateway, through every Lambda invocation, through every downstream call (DynamoDB, SNS, SendGrid), and back. No gaps. No disconnected segments. The operator never needs to leave X-Ray to understand what happened.

**Why this priority**: This is the core value proposition. Without end-to-end traces, operators must correlate across multiple tools (CloudWatch Logs Insights, custom correlation IDs, dashboard widgets), which costs investigation time during incidents and increases mean-time-to-resolution (MTTR).

**Independent Test**: Can be fully tested by triggering a single user request (e.g., ticker search) and verifying in the X-Ray console that one continuous trace spans from RUM through API Gateway through Dashboard Lambda through DynamoDB. Delivers immediate operator value.

**Acceptance Scenarios**:

1. **Given** a user searches for ticker "AAPL" in the browser, **When** the operator opens X-Ray traces for that time window, **Then** a single trace ID links the browser RUM segment, the API Gateway segment, the Dashboard Lambda segment (with all subsegments for auth, alerts, DynamoDB calls), and the DynamoDB segment.
2. **Given** the Ingestion Lambda processes a batch of articles and publishes to SNS, **When** the Analysis Lambda is invoked by that SNS message, **Then** both Lambda invocations appear under the same X-Ray trace ID with the SNS hop visible as a connecting segment.
3. **Given** the Notification Lambda sends an email via SendGrid, **When** the operator views the trace, **Then** the SendGrid HTTP call appears as an explicitly instrumented subsegment (not auto-instrumented — SendGrid's HTTP transport is not auto-patched) with status code, duration, and recipient count.
4. **Given** a user connects to the SSE streaming endpoint, **When** the operator views the trace, **Then** the connection lifecycle (authentication, connection acquisition, DynamoDB polling, event serialization) appears as named subsegments with timing data.
5. **(Round 3)** **Given** a request that results in an error/fault under any sampling configuration, **When** the operator searches X-Ray for errored traces, **Then** the complete trace is present including all subsegments and annotations — errored requests are never lost to sampling.

---

### User Story 2 - Operator Diagnoses Silent Failure via X-Ray Error Annotations (Priority: P1)

An operator notices stale dashboard data. Instead of grepping CloudWatch Logs, the operator filters X-Ray traces by error/fault status and immediately sees which subsegment failed — circuit breaker persistence, audit trail write, SNS notification publish, time-series fanout partial write, or self-healing item fetch.

**Why this priority**: Equal to P1 because silent failure paths are the most dangerous blind spots. The audit identified 7 silent failure paths where errors are caught, logged, but emit no metric and create no X-Ray subsegment. These paths currently require log-grepping to diagnose.

**Independent Test**: Can be tested by injecting a DynamoDB throttle on the circuit breaker table and verifying that the X-Ray trace shows an error annotation on the circuit breaker subsegment with the throttle details.

**Acceptance Scenarios**:

1. **Given** DynamoDB throttles the circuit breaker table during Ingestion Lambda execution, **When** the circuit breaker load or save fails, **Then** the X-Ray trace contains a subsegment named "circuit_breaker_load" or "circuit_breaker_save" marked as error with the exception details.
2. **Given** the audit trail DynamoDB write fails during ingestion, **When** the operator filters X-Ray traces by fault, **Then** a subsegment named "audit_trail_persist" appears with the ClientError details.
3. **Given** SNS publish fails in the notification publisher, **When** the operator views the trace, **Then** a subsegment named "downstream_notification_publish" is marked as fault with the SNS error.
4. **Given** a BatchWriteItem has unprocessed items after retries during time-series fanout, **When** the operator views the trace, **Then** a subsegment named "timeseries_fanout_batch_write" shows an error annotation with the count of unprocessed items and affected resolutions.
5. **Given** self-healing skips an item due to a fetch failure, **When** the operator views the trace, **Then** a subsegment named "self_healing_item_fetch" is marked as error with the failed source_id and exception.

---

### User Story 3 - Operator Views SSE Streaming Latency and Cache Performance in X-Ray (Priority: P2)

An operator investigating SSE live update latency opens X-Ray and sees per-event latency as subsegment durations and cache hit rate as annotations — without needing CloudWatch Logs Insights pctile() queries.

**Why this priority**: SSE streaming currently has only 1 X-Ray subsegment (stream_status) despite being the most latency-sensitive Lambda. All latency and cache data is in custom structured logs, requiring a separate CloudWatch Logs Insights query tool. Consolidating onto X-Ray provides single-pane observability.

**Architectural constraint (Round 2 finding, revised in Round 3)**: The SSE Lambda uses `RESPONSE_STREAM` invoke mode with Lambda Function URLs. The Lambda runtime's auto-created X-Ray segment closes when the handler returns the generator — but streaming continues after that. All DynamoDB polling, event dispatch, and CloudWatch metric calls happen DURING streaming, AFTER the segment closes.

~~Round 2 proposed creating "independent X-Ray segments" during streaming linked to the original trace ID.~~ **INVALIDATED in Round 3**: The X-Ray SDK's `begin_segment()` is a documented no-op in Lambda. The SDK's `LambdaContext.put_segment()` silently discards segments (source: `aws_xray_sdk/core/lambda_launcher.py:55-59`), and the `FacadeSegment` raises `FacadeSegmentMutationException` on all mutation operations. Independent segments CANNOT be created within Lambda.

**Round 3 corrected approach**: The system MUST use a tracing mechanism with a lifecycle independent of the Lambda runtime's X-Ray segment. This mechanism must create trace spans during response streaming, link them to the original invocation's trace ID, and export them to X-Ray. The mechanism runs as a separate process within the Lambda execution environment (e.g., a Lambda Extension) that survives after the handler returns and has its own shutdown phase for flushing buffered trace data.

**Independent Test**: Can be tested by connecting to the SSE streaming endpoint, waiting for events, and verifying X-Ray traces contain latency annotations and cache hit rate annotations on spans linked to the connection's trace ID.

**Acceptance Scenarios**:

1. **Given** the SSE Lambda processes a DynamoDB change and sends an event to a connected client, **When** the operator views the trace, **Then** a span named "sse_event_dispatch" contains annotations for `event_type`, `latency_ms`, and `is_cold_start`, linked to the original invocation trace ID.
2. **Given** the SSE Lambda polls DynamoDB for sentiment updates, **When** the operator views the trace, **Then** a span named "dynamodb_poll" contains annotations for `item_count`, `changed_count`, and `poll_duration_ms`.
3. **Given** the SSE Lambda cache reaches a periodic logging trigger, **When** the operator views the trace, **Then** the current span contains annotations for `cache_hit_rate`, `cache_entry_count`, and `cache_max_entries`.
4. **Given** a new SSE connection is acquired, **When** the operator views the trace, **Then** a span named "connection_acquire" contains annotations for `connection_id`, `current_count`, and `max_connections`.

---

### User Story 4 - Operator Traces Browser-to-Backend via X-Ray Headers (Priority: P2)

An operator investigating a specific user's slow experience uses the RUM trace ID to find the exact backend trace. The browser's X-Ray trace ID propagates through the frontend API client and SSE proxy into backend Lambdas, creating one continuous trace from browser to database.

**Why this priority**: Currently, CloudWatch RUM captures browser traces and backend Lambdas capture server traces, but they are disconnected. The frontend does not propagate `X-Amzn-Trace-Id` headers, breaking the client-to-server correlation chain.

**Architectural constraint (Round 3 finding)**: The standard `EventSource` API does not support custom HTTP headers (WHATWG HTML Living Standard, Section 9.2). CloudWatch RUM's automatic trace header injection only patches `window.fetch()`, not `EventSource`. To propagate `X-Amzn-Trace-Id` headers on SSE connections, the frontend MUST use `fetch()` with `ReadableStream` consumption instead of `EventSource`. This loses `EventSource`'s built-in auto-reconnection, which must be reimplemented.

**Independent Test**: Can be tested by making a browser API request, extracting the RUM trace ID, and verifying the same trace ID appears in the backend Lambda's X-Ray trace.

**Acceptance Scenarios**:

1. **Given** a browser makes an API call to the Dashboard endpoint, **When** the frontend API client sends the request, **Then** the request includes an `X-Amzn-Trace-Id` header containing the active RUM trace ID.
2. **Given** a browser connects to the SSE endpoint via the Next.js proxy, **When** the proxy forwards the request to the SSE Lambda, **Then** the proxy propagates the `X-Amzn-Trace-Id` header from the incoming request to the upstream SSE Lambda call.
3. **Given** the CORS configuration for both API Gateway and Lambda Function URLs, **When** a browser sends an `X-Amzn-Trace-Id` header, **Then** the CORS preflight response includes `X-Amzn-Trace-Id` in `Access-Control-Allow-Headers`.
4. **(Round 3)** **Given** the frontend SSE client uses `fetch()` + `ReadableStream` instead of `EventSource`, **When** the connection drops, **Then** the client reconnects with exponential backoff, jitter, and `Last-Event-ID` header propagation — matching the reliability guarantees of `EventSource` auto-reconnection.

---

### User Story 5 - Operator Replaces Custom Correlation IDs with X-Ray Trace IDs (Priority: P3)

Operators and automated systems that currently search for custom `{source_id}-{request_id}` correlation IDs in CloudWatch Logs instead use X-Ray trace IDs as the universal correlation key. The custom correlation ID system is removed.

**Why this priority**: The custom correlation system in `metrics.py:get_correlation_id()` creates a parallel tracing universe. Operators must know to search both X-Ray and CloudWatch Logs with different ID formats. Consolidating on X-Ray trace IDs as the single correlation key eliminates this cognitive overhead.

**Independent Test**: Can be tested by processing an article through ingestion, finding the X-Ray trace ID, and verifying that the same trace ID appears in structured log output as the correlation key (replacing the old `{source}-{request_id}` format).

**Acceptance Scenarios**:

1. **Given** the Ingestion Lambda processes an article, **When** it emits structured logs, **Then** the log field `correlation_id` contains the active X-Ray trace ID (format: `1-{hex_timestamp}-{hex_id}`) instead of the custom `{source_id}-{request_id}` format.
2. **Given** the custom `get_correlation_id()` function and `generate_correlation_id()` function, **When** this feature is complete, **Then** both functions are removed and all call sites use the X-Ray trace ID instead.
3. **Given** any Lambda function emitting structured logs, **When** the log entry is written, **Then** the X-Ray trace ID is automatically included as a top-level field so CloudWatch Logs Insights queries can join with X-Ray traces.

---

### User Story 6 - X-Ray Canary Validates Tracing Infrastructure Health (Priority: P3)

A lightweight, non-X-Ray canary periodically verifies that X-Ray itself is operational and that trace data is not being silently lost. This is the single permitted exception to the "X-Ray exclusive" rule — the "watcher of the watcher" that detects X-Ray regional degradation or throttling-induced data loss before it causes cascading alarm blindness.

**Why this priority**: The audit identified that if CloudWatch `put_metric_data` fails, the entire observability system goes dark. Analogously, if X-Ray ingestion fails or is throttled, all the new tracing coverage becomes invisible. A canary that does NOT depend on X-Ray must verify X-Ray is working.

**Independent Test**: Can be tested by running the canary and verifying it reports X-Ray health status via a non-X-Ray channel.

**Acceptance Scenarios**:

1. **Given** the canary runs on its scheduled interval, **When** it submits a known test trace to X-Ray and queries for it, **Then** within the expected propagation window, the trace is retrievable and the canary reports success.
2. **Given** X-Ray ingestion is degraded (simulated by IAM permission revocation in test), **When** the canary's test trace is not retrievable within the expected window, **Then** the canary reports failure via a non-X-Ray channel (CloudWatch metric with `treat_missing_data = breaching`).
3. **Given** the canary itself fails to run, **When** the expected canary metric is absent, **Then** the CloudWatch alarm with `treat_missing_data = breaching` fires, alerting the operator that the watcher is down.
4. **(Round 3)** **Given** the canary submits N test traces per interval, **When** fewer than N are retrievable within the query window, **Then** the canary reports a `trace_data_loss_ratio` metric, enabling detection of partial data loss from X-Ray throttling.

---

### User Story 7 - Metrics Lambda Gains Full X-Ray Instrumentation (Priority: P2)

The Metrics Lambda — which is the monitoring system's core — gains X-Ray instrumentation so that failures in the monitoring system itself are traceable. Currently, the Metrics Lambda has zero X-Ray SDK integration despite having Active tracing enabled in Terraform.

**Why this priority**: The Metrics Lambda is the "monitor of monitors." If it fails, custom metrics stop being emitted, and alarms that depend on those metrics either fire incorrectly or resolve to green (depending on `treat_missing_data`). Having no X-Ray visibility into its execution makes diagnosing these failures impossible.

**Independent Test**: Can be tested by invoking the Metrics Lambda and verifying X-Ray shows subsegments for DynamoDB queries and CloudWatch metric emission.

**Acceptance Scenarios**:

1. **Given** the Metrics Lambda is invoked, **When** it queries DynamoDB for stuck items, **Then** the X-Ray trace contains a subsegment for the DynamoDB query with item count and duration.
2. **Given** the Metrics Lambda emits CloudWatch metrics, **When** the `put_metric_data` call succeeds, **Then** the X-Ray trace contains a subsegment for the CloudWatch API call.
3. **Given** the Metrics Lambda's `put_metric_data` call fails (throttled, permission denied), **When** the operator views the trace, **Then** the subsegment is marked as error with the exception details.

---

### User Story 8 - Guaranteed Error Trace Capture (Priority: P1) *(Round 3 — New)*

An operator investigating a production error is guaranteed to find an X-Ray trace for that errored request. The system ensures that all error/fault traces are captured and queryable, regardless of traffic volume or sampling configuration. Successful requests may be sampled, but errors are always traced.

**Why this priority**: X-Ray sampling decisions are made BEFORE the request completes (at the entry point, the outcome is unknown). Under default sampling (1 request/second + 5%), most errored requests in a high-traffic window would be untraced. For a system that uses X-Ray as its EXCLUSIVE debugging tool, losing error traces makes incidents undiagnosable. This is a P1 because it directly affects the viability of Story 1 and Story 2 during real incidents.

**Independent Test**: Can be tested by configuring sampling, generating 100 requests that result in errors, and verifying that 100% of errored traces are retrievable in X-Ray.

**Acceptance Scenarios**:

1. **Given** the system is running with production sampling rules, **When** a Lambda invocation results in an error/fault, **Then** the complete trace (including all subsegments and annotations) is captured and queryable in X-Ray.
2. **Given** an X-Ray Group configured with the filter `fault = true OR error = true`, **When** errored traces are captured, **Then** the group generates CloudWatch metrics (error count, error rate, latency percentiles), enabling alarm-based monitoring from trace data.
3. **Given** dev/preprod environments, **When** any request is made, **Then** 100% of requests are traced (no sampling), ensuring complete debuggability during development.
4. **Given** the production sampling rate controls cost for successful requests, **When** the monthly X-Ray cost exceeds configured thresholds ($10, $25, $50), **Then** a CloudWatch billing alarm fires.

---

### User Story 9 - Trace Data Integrity Protection (Priority: P2) *(Round 3 — New)*

An operator is alerted when trace data is being silently lost due to X-Ray throttling, daemon failure, or systematic SDK errors. The system treats undetected trace data loss as a first-class failure mode, because an operator who trusts X-Ray traces that aren't there will misdiagnose issues.

**Why this priority**: X-Ray has a hard throughput limit (2,600 segments/second/region). When exceeded, `PutTraceSegments` returns throttled segments in `UnprocessedTraceSegments` — data is silently lost, not queued, no alarm fires. If the system's EXCLUSIVE debugging tool is losing data without alerting, operators have a false sense of complete observability.

**Independent Test**: Can be tested by configuring the canary to track submission-vs-retrieval ratios and verifying that simulated data loss (temporary IAM revocation or throughput saturation) triggers an alert within 2 canary intervals.

**Acceptance Scenarios**:

1. **Given** the X-Ray canary submits N test traces per interval, **When** fewer than N traces are retrievable within the query window, **Then** the canary emits a `trace_data_loss_ratio` metric and the CloudWatch alarm fires when the ratio exceeds the configured threshold.
2. **Given** the system approaches the X-Ray region throughput limit, **When** the canary detects consecutive intervals with data loss, **Then** an operator is alerted to investigate throttling or daemon health.

---

### User Story 10 - Operator Is Alerted to All Failure Modes Regardless of X-Ray Sampling (Priority: P1) *(Round 4 — New)*

An operator is automatically alerted when any Lambda function errors, when latency breaches SLO thresholds, or when critical application metrics indicate degradation — regardless of whether the specific request was X-Ray sampled. CloudWatch alarms on Lambda built-in metrics and custom application metrics provide the 24/7 alerting signal; X-Ray traces provide the diagnostic context when operators investigate.

**Why this priority**: The audit identified 2 Lambda functions (SSE Streaming, Metrics) with zero error alarms and 7 custom metrics emitted without alarms. Without alarms, operators only discover these failures from customer complaints or by staring at dashboards. This directly undermines the value of X-Ray traces — operators don't know to look at X-Ray if nothing alerts them to a problem.

**Scope clarification**: "X-Ray exclusive" applies to **tracing** — the mechanism for distributed request tracing, latency analysis, and dependency visualization. CloudWatch alarms on Lambda built-in metrics (Errors, Duration, Throttles) and custom application metrics are a separate, complementary system for **operational alerting**. Alarms tell operators "something is wrong"; X-Ray traces tell operators "what went wrong and where." Both are required. CloudWatch alarms are NOT duplicative with X-Ray.

**Independent Test**: Can be tested by injecting errors into each Lambda function and verifying CloudWatch alarms fire within the configured evaluation period.

**Acceptance Scenarios**:

1. **Given** the SSE Streaming Lambda errors, **When** the error count exceeds the configured threshold within the evaluation period, **Then** a CloudWatch alarm fires and notifies the on-call operator.
2. **Given** the Metrics Lambda errors, **When** the error count exceeds the configured threshold within the evaluation period, **Then** a CloudWatch alarm fires — alerting operators that the monitoring system itself has failed.
3. **Given** items remain stuck in "pending" status beyond the SLO threshold, **When** the StuckItems metric exceeds threshold, **Then** a CloudWatch alarm fires, alerting operators to stale dashboard data before customers notice.
4. **Given** SSE connection pool exhaustion, **When** ConnectionAcquireFailures exceeds threshold, **Then** a CloudWatch alarm fires, alerting operators that new SSE connections are being rejected.
5. **Given** SSE live update latency breaches the p95 < 3000ms SLO, **When** EventLatencyMs p95 exceeds threshold, **Then** a CloudWatch alarm fires.

---

### User Story 11 - Meta-Observability for Monitoring Infrastructure (Priority: P1) *(Round 4 — New)*

An operator is alerted when the monitoring infrastructure itself fails — whether X-Ray trace ingestion, CloudWatch metric emission, or the canary itself. A broken monitoring system that silently reports "all green" is the single most dangerous failure mode because it prevents detection of all other failures.

**Why this priority**: The audit Section 8.1 identifies CloudWatch `put_metric_data` failure as "the single most dangerous blind spot." If metric emission fails, alarms with `treat_missing_data = notBreaching` resolve to OK (false green), while alarms with `treat_missing_data = breaching` fire correctly but only cover those specific alarms. The X-Ray canary (US6) addresses X-Ray health but not CloudWatch health. The monitoring system needs a unified meta-observability canary that verifies both the tracing plane (X-Ray) and the alerting plane (CloudWatch metrics) are functional.

**Independent Test**: Can be tested by temporarily revoking the application Lambda's CloudWatch permissions and verifying the canary detects the emission failure within 2 intervals.

**Acceptance Scenarios**:

1. **Given** the canary emits a test metric to CloudWatch and queries for it after the ingestion delay, **When** the metric is retrievable, **Then** the canary reports CloudWatch metric pipeline health as healthy.
2. **Given** CloudWatch `put_metric_data` is failing for application Lambdas (simulated via IAM revocation), **When** the canary's test metric query returns empty results, **Then** the canary reports failure via a channel independent of CloudWatch alarms.
3. **Given** all alarms where metric absence indicates failure, **When** the `treat_missing_data` configuration is audited, **Then** 100% of those alarms use `breaching`.
4. **Given** the canary uses a separate IAM role from application Lambdas, **When** application IAM permissions are changed, **Then** the canary retains its ability to emit and query metrics, ensuring it can detect the failure.

---

### Edge Cases

- What happens when the X-Ray daemon is throttled (SDK rate limiting)? Subsegments beyond the sampling rate are dropped, but the Lambda still executes normally. The X-Ray SDK handles throttling internally; the system must not add error-suppression around it (see FR-018).
- What happens when a long-running SSE connection spans multiple X-Ray trace windows (traces have a 5-minute default window)? Each DynamoDB poll cycle within the SSE Lambda creates its own trace. The connection lifecycle subsegments must be scoped to the poll cycle, not the entire connection.
- What happens when the X-Ray canary reports failure but X-Ray is actually healthy (false positive)? The canary query window must account for X-Ray's eventual consistency delay (typically 5-30 seconds). The canary MUST NOT trigger on a single missed query — require consecutive failures.
- What happens when CloudWatch RUM sampling is reduced in production (10% sampling)? Only 10% of browser sessions generate RUM trace IDs. For the remaining 90%, the browser-side API client MUST still generate a valid `X-Amzn-Trace-Id` header so backend traces are created regardless of RUM sampling. Note: this applies to the browser-side API client only; the server-side SSE proxy (see next edge case) has different behavior.
- What happens when the SSE proxy (server-side) does not have an incoming `X-Amzn-Trace-Id` header? The server-side proxy MUST NOT generate a synthetic trace ID. Let the Lambda runtime assign the trace ID. Generating server-side trace IDs without proper SDK context would produce invalid trace context. The boundary is: browsers generate trace IDs (via RUM SDK); server-side proxies forward but never originate them.
- What happens when X-Ray annotation limits are reached? X-Ray allows a maximum of 50 annotations per subsegment, with values up to 2,048 characters. The annotations defined in FR-006 through FR-009 total fewer than 15 per subsegment. If future extensions approach the limit, non-essential annotations must be moved to X-Ray metadata (non-indexed, no limit).
- What happens to CloudWatch alarms, Log Insights saved queries, or dashboards that reference the old custom correlation ID format or the structured log fields being removed? Any downstream consumers of the removed systems (latency_logger fields, cache_logger fields, `correlation_id` in the old format) MUST be audited and updated as part of this feature. Failing to update them would cause silent breakage.
- What is the relationship between X-Ray sampling and the "any user request" tracing claim? X-Ray uses sampling (default: 1 request/second + 5% of additional requests). SC-001 applies to sampled requests. Dev/preprod uses 100% sampling. Production sampling must guarantee error trace capture (see FR-034) while controlling cost for successful requests.
- **(Round 2)** What happens when the SSE Lambda's X-Ray segment closes before streaming completes? The Lambda runtime creates a segment on invocation and closes it when the handler returns the generator. But `RESPONSE_STREAM` mode means the bootstrap continues polling the generator for chunks AFTER the handler returns. All boto3 calls during streaming (DynamoDB polls, CloudWatch metric emission) happen after segment closure. ~~The system MUST create independent segments during streaming, linked to the original trace ID.~~ **INVALIDATED in Round 3**: `begin_segment()` is a no-op in Lambda (see Assumption 7 invalidation). The system MUST use a tracing mechanism with an independent lifecycle (see FR-026 revised).
- **(Round 2)** What happens when `asyncio.new_event_loop()` is used in the async-to-sync bridge? The default `threading.local()` context storage DOES correctly propagate X-Ray context through `asyncio.new_event_loop().run_until_complete()` on the same thread. However, the X-Ray SDK's `AsyncContext` (which uses `TaskLocalStorage`) loses context across event loop boundaries. The system MUST NOT configure `AsyncContext` (see FR-037).
- **(Round 2)** What happens with X-Ray annotation type validation? The X-Ray SDK accepts `str`, `int`, `float`, and `bool` as annotation values. Any other type (including `None`, `list`, `dict`) is silently dropped with only a `log.warning()`. All annotations defined in FR-006 through FR-009 use valid types (confirmed: `latency_ms` is int, `cache_hit_rate` is float, `is_cold_start` is bool). Implementers MUST NOT pass `None` values — use a sentinel value or omit the annotation.
- **(Round 2)** What happens when raw `@xray_recorder.capture` is used instead of Powertools `@tracer.capture_method`? Raw xray_recorder does NOT automatically capture exceptions as subsegment errors. If an exception is raised inside a `@xray_recorder.capture`-decorated function, the subsegment is closed but not marked as error/fault. Powertools Tracer does this automatically. The spec's FR-005 requirement (mark subsegments as error on exception) is NOT satisfied by raw xray_recorder alone — either use Powertools Tracer or add manual exception capture.
- **(Round 3)** What happens when `@tracer.capture_method` is applied to an async generator function? The decorator does NOT check `inspect.isasyncgenfunction()` (confirmed: zero matches in Powertools codebase). It falls through to `_decorate_sync_function()`, which wraps the generator CREATION (near-zero time) rather than ITERATION. The subsegment opens and closes instantly, capturing no meaningful timing. Additionally, the decorator destroys the async generator's type signature (`inspect.isasyncgenfunction()` returns `False` on the wrapped function). System MUST NOT use `@tracer.capture_method` on async generators (see FR-031).
- **(Round 3)** What happens when the frontend uses `EventSource` for SSE connections? The `EventSource` API does not support custom HTTP headers (WHATWG HTML Living Standard, Section 9.2). The constructor accepts only a URL and optional `withCredentials`. CloudWatch RUM's automatic trace header injection patches `window.fetch()` but NOT `EventSource`. Trace headers CANNOT be propagated via `EventSource`. The frontend MUST use `fetch()` + `ReadableStream` for SSE connections that require trace propagation (see FR-032).
- **(Round 3)** What happens when a malicious client sends `Sampled=1` in the `X-Amzn-Trace-Id` header? X-Ray respects the incoming `Sampled` field as the authoritative sampling decision when it is already decided (not `?`). A client that always sends `Sampled=1` forces 100% server-side sampling, inflating costs at $5 per million traces. Server-side sampling rules MUST override client-supplied decisions (see FR-035).
- **(Round 3)** What happens when X-Ray throughput exceeds the region limit (2,600 segments/second)? `PutTraceSegments` returns HTTP 429 `ThrottledException`. Throttled segments appear in the `UnprocessedTraceSegments` response array — data is silently lost, not queued, not retried. The X-Ray daemon buffers and batches, providing some burst capacity, but sustained throughput above the limit results in permanent data loss with no alarm or notification.
- **(Round 3)** What happens when the SSE Lambda adds the ADOT Lambda Extension for independent lifecycle tracing? Lambda Extensions add cold start overhead (typically 50-200ms for ADOT) and memory overhead (~40-60MB). For the SSE Lambda with a 15-second streaming lifecycle, the relative impact is small. The extension's INIT phase runs concurrently with the Lambda runtime's INIT, partially masking the overhead.
- **(Round 3)** What happens when traffic grows beyond cost-effective 100% sampling? At current traffic levels (<1M requests/month), 100% sampling costs <$5/month. If traffic grows to 100M requests/month, 100% sampling costs ~$500/month. The X-Ray cost budget alarm (FR-038) provides early warning. The scaling path is tail-based sampling via an external OpenTelemetry Collector with a tail sampling processor, which makes sampling decisions AFTER spans complete — always keeping error traces while dropping a percentage of successful ones. This is documented as a future upgrade path, not a current requirement.
- **(Round 4)** What happens if CloudFront is re-introduced to the architecture? CloudFront treats `X-Amzn-Trace-Id` as a restricted header and replaces the client-sent value with its own trace ID. If CloudFront is added between browser and API Gateway, the RUM-generated trace context from the browser would be lost. The mitigation would be to rely on CloudFront's own X-Ray edge segment rather than browser-originated trace IDs. Source: AWS CloudFront documentation — `X-Amzn-Trace-Id` is listed as a restricted header that cannot be overridden via custom headers policies.
- **(Round 4)** What happens when ADOT auto-instrumentation is accidentally enabled on the SSE Lambda via `AWS_LAMBDA_EXEC_WRAPPER=/opt/otel-handler`? ADOT's `AwsLambdaInstrumentor` wraps the handler entry point (conflicting with Powertools `@tracer.capture_lambda_handler`), and `BotocoreInstrumentor` patches botocore (conflicting with X-Ray SDK patching from Powertools). Every boto3 call produces duplicate spans/subsegments in X-Ray. The env var MUST NOT be set on any Lambda that uses Powertools Tracer.
- **(Round 4)** What happens when the OTel `service.name` resource attribute and Powertools `POWERTOOLS_SERVICE_NAME` differ on the SSE Lambda? X-Ray service map aggregates nodes by service name. Mismatched names cause the same Lambda to appear as two disconnected nodes — one for handler-phase subsegments (Powertools) and one for streaming-phase spans (ADOT). Both names MUST be identical for a unified service map.
- **(Round 4)** What happens when an SSE connection drops and the client reconnects with `fetch()` + `ReadableStream`? The new `fetch()` call gets a new trace ID from RUM. The old trace (showing the connection drop) and new trace (showing the reconnection) are disconnected in X-Ray. X-Ray does not support native OpenTelemetry span links — link data stored in segment metadata is not surfaced in the X-Ray console or queryable via API. The system uses annotation-based correlation: a stable `session_id` annotation on all SSE traces enables querying all traces for a logical session, and a `previous_trace_id` annotation on reconnection traces provides explicit backward reference.
- **(Round 4)** What happens when X-Ray sampling rate is <100% in production and an error occurs on an unsampled request? The error is captured by the CloudWatch Lambda `Errors` metric (100% fidelity, independent of X-Ray sampling), but the X-Ray trace for that specific errored request does not exist — unsampled traces (`Sampled=0`) are permanently lost because data is never sent to the X-Ray service. Operators use CloudWatch alarms for alerting (100% error detection) and X-Ray traces for diagnosis (available for sampled requests). At current traffic levels with 100% sampling, this gap does not manifest.
- **(Round 4)** What is the relationship between X-Ray Groups and X-Ray sampling? X-Ray Groups are post-ingestion filters that only operate on already-sampled, already-ingested traces. A Group with filter `fault = true` generates CloudWatch metrics only from faulted traces that were sampled and ingested. If a request errors but was not sampled (`Sampled=0`), no X-Ray data exists and no Group will ever see it. Groups are a monitoring tool for sampled trace data, not a mechanism to ensure error capture. This is why CloudWatch alarms on Lambda built-in metrics (which capture 100% of invocations regardless of X-Ray) are required alongside X-Ray Groups.
- **(Round 5)** What happens when the OTel SDK's `TracerProvider` uses the default `RandomIdGenerator` instead of `AwsXRayIdGenerator`? The first 4 bytes of generated trace IDs are random instead of a valid unix timestamp. When the ADOT exporter converts to X-Ray format (`1-{first_8_hex}-{remaining_24_hex}`), the epoch field contains a garbage timestamp. X-Ray indexes traces by time window using this embedded timestamp — traces with invalid epochs may not appear in time-range queries, may be placed in wrong time buckets, or may be rejected entirely. All OTel `TracerProvider` instances on Lambdas that export to X-Ray MUST use `AwsXRayIdGenerator`. Source: `opentelemetry-sdk-extension-aws` package — `AwsXRayIdGenerator` embeds `int(time.time())` in the first 4 bytes.
- **(Round 5)** What is the X-Ray segment document size limit? Each emitted segment or subsegment document is capped at **64KB** by the `PutTraceSegments` API. Documents exceeding this limit are silently rejected and appear in `UnprocessedTraceSegments` in the API response — no error is thrown at the SDK level. The X-Ray SDK mitigates this by emitting completed subsegments as independent documents (with `parent_id` referencing the parent segment). However, a single subsegment with very large metadata (full HTTP response bodies, deep stack traces, large error messages) can still exceed 64KB. All metadata payloads MUST be bounded — truncate error messages to 2048 characters and never attach full response bodies as metadata. Source: AWS X-Ray API Reference, `PutTraceSegments` — "Each segment document can be up to 64 kB."
- **(Round 5)** What happens when the SSE Lambda's execution environment freezes without calling `force_flush()` on the OTel `TracerProvider`? The `BatchSpanProcessor` holds buffered spans in memory with a configurable batch timeout (FR-073 tunes to 1000ms; default is 5000ms — see FR-084 for correction of the original 200ms assumption). When the execution environment freezes after streaming completes, any spans created in the final batch interval that haven't been batched yet are suspended in memory. On the NEXT invocation (when the environment thaws), these stale spans from the PREVIOUS invocation are exported — carrying previous trace IDs that confuse operators (spans from invocation N appearing during invocation N+1's processing). Calling `force_flush(timeout_millis=5000)` in the generator's `finally` block ensures all spans reach the ADOT Extension's local OTLP receiver (localhost, sub-millisecond) before the environment freezes.
- **(Round 5)** What happens when the ADOT Extension's SHUTDOWN timeout is insufficient to flush all buffered spans to X-Ray? The Extension receives SHUTDOWN only when the execution environment is being destroyed (idle timeout, ~5-15 minutes of inactivity), NOT after each invocation. During SHUTDOWN, the ADOT collector calls `ForceFlush()` on all exporters with a default timeout of 2 seconds (configurable up to 10 seconds via `AWS_LAMBDA_EXTENSION_SHUTDOWN_TIMEOUT_MS`). If the X-Ray export takes longer, remaining spans are dropped. This is low risk because: (a) `force_flush()` in FR-055 pushes spans to the ADOT Extension's local OTLP receiver on each invocation (fast, localhost), (b) the ADOT batch processor exports to X-Ray incrementally during streaming (1000ms batch timeout per FR-073), (c) SHUTDOWN only occurs on environment destruction, not routine operation.
- **(Round 5)** What happens when the OTel SDK uses `always_on` sampler instead of `parentbased_always_on` on the SSE Lambda? When the Lambda runtime sets `Sampled=0` in `_X_AMZN_TRACE_ID` (unsampled request), the `AwsXRayLambdaPropagator` extracts this flag. With `parentbased_always_on`, the sampler respects the parent's decision and suppresses span creation — correct behavior. With `always_on`, the sampler ignores the parent's decision and creates spans regardless. These spans are exported to X-Ray via ADOT, but X-Ray has no corresponding Lambda facade segment (since the request was unsampled). The result is orphaned spans that appear as root-level traces with no parent, creating noise in the X-Ray console and inflating costs for unsampled traffic.
- **(Round 6)** What happens on warm invocations when OTel context is extracted at module level instead of per-invocation? The `_X_AMZN_TRACE_ID` environment variable changes on every Lambda invocation (the runtime updates it before calling the handler). If `propagate.extract(carrier={})` is called at module level (cold start only), the returned context captures the FIRST invocation's trace ID. All subsequent warm invocations' streaming spans link to this stale trace ID — creating cross-invocation trace contamination. This is invisible in cold-start-heavy dev testing but manifests in production under sustained load where warm invocations dominate. The `AwsXRayLambdaPropagator.extract()` method reads `os.environ` on EVERY call (no caching), so calling it at handler entry correctly provides the current invocation's context.
- **(Round 6)** What happens when Powertools Tracer's `auto_patch=True` is active on the SSE Lambda during the streaming phase? Powertools' auto-patching instruments boto3/botocore at module import time via the X-Ray SDK's `patch()` mechanism. These patches are GLOBAL — they affect ALL boto3 calls regardless of execution phase. During SSE streaming, a DynamoDB query creates: (a) an X-Ray subsegment via auto-patching (transmitted to X-Ray daemon, which may or may not accept data after handler returns — undocumented by AWS for RESPONSE_STREAM), AND (b) an OTel span via manual instrumentation (transmitted to ADOT Extension, reliable). If the X-Ray daemon continues accepting data during streaming, operators see DUPLICATE entries for each AWS SDK call. If the daemon stops, only OTel spans appear. The inconsistency is unpredictable. Setting `auto_patch=False` and using explicit `@tracer.capture_method` on handler-phase functions eliminates the dual-emission during streaming while preserving handler-phase tracing.
- **(Round 6)** What happens when the SSE Lambda's execution environment is destroyed without a SHUTDOWN event (e.g., during infrastructure updates or spot reclamation)? The ADOT Extension does not receive SHUTDOWN, so its final flush does not execute. Spans in the ADOT pipeline are lost. The `BatchSpanProcessor`'s in-process buffer is also lost. The canary (FR-019) detects the data loss on subsequent intervals. This is a rare edge case — standard Lambda lifecycle always includes SHUTDOWN before destruction.
- **(Round 6)** What happens when the ADOT Extension's OTLP receiver is not ready during Lambda cold start? During cold start, the ADOT Extension and Lambda runtime initialize concurrently. The ADOT Extension's OTLP receiver on `localhost:4318` may not be listening when the first handler-phase spans are created. The `OTLPSpanExporter` fails to connect. The `BatchSpanProcessor` buffers the spans and retries on the next scheduled export (1000ms later per FR-073). By then, the ADOT Extension's receiver is ready. Handler-phase spans are delayed by up to 200ms but are NOT lost. No action required — this is handled by the existing retry/buffer mechanism.
- **(Round 6)** What happens when existing CloudWatch alarm thresholds use different calibration than new alarms? Task 17 adds new latency alarms at 80-90% of Lambda timeout. But existing alarms (e.g., `analysis_latency_high` at 25s for 60s timeout = 42%) were set with different methodology. During an incident, the new SSE latency alarm fires at 14s/15s (93% of timeout) while the existing Analysis alarm doesn't fire until 25s/60s (42% of timeout). The inconsistency creates uneven alerting — some Lambdas alert early, others alert late. All thresholds MUST use consistent calibration.
- **(Round 7)** What happens when the ADOT Extension process crashes during SSE streaming? The OTel SDK's `BatchSpanProcessor._export()` catches all exceptions via `except Exception` and logs them. The `OTLPSpanExporter` retries once on `ConnectionError`, then the error propagates to the `BatchSpanProcessor` which swallows it. The SSE Lambda continues streaming normally — the client receives a complete response. However, zero trace data exists for the remainder of that stream. The operator has no per-invocation detection mechanism; the canary (FR-019/FR-036) detects trace absence at aggregate level. This is a known, accepted gap mitigated by the fact that ADOT communicates via localhost (connection failure = Extension process death, which is logged by the Lambda runtime).
- **(Round 7)** What happens when the ADOT Extension's default collector configuration includes unnecessary components (metrics pipeline, health check endpoint, zpages extension)? Each component consumes memory and CPU without value in sidecar-only mode. A custom stripped-down collector configuration (`OPENTELEMETRY_COLLECTOR_CONFIG_FILE`) containing only the OTLP HTTP receiver and AWS X-Ray exporter reduces the Extension's memory footprint from ~50MB to ~35MB, freeing headroom within the 512MB allocation.
- **(Round 7)** Why `BatchSpanProcessor` instead of `SimpleSpanProcessor` for the SSE Lambda? `SimpleSpanProcessor` exports each span synchronously — every DynamoDB query span blocks on an OTLP HTTP POST to localhost before the next query executes, adding per-span latency to the streaming response. `BatchSpanProcessor` buffers spans and exports asynchronously every 1000ms (per FR-073; default is 5000ms — corrected from original 200ms assumption, see FR-084), requiring `force_flush()` at stream end (FR-055). For latency-sensitive streaming, `BatchSpanProcessor` is the correct choice — the 1000ms batch interval is negligible relative to the 15-second streaming lifecycle, and async export prevents span creation from blocking the response.
- **(Round 7)** What happens when `TracerProvider` is mistakenly created per invocation instead of as a module-level singleton? Each `TracerProvider()` instantiation spawns a new `BatchSpanProcessor` daemon thread and opens a new OTLP HTTP connection to ADOT. Previous instances are not garbage-collected (daemon threads hold references). After N warm invocations, the execution environment has N orphaned threads consuming ~N MB of memory. Eventually, Lambda hits its 512MB memory limit and OOM-terminates the environment. This is why FR-065 clarifies that FR-059's per-invocation requirement applies ONLY to `propagate.extract()`, not to infrastructure objects.
- **(Round 7)** What happens with the ADOT Extension + RESPONSE_STREAM lifecycle interaction? In RESPONSE_STREAM mode, the Lambda invocation is "complete" only when the generator is exhausted and the response stream closes — NOT when the handler returns the generator. The ADOT Extension receives the INVOKE completion notification after streaming finishes. The `force_flush()` in the generator `finally` block (FR-055) pushes the final spans to ADOT's OTLP receiver before INVOKE completion. The Extension then has time to export to X-Ray in the post-INVOKE processing window before environment freeze. This lifecycle is compatible because ADOT is designed for Lambda — it waits for the INVOKE response before starting its post-processing.
- **(Round 7)** What is the distinction between the ADOT Python SDK layer and the ADOT collector-only layer? Two layer types exist: (1) `aws-otel-python-amd64-*` includes both the OTel Collector binary AND Python auto-instrumentation wrapper (`/opt/otel-instrument`) — ~80MB unzipped, (2) `aws-otel-collector-amd64-*` includes only the OTel Collector binary — ~35MB unzipped. For sidecar-only mode (FR-046: no `AWS_LAMBDA_EXEC_WRAPPER`), the collector-only layer is correct. Using the Python SDK layer wastes ~45MB and includes unused auto-instrumentation code that could be accidentally activated. Source: ADOT Lambda Layer ARN repository — separate per-region ARN mappings for each layer type.
- **(Round 8)** What happens when FR-062/FR-063 specify Lambda Layers for a container-based Lambda? Lambda container images CANNOT use Lambda Layers — the `layers` attribute is only valid for ZIP deployments. Terraform will either error (if using `aws_lambda_function` with `image_uri` and `layers` simultaneously) or silently ignore the layer configuration. The SSE Lambda (`python:3.13-slim` + custom bootstrap), Analysis Lambda (`public.ecr.aws/lambda/python:3.13`), and Dashboard Lambda (`public.ecr.aws/lambda/python:3.13`) are ALL container-based. ADOT deployment for the SSE Lambda requires embedding the collector binary in the Docker image. Source: AWS Lambda Container Image docs — "You cannot add Lambda layers to container images."
- **(Round 8)** What happens when `OTEL_SERVICE_NAME` is not set on the SSE Lambda? The `AwsLambdaResourceDetector` populates `cloud.*` and `faas.*` attributes but does NOT set `service.name`. The OTel SDK defaults `service.name` to `unknown_service`. The ADOT X-Ray exporter uses `service.name` as the node name in the X-Ray service map. All SSE Lambda invocations appear under a single `unknown_service` node — indistinguishable from any other unconfigured Lambda. SC-028 becomes unachievable. Operators cannot identify the SSE Lambda in the service map without searching by trace ID. Source: `opentelemetry-sdk-extension-aws` — `AwsLambdaResourceDetector` implementation; AWS X-Ray OTLP integration docs.
- **(Round 8)** What happens when `BatchSpanProcessor` uses default configuration (`schedule_delay_millis=5000`) on the SSE Lambda? The background worker thread wakes every 5 seconds to check for pending spans. During a 15-second streaming session producing ~50-200 spans, the worker exports approximately 3 batches (at 5s, 10s, 15s). With `force_flush()` in the generator `finally` block, the remaining spans are drained synchronously. Functionally correct, but the 5-second delay means a mid-stream crash loses up to 5 seconds of spans (vs 1 second with tuned configuration). The 2048-entry queue buffer also wastes ~200KB of memory unnecessarily. For Lambda, smaller/faster is better.
- **(Round 9)** What happens when the ADOT Extension's collector context is canceled before HTTP exports to X-Ray complete? This is a KNOWN issue (GitHub aws-otel-lambda#886, opentelemetry-lambda#224) with measured ~30% span drop rate in worst-case scenarios. The root cause: the collector's internal context is canceled when the Lambda Extension calls Next API, but the X-Ray HTTP exporter may still have in-flight requests. Error pattern: `"context canceled"` during export followed by `"Exporting failed. Dropping data."` The decouple processor (Lambda-specific batch processor in ADOT) mitigates this by decoupling span reception from export — it holds the Extension from calling Next until all queued data is exported. However, under high concurrency or when `span.end()` is the very last operation before function exit, a race exists between the collector's shutdown and the HTTP export completion. The spec's `force_flush()` (FR-055) handles the SDK→Extension hop (fast, localhost). The Extension→X-Ray backend hop is the vulnerable path. The canary (FR-019/FR-036) provides aggregate detection. Per-invocation detection is NOT possible because the span loss occurs in the Extension process, not the SDK. Source: https://github.com/aws-observability/aws-otel-lambda/issues/886
- **(Round 9)** What happens when `TracerProvider` is created with the default `shutdown_on_exit=True`? Python's `atexit` module registers a handler that calls `TracerProvider.shutdown()`. In Lambda, the interpreter does not exit between warm invocations, so `atexit` does not fire between invocations. However, when the execution environment is recycled (idle timeout), the interpreter finalization sequence runs, triggering `atexit` handlers. In Python 3.13+, `ConcurrentMultiSpanProcessor.shutdown()` shuts down the thread pool executor. If the shutdown races with a pending export (e.g., a span was queued just before recycling), a `RuntimeError: cannot schedule new futures after shutdown` is raised (opentelemetry-python#4461). The `shutdown_on_exit=False` parameter prevents this race by skipping the `atexit` registration. Since the Lambda runtime calls `force_flush()` per FR-055, the `atexit` handler is redundant. Source: https://github.com/open-telemetry/opentelemetry-python/issues/4461
- **(Round 9)** What happens when OTel `BotocoreInstrumentor().instrument()` is called on the SSE Lambda alongside Powertools Tracer (even with `auto_patch=False`)? `BotocoreInstrumentor` patches `botocore.client.BaseClient._make_api_call` with OTel span creation. Powertools Tracer's `@tracer.capture_method` also creates X-Ray subsegments around handler-phase AWS SDK calls. During the handler phase, each AWS SDK call would produce BOTH an OTel span (via BotocoreInstrumentor) AND an X-Ray subsegment (via capture_method). During the streaming phase, only OTel spans are created (correct). The BotocoreInstrumentor does NOT auto-activate in sidecar-only mode — it must be explicitly called. FR-060 prohibits `auto_patch` but does not prohibit `BotocoreInstrumentor`. Without explicit prohibition, a future implementer could import and call it thinking it's the "OTel way" to instrument AWS SDK calls.
- **(Round 9)** What happens when the X-Ray canary queries BatchGetTraces before the trace is indexed? The X-Ray API cannot distinguish "not yet indexed" from "trace does not exist." Both scenarios result in the trace ID appearing in `UnprocessedTraceIds` or being absent from the `Traces` response array. A single query at 30 seconds will produce false negatives under load. The canary must retry with backoff (query at 30s, retry at 60s) to account for eventual consistency. Two consecutive failures across canary intervals remains the alarm threshold. Source: https://aws.amazon.com/xray/faqs/ — "Trace data sent to X-Ray is generally available for retrieval and filtering within 30 seconds"
- **(Round 9)** What happens when CloudWatch `put_metric_data` fails with an IAM error (AccessDeniedException) vs a transient error (ThrottlingException)? The canary (FR-049) currently treats all failures uniformly. IAM/credential errors (AccessDeniedException, NotAuthorized, InvalidClientTokenId) indicate a permanent permission change — the exact failure mode FR-051's separate IAM role is designed to detect — and require immediate out-of-band escalation. Transient errors (ThrottlingException, InternalFailure, ServiceUnavailable) should be retried with exponential backoff before escalating. Without this classification, an IAM revocation is silently retried for multiple canary intervals before escalating, losing critical response time. Source: https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_PutMetricData.html
- **(Round 9)** What happens when the SSE Lambda server ignores the `Last-Event-ID` request header on reconnection? FR-033 specifies client-side `Last-Event-ID` propagation for stream resumption, but if the server does not check this header and resume from the corresponding position, the client receives a fresh stream from the beginning. This causes duplicate events (already-processed events re-sent) and missed events (events emitted between disconnect and reconnect start if server resets). The client must handle idempotently (deduplicate by event ID). Without server-side support, the "stream resumption" guarantee in FR-033 is hollow.
- **(Round 9)** What happens when the frontend needs to read the `X-Amzn-Trace-Id` response header for FR-048's `previous_trace_id` capture? Per the CORS specification, browsers can only read response headers listed in `Access-Control-Expose-Headers`. The SSE Lambda Function URL's CORS configuration currently does not include `x-amzn-trace-id` in `ExposeHeaders`. Without it, the browser JavaScript cannot read the response trace ID, making FR-048's reconnection trace correlation impossible. This is a hard prerequisite, not optional.
- **(Round 9)** What happens when Safari processes small SSE frames from a fetch()+ReadableStream connection? Safari may buffer small streaming responses differently than Chrome/Firefox, potentially batching multiple small SSE frames before delivering them to the ReadableStream reader. This causes perceived latency spikes where events arrive in bursts rather than individually. Workaround: server sends padding bytes or explicit flush signals. Source: sveltejs/kit#10315
- **(Round 9)** What is the canonical way to distinguish graceful server close from network error in a fetch()+ReadableStream SSE connection? `ReadableStreamDefaultReader.read()` returns `{done: true}` on graceful close and throws `TypeError` on network error. The `reader.closed` Promise fulfills on graceful close and rejects on error. FR-033's reconnection logic should use `retry:` value (from SSE protocol) or short fixed delay for graceful close (expected Lambda timeout), and exponential backoff with jitter for network errors. Uniform backoff on all close types unnecessarily delays reconnection after expected server-initiated closes. Source: WHATWG Streams Standard — ReadableStreamDefaultReader.closed
- **(Round 9)** What happens if AWS Account-level Transaction Search is enabled? If the account migrates to CloudWatch-native tracing, traces stored in `/aws/spans` are NOT queryable via X-Ray's `BatchGetTraces` API — only traces stored via `PutTraceSegments` to the legacy X-Ray segment store are retrievable. The canary's trace verification (FR-019) uses `BatchGetTraces`. An account-level settings change could break the canary without any code change. The canary should verify that its test traces are retrievable via the API it uses, not assume the storage backend. Source: https://repost.aws/questions/QUUl7bopxdQii5vVCNZ3FtPg
- **(Round 10)** What happens when the SSE Lambda continues streaming after the client disconnects? AWS docs confirm: "Streamed responses are not interrupted or stopped when the invoking client connection is broken. Customers are billed for the full function duration." The SSE Lambda continues DynamoDB polling, CloudWatch metric emission, and OTel span creation after client disconnects. `force_flush()` runs in the `finally` block for a phantom stream. Python custom runtimes have NO standard mechanism to detect client disconnect (unlike Node.js `StreamifyResponse` which provides `metadata.onAbort`). The only signal is `BrokenPipeError` raised when the generator attempts to write to the closed response stream. Spans created after disconnect annotate "successful" streaming with no client receiving the data. The canary should account for elevated "successful but undelivered" traces in its health assessment. Source: https://docs.aws.amazon.com/lambda/latest/dg/configuration-response-streaming.html
- **(Round 10)** What happens when `BatchSpanProcessor` queue fills during high-throughput SSE streaming? `BatchSpanProcessor.emit()` uses `queue.put_nowait()` which raises `queue.Full` exception, silently caught and logged as a WARNING. Spans are DROPPED, not queued or retried. At FR-073's original `max_queue_size=512`, a 15-second streaming Lambda producing ~15 DynamoDB polls/second with ~3 spans per poll generates ~675 spans per invocation. This EXCEEDS the 512 queue capacity. If `force_flush()` (which drains the queue synchronously) does not keep pace with span production between batch intervals, ~163 spans are silently lost. The drop is only visible as a WARNING log line — no metric, no exception, no span attribute. Source: OTel Python SDK source code — `BatchSpanProcessor.on_end()` method.
- **(Round 10)** What happens when custom ADOT collector config omits the decouple processor? The ADOT Lambda Layer's default configuration automatically includes the decouple processor when the batch processor is used. However, this auto-configuration ONLY applies to Lambda Layer deployments — NOT to container-based deployments with custom collector YAML (FR-064/FR-069). If the custom collector config defines a pipeline without explicitly listing `decouple` in the processors chain, span drops revert to the known ~30% rate (aws-otel-lambda#886) because the collector context is canceled before HTTP exports complete. Cross-reference: FR-075 (decouple processor requirement), FR-064 (container-based deployment), FR-069 (custom config). Source: https://github.com/open-telemetry/opentelemetry-lambda/blob/main/collector/processor/decoupleprocessor/README.md
- **(Round 10)** What happens when Active Tracing is accidentally disabled on a Lambda function? `AwsXRayLambdaPropagator` reads `_X_AMZN_TRACE_ID` environment variable, which is ALWAYS set by the Lambda runtime regardless of Active Tracing status. With Active Tracing disabled, the trace ID still populates but with `Sampled=0`. The propagator extracts this context (open issue opentelemetry-lambda#1782 — the propagator does not distinguish between "legitimately unsampled" and "tracing disabled"). The `parentbased_always_on` sampler honors the parent's `Sampled=0` flag and suppresses ALL span creation. Result: zero spans emitted, zero errors logged, complete silent tracing failure. Cross-reference: FR-052 (Active Tracing requirement). Source: https://github.com/open-telemetry/opentelemetry-lambda/issues/1782
- **(Round 10)** What happens when X-Ray subsegment count exceeds 100 per segment (Powertools Tracer)? The X-Ray SDK has a default subsegment streaming threshold of 100. When exceeded, completed subsegments are automatically sent to the X-Ray daemon out of band. For non-streaming Lambdas with many boto3 calls, this is benign. For the SSE Lambda handler phase (before streaming begins), the count should be well under 100. However, if `auto_patch=True` were accidentally re-enabled (violating FR-060), streaming-phase boto3 calls would create X-Ray subsegments via the X-Ray daemon (unreliable after handler returns), counting against the 100-subsegment threshold and triggering automatic streaming of partially-complete subsegments. This reinforces the criticality of FR-060 (`auto_patch=False`). Source: https://docs.aws.amazon.com/xray/latest/devguide/xray-troubleshooting.html
- **(Round 10)** What happens when X-Ray centralized sampling rules are temporarily unavailable? The OTel X-Ray remote sampler falls back to a `TraceIDRatioBased` sampler with reservoir borrowing (1 trace per second). In dev/preprod environments where 100% sampling is required (FR-034/FR-035), this fallback silently drops to 1 request/second sampling during X-Ray service degradation. The fallback is a safety mechanism (not a bug), but the drop from 100% to ~0.01% (at 10K req/s) is dramatic and invisible without explicit monitoring. No metric or alarm fires for the sampling mode change. Source: https://aws-otel.github.io/docs/getting-started/remote-sampling/

## Requirements *(mandatory)*

### Functional Requirements

**Backend Lambda X-Ray Subsegments**

- **FR-001**: System MUST add X-Ray tracing to all SSE Streaming Lambda operations: global stream handling, config stream handling, DynamoDB polling, connection acquisition, connection release, and event dispatch. Because the SSE Lambda uses `RESPONSE_STREAM` invoke mode, the Lambda runtime's auto-created segment closes before streaming begins. Tracing during streaming MUST use a mechanism with an independent lifecycle that does not depend on the Lambda runtime's X-Ray segment being open (see FR-026 revised).
- **FR-002**: System MUST add X-Ray subsegments to all 7 silent failure paths: (1) circuit breaker load, (2) circuit breaker save, (3) audit trail persistence, (4) downstream notification SNS publish, (5) time-series fanout batch write, (6) self-healing item fetch, (7) parallel fetcher error aggregation.
- **FR-003**: All service calls made by the Metrics Lambda (DynamoDB queries, CloudWatch API calls) MUST appear as auto-instrumented subsegments in X-Ray traces. Currently, the Metrics Lambda has zero X-Ray SDK integration despite Active tracing being enabled at the infrastructure level.
- **FR-004**: System MUST add explicit X-Ray subsegments to the Metrics Lambda for DynamoDB queries and CloudWatch `put_metric_data` calls.
- **FR-005**: System MUST mark X-Ray subsegments as error/fault when the wrapped operation catches an exception, including the exception type and message as metadata.

**X-Ray Annotations and Metadata**

- **FR-006**: System MUST replace custom structured latency logging in `latency_logger.py` with X-Ray subsegment annotations containing: `event_type`, `latency_ms`, `is_cold_start`, `is_clock_skew`, `connection_count`.
- **FR-007**: System MUST replace custom structured cache logging in `cache_logger.py` with X-Ray subsegment annotations containing: `cache_hit_rate`, `cache_entry_count`, `cache_max_entries`, `trigger`, `is_cold_start`.
- **FR-008**: System MUST include X-Ray annotations on connection lifecycle subsegments: `connection_id`, `current_count`, `max_connections`.
- **FR-009**: System MUST include X-Ray annotations on DynamoDB polling subsegments: `item_count`, `changed_count`, `poll_duration_ms`, `sentiment_type`.

**Correlation ID Consolidation**

- **FR-010**: System MUST replace the custom `get_correlation_id()` function with X-Ray trace ID retrieval from the active segment.
- **FR-011**: System MUST replace the custom `generate_correlation_id()` function with X-Ray trace ID retrieval.
- **FR-012**: System MUST include the X-Ray trace ID as a top-level field in all operational structured log entries (error logs, warning logs, info-level operational events) so CloudWatch Logs Insights queries can join log entries to X-Ray traces. Note: FR-022 and FR-023 remove tracing-specific structured logs; this requirement applies to the operational logs that remain.

**Cross-Service Trace Propagation**

- **FR-013**: System MUST verify that Active X-Ray tracing is enabled on both the Ingestion Lambda and the Analysis Lambda, and that the SNS subscription between them supports automatic trace context propagation via the `AWSTraceHeader` system attribute. The result is that both Lambda invocations appear under the same trace ID with zero manual MessageAttributes configuration.
- **FR-014**: System MUST propagate the `X-Amzn-Trace-Id` header from the frontend SSE proxy (Next.js API route) to the upstream SSE Lambda call when the header is present on the incoming request.
- **FR-015**: System MUST add `X-Amzn-Trace-Id` to the CORS `Access-Control-Allow-Headers` list for both API Gateway and Lambda Function URLs.
- **FR-016**: The frontend API client MUST include the `X-Amzn-Trace-Id` header on all outgoing requests when the browser has an active trace context. In test environments where RUM sampling is set to 100%, every API request from the browser MUST carry this header.

**Infrastructure Alignment**

- **FR-017**: All Lambda execution roles MUST have sufficient permissions to emit X-Ray trace segments and telemetry records. Currently, only 2 of 6 Lambda roles have explicit X-Ray write permissions; the remaining 4 (Ingestion, Analysis, Dashboard, Metrics) MUST be aligned.
- **FR-018**: X-Ray instrumentation errors MUST propagate to the Lambda runtime unhandled, resulting in Lambda invocation failure visible in standard error metrics. No error-suppression or fallback logic around tracing calls is permitted.

**X-Ray Canary (Watcher of the Watcher)**

- **FR-019**: System MUST implement a canary that periodically submits a batch of test traces to X-Ray and queries for their retrieval, reporting health via a non-X-Ray channel (CloudWatch metric). The canary MUST track both presence (did my test trace arrive?) and completeness (did all N of my test traces arrive?) to detect partial data loss from throttling.
- **FR-020**: System MUST configure the canary's health metric alarm with `treat_missing_data = breaching` so the alarm fires if the canary itself stops running.
- **FR-021**: The canary MUST be the sole non-X-Ray tracing and correlation mechanism in the system. All other custom tracing logs (latency_logger, cache_logger), custom correlation IDs, and custom log-based trace correlation MUST be consolidated onto X-Ray. Standard operational logging (error messages, audit records) and CloudWatch metrics (billing alarms, auto-scaling triggers, SLO dashboards) are not in scope and remain unchanged.

**Removal of Replaced Systems**

- **FR-022**: System MUST remove the custom latency structured logging from `latency_logger.py`. After X-Ray subsegment annotations replace its tracing function, the module and all its call sites MUST be deleted. No adapter or wrapper is permitted — the X-Ray annotations ARE the replacement.
- **FR-023**: System MUST remove the custom cache structured logging from `cache_logger.py`. After X-Ray annotations replace its tracing function, the module and all its call sites MUST be deleted.
- **FR-024**: System MUST remove the custom `get_correlation_id()` and `generate_correlation_id()` functions after X-Ray trace ID retrieval replaces them.

**SSE Streaming Trace Context (Round 2, revised in Round 3)**

- **FR-025** [Round 3 — Revised]: The SSE Lambda MUST preserve trace context across the async-to-sync bridge boundary. The system MUST rely on the default `threading.local()` context storage (which correctly propagates through `asyncio.new_event_loop().run_until_complete()` on the same thread). The system MUST NOT configure the X-Ray SDK's `AsyncContext`, which uses `TaskLocalStorage` that loses context across event loop boundaries (known bugs: aws-xray-sdk-python #164, #310, #446).
- **FR-026** [Round 3 — Replaced]: ~~During SSE streaming, the system MUST create independent X-Ray segments for each poll cycle.~~ **INVALIDATED**: `begin_segment()` is a no-op in Lambda (`LambdaContext.put_segment()` silently discards, `FacadeSegment` raises `FacadeSegmentMutationException`). **Replacement**: The SSE Lambda MUST use a tracing mechanism with an independent lifecycle for operations that execute during response streaming (after the handler returns the generator). This mechanism MUST: (a) create trace spans during active streaming, (b) link spans to the original invocation's trace ID for unified visualization, (c) export spans to X-Ray, (d) function independently of the Lambda runtime's facade segment lifecycle. The mechanism MUST run as a separate process (Lambda Extension) with its own INIT and SHUTDOWN phases, ensuring trace data is flushed even when the Lambda execution environment freezes.
- **FR-027** [Round 3 — Replaced]: ~~Auto-patched boto3 calls during streaming MUST execute within independently created segments.~~ **Replacement**: All service calls during SSE response streaming (DynamoDB queries, CloudWatch metric emission) MUST be captured within traced spans. These calls occur after the Lambda handler returns, when the X-Ray SDK's facade segment is closed. The independent lifecycle tracing mechanism from FR-026 MUST capture these calls so they appear in X-Ray as part of the connection's trace.

**SendGrid Explicit Instrumentation (Round 2 — Emergent)**

- **FR-028**: The SendGrid email sending operation in the Notification Lambda MUST be wrapped in an explicit X-Ray subsegment. The SendGrid SDK uses `urllib`/`python-http-client` for HTTP transport, which is NOT auto-patched by the X-Ray SDK. The subsegment MUST capture: HTTP status code, request duration, and error details on failure.

**Tracer Standardization (Round 2, updated in Round 3)**

- **FR-029** [Round 3 — Updated]: All non-streaming Lambda functions (Ingestion, Analysis, Dashboard, Notification, Metrics) MUST use a consistent X-Ray instrumentation approach that automatically captures exceptions as subsegment errors. The SSE Streaming Lambda MUST use the independent lifecycle tracing mechanism from FR-026, which has different instrumentation requirements due to the RESPONSE_STREAM segment lifecycle constraint. Both approaches MUST produce traces that appear in the same X-Ray console with unified trace IDs.
- **FR-030**: The system MUST eliminate double-patching of HTTP/AWS SDK clients. Currently, the Dashboard Lambda calls both explicit `patch_all()` and initializes a Tracer with `auto_patch=True` (the default), causing boto3 and requests to be patched twice. After standardization, each Lambda MUST have exactly one patching mechanism.

**Async Generator Tracing Safety (Round 3 — New)**

- **FR-031**: The system MUST NOT apply `@tracer.capture_method` or any equivalent function-wrapping decorator to async generator functions. The Powertools Tracer dispatcher does not check `inspect.isasyncgenfunction()` and silently routes async generators to the synchronous wrapper, which captures only generator creation time (near-zero) rather than iteration time. All async generator tracing MUST use manual subsegment or span context managers within the generator body. Source: `aws_lambda_powertools/tracing/tracer.py` — `isasyncgenfunction()` is never called in the dispatch chain.

**Frontend SSE Trace Propagation (Round 3 — New)**

- **FR-032**: The frontend SSE client MUST use `fetch()` with `ReadableStream` consumption instead of the `EventSource` API for connections that require trace header propagation. The `EventSource` API does not support custom HTTP headers per the WHATWG HTML Living Standard (Section 9.2). Using `fetch()` enables CloudWatch RUM's automatic `X-Amzn-Trace-Id` header injection (when `addXRayTraceIdHeader: true` is configured in the RUM http telemetry) and provides a mechanism for manual header attachment.
- **FR-033**: Since the `EventSource` API's built-in auto-reconnection is lost when switching to `fetch()` + `ReadableStream` (FR-032), the frontend SSE client MUST implement equivalent reconnection logic: exponential backoff with jitter on connection failure, `Last-Event-ID` header propagation for stream resumption, and connection state management matching the reliability guarantees of the replaced `EventSource`.

**Sampling Strategy (Round 3 — New)**

- **FR-034**: The system MUST configure X-Ray sampling rules per environment: 100% sampling (reservoir=1, fixed_rate=1.0) in dev/preprod for complete debuggability, and a documented production rate that balances cost with trace availability. The system MUST configure an X-Ray Group with filter expression `fault = true OR error = true` that automatically generates CloudWatch metrics (ApproximateErrorCount, FaultCount, latency percentiles) from errored traces, enabling error rate monitoring and alarming directly from trace data.
- **FR-035**: The system MUST configure server-side X-Ray sampling rules at the API Gateway and Lambda level that make independent sampling decisions, regardless of client-supplied `Sampled=1` in the `X-Amzn-Trace-Id` header. A malicious client MUST NOT be able to force 100% sampling and inflate X-Ray costs. The server-side sampling configuration is the authoritative source of truth for sampling decisions.

**Trace Data Integrity (Round 3 — New)**

- **FR-036**: The X-Ray canary (FR-019) MUST additionally track the ratio of submitted test traces to successfully retrieved test traces across consecutive intervals. When the retrieval ratio drops below a configured threshold (indicating systematic data loss from throttling, daemon failure, or SDK errors), the canary MUST emit a dedicated `trace_data_loss` CloudWatch metric that triggers an alarm.
- **FR-037**: The system MUST NOT manually configure the X-Ray SDK's `AsyncContext` on any Lambda function. The default `threading.local()` context storage correctly propagates through `asyncio.new_event_loop().run_until_complete()` calls on the same thread. Manually configuring `AsyncContext` uses `TaskLocalStorage`, which stores context on `asyncio.Task` objects — new tasks in a new event loop have no inherited context, breaking the async-to-sync bridge pattern used by the SSE Lambda. Known open issues: aws-xray-sdk-python #164 (concurrent async subsegments cause `AlreadyEndedException`), #310 (concurrent asyncio tasks + X-Ray in Lambda), #446 (context not propagated with `run_in_executor`).

**Cost Guard (Round 3 — New)**

- **FR-038**: The system MUST have CloudWatch billing alarms for X-Ray costs at thresholds of $10, $25, and $50 per month. At 100% sampling, X-Ray costs scale linearly at $5 per million traces recorded plus $0.50 per million traces retrieved/scanned. The free tier covers 100,000 traces recorded and 1,000,000 retrieved per month.

**Scope Clarification (Round 4 — New)**

- **FR-039**: "X-Ray exclusive" applies to **tracing** — the mechanism for distributed request tracing, latency analysis, and dependency visualization. CloudWatch alarms on Lambda built-in metrics (Errors, Duration, Throttles) and custom application metrics are a separate, complementary system for operational alerting. Alarms tell operators "something is wrong"; X-Ray traces tell operators "what went wrong and where." Both systems are required for complete observability. CloudWatch alarms are NOT duplicative with X-Ray and MUST NOT be removed as part of the X-Ray consolidation effort. Standard operational logging (error messages, audit records) also remains unchanged per FR-021.

**Operational Alarm Coverage (Round 4 — New)**

- **FR-040**: All 6 Lambda functions MUST have CloudWatch error alarms on the `AWS/Lambda` `Errors` metric. Currently, SSE Streaming Lambda and Metrics Lambda have zero error alarms (audit Section 4.3).
- **FR-041**: All 6 Lambda functions MUST have CloudWatch latency alarms on the `AWS/Lambda` `Duration` metric at the p95 statistic. Currently, Ingestion Lambda, Notification Lambda, SSE Streaming Lambda, and Metrics Lambda are missing latency alarms (audit Section 9.3).
- **FR-042**: The following custom metrics currently emitted without alarms MUST each have a CloudWatch alarm configured: `StuckItems` (items stuck in "pending" >5min), `ConnectionAcquireFailures` (SSE connection pool exhaustion), `EventLatencyMs` (SSE live update latency p95 > 3000ms SLO), `MetricsLambdaErrors` (monitoring system errors), `HighLatencyAlert` (ingestion latency >30s), `PollDurationMs` (DynamoDB poll duration), `AnalysisErrors` (analysis processing errors). Source: audit Section 4.2.
- **FR-043**: The 7 silent failure paths identified in audit Section 8 MUST emit dedicated CloudWatch metrics in addition to X-Ray subsegments (FR-002, FR-005). X-Ray subsegments provide trace context for debugging; CloudWatch metrics provide 100% occurrence capture for alarming regardless of X-Ray sampling configuration. The required metrics are: `CircuitBreakerPersistenceFailure` (paths 1-2), `AuditEventPersistenceFailure` (path 3), `DownstreamNotificationFailure` (path 4), `TimeseriesFanoutPartialFailure` (path 5), `SelfHealingItemFetchFailure` (path 6), `ParallelFetcherErrors` (path 7).
- **FR-044**: The CloudWatch dashboard alarm status widget MUST display ALL configured alarms. The audit identified that the current widget shows only 6 of 30+ alarms (audit Section 6.3), giving operators a false sense of "all green" when non-displayed alarms may be firing.
- **FR-045**: All CloudWatch alarms monitoring metrics where absence indicates failure (heartbeat metrics, canary metrics, pipeline throughput metrics such as `NewItemsIngested`) MUST use `treat_missing_data = breaching`. Alarms monitoring error count metrics where zero data points means "no errors occurred" MUST use `treat_missing_data = notBreaching`. The configuration MUST be audited and corrected for all existing alarms.

**ADOT Coexistence Constraints (Round 4 — New)**

- **FR-046**: The SSE Lambda MUST NOT enable ADOT auto-instrumentation via the `AWS_LAMBDA_EXEC_WRAPPER=/opt/otel-handler` environment variable. ADOT MUST function solely as an OTLP receiver sidecar (Lambda Extension) that accepts spans on `localhost:4318` and exports them to X-Ray. ADOT auto-instrumentation wraps the handler entry point (conflicting with Powertools `@tracer.capture_lambda_handler` from FR-029) and patches botocore (conflicting with X-Ray SDK patching from Powertools `auto_patch=True`), causing duplicate handler wrapping and duplicate spans for every boto3 call.
- **FR-047**: The OTel `service.name` resource attribute configured for ADOT span export on the SSE Lambda MUST match the `POWERTOOLS_SERVICE_NAME` environment variable used by Powertools Tracer. X-Ray service map aggregates nodes by service name. Mismatched names cause the same Lambda to appear as two disconnected nodes in the service map — one for handler-phase subsegments and one for streaming-phase spans.

**SSE Reconnection Trace Correlation (Round 4 — New)**

- **FR-048**: All SSE connection traces MUST include a stable `session_id` annotation that persists across reconnections, enabling operators to query X-Ray by `session_id` to find all traces for a logical SSE session. On reconnection (new trace ID due to new `fetch()` call), the trace MUST additionally include a `previous_trace_id` annotation referencing the prior connection's X-Ray trace ID and a `connection_sequence` annotation (incrementing integer starting at 1). X-Ray does not support native span links (OpenTelemetry link data stored in segment metadata is not queryable via X-Ray console or API), so annotation-based correlation is the required pattern.

**Meta-Observability (Round 4 — New)**

- **FR-049**: The observability canary (extending FR-019) MUST additionally verify CloudWatch metric emission health on each canary interval. The canary emits a known test metric via `put_metric_data`, waits for the CloudWatch ingestion delay (typically 60-120 seconds), and then queries for it via `get_metric_statistics`. If the test metric is not retrievable within the expected window, the canary reports CloudWatch metric pipeline failure. This check is in addition to the X-Ray trace submission/retrieval check from FR-019.
- **FR-050**: The canary MUST report CloudWatch metric pipeline failure via a channel independent of CloudWatch alarms. If CloudWatch is degraded, a CloudWatch alarm on the canary's own metric may also fail to fire. The canary MUST use an out-of-band alerting mechanism (e.g., SNS direct publish to email/SMS, external monitoring API) for meta-observability failures. Source: AWS Well-Architected Operational Excellence Pillar — "never alert on failure using the same system that failed."
- **FR-051**: The canary Lambda MUST use a separate IAM role from all application Lambdas. If an IAM policy change revokes X-Ray or CloudWatch permissions from application Lambda roles, the canary MUST retain its permissions to detect and report the failure. This isolation ensures the canary is not affected by the same failure mode it is designed to detect.

**OTel-to-X-Ray Trace Context Bridging (Round 5 — New)**

- **FR-052**: The SSE Lambda's OTel SDK MUST be configured with the `AwsXRayLambdaPropagator` (from `opentelemetry-propagator-aws-xray`) as the global text map propagator. This propagator reads the `_X_AMZN_TRACE_ID` Lambda environment variable (updated by the Lambda runtime on each invocation) and extracts the trace ID, parent span ID, and sampling decision into the OTel context. Without this propagator, streaming-phase OTel spans carry auto-generated trace IDs that differ from the Lambda runtime's X-Ray facade segment, creating disconnected traces that violate SC-010 and SC-023. The propagator MUST be configured before any OTel spans are created during the streaming phase. Source: `opentelemetry-propagator-aws-xray` package — `AwsXRayLambdaPropagator.extract()` reads `os.environ.get("_X_AMZN_TRACE_ID")`.
- **FR-053**: The SSE Lambda's OTel `TracerProvider` MUST be configured with `AwsXRayIdGenerator` (from `opentelemetry-sdk-extension-aws`). This ID generator embeds the current unix timestamp in the first 4 bytes of generated trace IDs, matching X-Ray's trace ID format requirement (`1-{unix_epoch_hex}-{unique_hex}`). The default OTel `RandomIdGenerator` produces random bytes in the epoch field, causing X-Ray to misindex traces by time window. Note: when linking to an existing Lambda trace via FR-052, the trace ID comes from `_X_AMZN_TRACE_ID` (already valid). `AwsXRayIdGenerator` matters for any new root spans (e.g., canary test traces) and is a safety net for correct X-Ray interoperability.
- **FR-054**: The SSE Lambda MUST configure the OTel OTLP exporter to send spans to the ADOT Extension's local HTTP endpoint (`http://localhost:4318/v1/traces`). This MUST be configured via the `OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318` environment variable in Terraform or programmatically via the exporter constructor. The HTTP/protobuf exporter (`opentelemetry-exporter-otlp-proto-http`) is required — the gRPC exporter adds unnecessary binary dependency overhead in Lambda.

**OTel Span Lifecycle Management (Round 5 — New)**

- **FR-055**: The SSE Lambda's streaming generator MUST call `TracerProvider.force_flush()` in a `finally` block at the end of iteration, with a timeout of at least 5 seconds. The OTel `BatchSpanProcessor` buffers spans in memory and exports them to the ADOT Extension in batches (default batch timeout: 200ms). When the Lambda execution environment freezes after streaming completes, any spans buffered in the final batch interval are suspended in memory. Without `force_flush()`, these spans survive the freeze and are exported on the NEXT invocation — carrying stale trace IDs from the previous invocation, confusing operators and polluting traces. The `force_flush()` target is the local ADOT Extension OTLP receiver (localhost, sub-millisecond latency), not the remote X-Ray API.
- **FR-056**: The SSE Lambda's OTel `TracerProvider` MUST use the `parentbased_always_on` sampler (the OTel SDK default). This sampler reads the sampling decision from the parent context extracted by FR-052's propagator. When the Lambda runtime sets `Sampled=0` in `_X_AMZN_TRACE_ID`, the sampler suppresses span creation and export, preventing orphaned traces in X-Ray. When `Sampled=1`, all streaming-phase spans are created and exported normally. The system MUST NOT override the sampler to `always_on`, which would ignore the parent's sampling decision and create spans for unsampled invocations.

**OTel SDK Dependencies (Round 5 — New)**

- **FR-057**: The SSE Lambda's deployment package MUST include the following OTel Python packages for the streaming-phase instrumentation: (1) `opentelemetry-sdk` — core SDK including `TracerProvider`, `BatchSpanProcessor`, (2) `opentelemetry-sdk-extension-aws` — `AwsXRayIdGenerator` for X-Ray-compatible trace ID generation, (3) `opentelemetry-propagator-aws-xray` — `AwsXRayLambdaPropagator` for reading `_X_AMZN_TRACE_ID`, (4) `opentelemetry-exporter-otlp-proto-http` — OTLP HTTP exporter for sending spans to the ADOT Extension. These packages are ONLY required on the SSE Lambda — non-streaming Lambdas use Powertools Tracer (FR-029) which has no OTel dependency.

**Segment Document Size Guard (Round 5 — New)**

- **FR-058**: All X-Ray subsegment metadata payloads (both X-Ray SDK and OTel span attributes exported via ADOT) MUST be bounded to prevent exceeding the 64KB `PutTraceSegments` document size limit. Error messages MUST be truncated to 2,048 characters. HTTP response bodies MUST NOT be attached as metadata. Stack traces MUST be truncated to the first 10 frames. Documents exceeding 64KB are silently rejected by the X-Ray API (appearing in `UnprocessedTraceSegments` with no SDK-level error), causing permanent trace data loss with no alarm.

**Warm Invocation Trace Context Safety (Round 6 — New)**

- **FR-059**: The SSE Lambda's OTel trace context MUST be extracted from `_X_AMZN_TRACE_ID` at the start of each handler invocation, NOT at module level. On warm invocations, the Lambda runtime updates `_X_AMZN_TRACE_ID` to reflect the new invocation's trace ID, but module-level code executes only once (on cold start). Module-level `propagate.extract()` captures the first invocation's trace context and reuses it for ALL subsequent warm invocations, linking every streaming-phase OTel span to the wrong trace ID. The per-invocation extraction MUST occur inside the handler function, before any OTel spans are created. Source: AWS Lambda execution model — environment variables are updated by the runtime between invocations; module-level code runs only on INIT.

**SSE Lambda Auto-Patching Constraint (Round 6 — New)**

- **FR-060**: The SSE Lambda MUST initialize Powertools Tracer with `auto_patch=False` to prevent Powertools' global boto3/botocore patching from creating duplicate X-Ray subsegments alongside OTel spans during the streaming phase. With `auto_patch=True` (the default), Powertools patches `botocore.client.BaseClient._make_api_call` at import time. These patches persist globally and intercept ALL boto3 calls — including those made during the streaming phase after the handler returns. During streaming, each DynamoDB query and CloudWatch `put_metric_data` call creates BOTH an X-Ray subsegment (via the auto-patch, sent to the X-Ray daemon whose lifecycle during RESPONSE_STREAM is undocumented) AND an OTel span (via manual instrumentation, sent to ADOT). This dual-emission produces duplicate entries in X-Ray. Handler-phase AWS SDK calls MUST be traced via explicit `@tracer.capture_method` decorators rather than auto-patching.

**Alarm Threshold Calibration (Round 6 — New)**

- **FR-061**: Task 17 (alarm coverage) MUST review ALL existing CloudWatch alarm thresholds and align them with the 80-90% of timeout threshold standard applied to new alarms. The audit (Section 6.1) identified `analysis_latency_high` at 25s (42% of the Analysis Lambda's 60s timeout) as too generous — this threshold would not fire until nearly half the timeout is consumed, leaving insufficient margin for investigation and remediation before timeout failures begin. Existing alarms MUST be recalibrated to match the threshold methodology applied to new alarms. This review covers all pre-existing alarms, not only the one explicitly called out in the audit.

**ADOT Extension Operational Lifecycle (Round 7 — New)**

- **FR-062**: ~~The SSE Lambda's ADOT Lambda Extension MUST use the collector-only layer distribution.~~ **REWRITTEN in Round 8**: The SSE Lambda is container-based (ECR image with `python:3.13-slim` + custom bootstrap) — Lambda container images CANNOT use Lambda Layers. The ADOT collector MUST be embedded in the SSE Lambda's Dockerfile via multi-stage build using the collector-only container image from `public.ecr.aws/aws-observability/aws-otel-lambda-extension-amd64`. The collector-only image contains the OTel Collector binary that runs the OTLP receiver and X-Ray exporter pipeline, without bundled Python auto-instrumentation code (~35MB vs ~80MB). The Dockerfile MUST copy the collector binary to `/opt/extensions/collector` and the default configuration to `/opt/collector-config/config.yaml`. See FR-069 for deployment details and FR-070 for version pinning. Source: AWS Lambda Container Image docs — "You cannot add Lambda layers to container images"; ADOT ECR Public Gallery — `public.ecr.aws/aws-observability/`.
- **FR-063**: ~~The ADOT Extension layer ARN in Terraform MUST be pinned to a specific version.~~ **REWRITTEN in Round 8**: The ADOT container image reference in the SSE Lambda's Dockerfile MUST be digest-pinned (not tag-referenced). See FR-070 for requirements. The digest determines the embedded OTel Collector binary version, which defines the collector configuration schema, available receivers/exporters, and wire protocol compatibility. An unpinned tag (`:latest` or `:v0.40.0`) could silently introduce breaking changes when the Docker image is rebuilt. Unlike Lambda Layer ARNs which are region-specific, container images from ECR Public Gallery are region-agnostic. Source: OCI Distribution Specification — content-addressable image references.
- **FR-064**: The SSE Lambda MUST verify that the ADOT Extension's collector pipeline includes: (1) an OTLP HTTP receiver on port 4318, and (2) an AWS X-Ray exporter. The ADOT Extension's default collector configuration for Lambda includes this pipeline. If the default configuration needs modification (e.g., to strip unnecessary components for reduced memory footprint, or to add custom processors for attribute filtering), a custom collector configuration MUST be provided via the `OPENTELEMETRY_COLLECTOR_CONFIG_FILE` environment variable pointing to a configuration file in the Lambda deployment package. Unnecessary collector components (metrics pipelines, Prometheus receivers, zpages extensions) MUST be removed from custom configurations to minimize memory and CPU overhead within the 512MB allocation.

**TracerProvider Singleton Lifecycle (Round 7 — New)**

- **FR-065**: The SSE Lambda's OTel `TracerProvider` MUST be instantiated exactly once at module level (during Lambda INIT phase) and reused across all subsequent invocations on that execution environment. The `TracerProvider`, `BatchSpanProcessor`, and `OTLPSpanExporter` are invocation-independent infrastructure — they manage the export pipeline to the ADOT Extension. FR-059's per-invocation trace context extraction requirement applies ONLY to `AwsXRayLambdaPropagator.extract()` (reading the current `_X_AMZN_TRACE_ID` environment variable), NOT to `TracerProvider` instantiation. Creating a new `TracerProvider` per invocation spawns a new `BatchSpanProcessor` daemon thread (~1MB per thread) and OTLP HTTP connection, orphaning previous instances and causing monotonic memory growth that eventually triggers Lambda OOM termination on warm execution environments. Source: OTel Python SDK — `BatchSpanProcessor.__init__()` starts a daemon thread via `threading.Thread(daemon=True)`.

**OTel Export Failure Detection (Round 7 — New)**

- **FR-066**: The OTel Python SDK's `BatchSpanProcessor` silently catches all exporter exceptions (via `except Exception` in internal `_export()` method) and `force_flush()` returns `True` regardless of whether the underlying export succeeded or failed. This behavior is mandated by the OTel specification — exporters MUST NOT crash the application — and CANNOT be overridden via configuration. There is no built-in fail-fast mode. The SSE Lambda MUST NOT rely on `force_flush()` return value or exceptions to detect ADOT Extension failures. Instead, ADOT Extension health MUST be monitored via: (1) the X-Ray canary's end-to-end trace verification (FR-019/FR-036), which detects trace absence at aggregate level, and (2) Lambda Extension lifecycle logs (the runtime logs Extension registration and deregistration events, making Extension crashes visible in CloudWatch Logs). The practical mitigation is that the ADOT Extension runs on localhost with sub-millisecond latency — connection failures indicate the Extension process has crashed, not a transient network issue. Source: OTel Python SDK v1.39.1 — `BatchProcessor._export()` at `opentelemetry/sdk/_shared_internal/__init__.py`; OTel Specification, SDK Exporters section — "The SDK MUST NOT throw unhandled exceptions."

**OTel Span Semantic Conventions for X-Ray (Round 7 — New)**

- **FR-067**: Streaming-phase OTel spans MUST use semantic conventions that the ADOT X-Ray exporter maps to native X-Ray segment fields: (1) DynamoDB query spans MUST set `db.system = "dynamodb"` and `aws.dynamodb.table_names = ["{table_name}"]` attributes for X-Ray to render them as typed DynamoDB subsegments in the service map, (2) CloudWatch `put_metric_data` spans MUST set `rpc.service = "CloudWatch"` and `rpc.method = "PutMetricData"` for X-Ray to render them as typed CloudWatch subsegments, (3) error spans MUST record exceptions via the OTel span's `record_exception()` method (which sets `exception.type`, `exception.message`, and `exception.stacktrace` attributes per OTel semantic conventions), enabling X-Ray to populate the `cause` field with structured error data. Non-standard attribute names are stored as X-Ray metadata (non-indexed, non-searchable via X-Ray console or API filter expressions), rendering them invisible in default views and eliminating their value for operator debugging. Source: OTel Semantic Conventions for Database (https://opentelemetry.io/docs/specs/semconv/database/) and RPC (https://opentelemetry.io/docs/specs/semconv/rpc/); ADOT X-Ray exporter attribute mapping at `opentelemetry-collector-contrib/exporter/awsxrayexporter`.

**Propagator Configuration Hygiene (Round 7 — New)**

- **FR-068**: The SSE Lambda MUST NOT set the `OTEL_PROPAGATORS` environment variable in its Terraform configuration. The OTel Python SDK reads `OTEL_PROPAGATORS` at module import time and constructs the initial global propagator from it (default: `tracecontext,baggage` for W3C trace context). FR-052 requires `set_global_textmap(AwsXRayLambdaPropagator())` which unconditionally overwrites whatever `OTEL_PROPAGATORS` configured. Setting both creates misleading configuration — the environment variable appears active in Terraform/Lambda console but is silently overridden at runtime, causing operator confusion during debugging when the visible configuration does not match actual behavior. The `set_global_textmap()` call in application code is the single source of truth for propagator selection. Source: OTel Python SDK v1.39.1 — `opentelemetry/propagate/__init__.py`, module-level `environ.get(OTEL_PROPAGATORS)` parsing and `set_global_textmap()` override function.

**Container-Based ADOT Deployment (Round 8 — New)**

- **FR-069**: The SSE Lambda's Dockerfile MUST include the ADOT collector binary via multi-stage build from the official AWS ADOT Lambda Extension container image. The collector binary MUST be placed at `/opt/extensions/collector` and the default collector configuration at `/opt/collector-config/config.yaml`. Lambda discovers extensions by scanning `/opt/extensions/` during INIT — this is a Lambda service behavior independent of the runtime base image. The SSE Lambda uses `python:3.13-slim` with a custom bootstrap (not the AWS Lambda base image), which is fully compatible with Lambda Extensions. The multi-stage pattern `COPY --from=<adot-image> /opt/extensions/ /opt/extensions/` is the canonical approach for container-based Lambda Extensions. Source: AWS Lambda Container Image docs — "You cannot add Lambda layers to container images"; AWS Lambda Extensions API — extension discovery in `/opt/extensions/`.
- **FR-070**: The ADOT Lambda Extension container image reference in the SSE Lambda's Dockerfile MUST be digest-pinned (e.g., `public.ecr.aws/aws-observability/aws-otel-lambda-extension-amd64@sha256:{digest}`), NOT tag-referenced (`:latest` or `:v0.40.0`). Container image tags are mutable — the publisher can update the image behind a tag without notice. Digest pinning ensures reproducible builds and prevents silent introduction of breaking collector changes (schema changes, removed exporters) when the Docker image is rebuilt. Version updates MUST be explicit Dockerfile changes tracked in version control. Source: OCI Distribution Specification — content-addressable image references; Docker best practices for reproducible builds.

**OTel Resource Configuration (Round 8 — New)**

- **FR-071**: The SSE Lambda MUST set the `OTEL_SERVICE_NAME` environment variable in its Terraform configuration to a value matching `POWERTOOLS_SERVICE_NAME` (per FR-047). The OTel `AwsLambdaResourceDetector` does NOT set the `service.name` resource attribute. Without `OTEL_SERVICE_NAME`, the OTel SDK defaults to `unknown_service`, causing the SSE Lambda to appear as an `unknown_service` node in the X-Ray service map — indistinguishable from any other unconfigured service. This breaks SC-028 (service map correctness). The `OTEL_SERVICE_NAME` env var is read by `Resource.create()` during `TracerProvider` initialization and does not require code changes. Source: OTel Python SDK — `opentelemetry.sdk.resources.Resource.create()` reads `OTEL_SERVICE_NAME`; AWS X-Ray uses `service.name` for service map node naming.
- **FR-072**: The SSE Lambda's `TracerProvider` MUST be configured with `AwsLambdaResourceDetector` to populate resource attributes (`cloud.provider`, `cloud.region`, `faas.name`, `faas.version`, `faas.instance`, `faas.max_memory`) on all emitted spans. These attributes provide Lambda execution context in X-Ray trace metadata, enabling operators to identify which Lambda function, version, and execution environment produced each span. The resource configuration MUST merge `Resource.create()` (which reads `OTEL_SERVICE_NAME`) with `AwsLambdaResourceDetector().detect()` to combine service identity with Lambda metadata. Source: `opentelemetry-sdk-extension-aws` package — `AwsLambdaResourceDetector` in `resource/_lambda.py`.

**BatchSpanProcessor Lambda Configuration (Round 8 — New)**

- **FR-073**: The SSE Lambda's `BatchSpanProcessor` MUST be configured with Lambda-tuned parameters: `schedule_delay_millis=1000` (1 second, down from default 5 seconds), `max_queue_size=256` (down from default 2048), `max_export_batch_size=64` (down from default 512). The defaults are designed for long-running server processes with high span throughput. The SSE Lambda produces a bounded number of spans per streaming session (~50-200 over 15 seconds). Reduced parameters consume less memory (smaller queue buffer), export more frequently (1s vs 5s), and reduce the span count at risk if `force_flush()` is not reached. Configuration MUST be via `OTEL_BSP_SCHEDULE_DELAY`, `OTEL_BSP_MAX_QUEUE_SIZE`, and `OTEL_BSP_MAX_EXPORT_BATCH_SIZE` environment variables in Terraform for visibility, or via constructor parameters in application code. Source: OTel Python SDK — `BatchSpanProcessor` defaults and `OTEL_BSP_*` environment variable support.

**Two-Hop Flush Architecture Acknowledgment (Round 9 — New)**

- **FR-074**: The spec acknowledges that span export from the SSE Lambda to X-Ray involves TWO asynchronous hops: (1) SDK → ADOT Extension via OTLP HTTP on localhost:4318 (fast, sub-millisecond, reliable — handled by FR-055's `force_flush()`), and (2) ADOT Extension → X-Ray backend via HTTPS (slow, measured in hundreds of milliseconds, subject to KNOWN race condition documented in aws-otel-lambda#886 and opentelemetry-lambda#224). The decouple processor in the ADOT Lambda collector mitigates the second hop by holding the Extension from signaling completion until queued data is exported. However, under worst-case timing (span.end() as the very last operation before function exit), the collector context may be canceled before HTTP exports complete, resulting in span loss with error pattern "context canceled" + "Exporting failed. Dropping data." Per-invocation detection of this loss is NOT possible because it occurs in the Extension process. The canary (FR-019/FR-036) provides aggregate detection. The system MUST NOT add fallback logic or retry mechanisms around this — per FR-018, the Extension's internal failures are the Extension's responsibility. The ADOT Extension's decouple processor is the accepted mitigation, not a fallback.
- **FR-075**: The ADOT Extension's collector configuration for the SSE Lambda MUST include the `decouple` processor in the pipeline to prevent the Lambda-specific span drop race condition. The decouple processor separates span reception from export, ensuring the Extension does not call the Next API until all queued spans are exported to the X-Ray backend. Without the decouple processor, the Extension signals invocation completion while HTTP exports are still in-flight, and the execution environment freezes before exports complete. The decouple processor is included in the ADOT Lambda Extension's default configuration — a custom `collector.yaml` MUST preserve it if FR-064's configuration modifications remove it. Source: https://github.com/open-telemetry/opentelemetry-lambda/blob/main/collector/processor/decoupleprocessor/README.md

**OTel SDK Lambda Safety (Round 9 — New)**

- **FR-076**: The SSE Lambda MUST NOT call `BotocoreInstrumentor().instrument()` or any OTel auto-instrumentation for AWS SDK clients. The SSE Lambda's instrumentation architecture is: (a) handler-phase AWS SDK calls traced via explicit Powertools `@tracer.capture_method` decorators (per FR-060), (b) streaming-phase AWS SDK calls traced via manual OTel span creation with semantic conventions (per FR-067). Enabling `BotocoreInstrumentor` would patch `botocore.client.BaseClient._make_api_call` globally, creating OTel spans for EVERY AWS SDK call — including handler-phase calls that are already traced by Powertools, producing duplicate entries in X-Ray. The prohibition on `auto_patch=True` (FR-060) prevents Powertools' X-Ray SDK patching but does NOT prevent OTel botocore instrumentation, which is a separate mechanism. Both MUST be prohibited.
- **FR-077**: The SSE Lambda's `TracerProvider` MUST be instantiated with `shutdown_on_exit=False`. The default `shutdown_on_exit=True` registers a Python `atexit` handler that calls `TracerProvider.shutdown()` during interpreter finalization. In Lambda, `atexit` handlers fire during execution environment recycling (not between warm invocations). In Python 3.13+, the `ConcurrentMultiSpanProcessor.shutdown()` can race with a pending export, causing `RuntimeError: cannot schedule new futures after shutdown`. Since FR-055 mandates `force_flush()` at end of each streaming session, the `atexit` handler is redundant. Setting `shutdown_on_exit=False` eliminates the race with zero functional impact. Source: opentelemetry-python#4461 — `ConcurrentMultiSpanProcessor` shutdown race condition.

**Canary Robustness (Round 9 — New)**

- **FR-078**: The X-Ray canary (FR-019) MUST implement retry-with-backoff for trace retrieval. After submitting test traces, the canary queries `BatchGetTraces` at 30 seconds, then retries at 60 seconds if traces are not found. X-Ray's propagation delay is "generally 30 seconds" per the AWS FAQ but is NOT a deterministic SLA — under regional load, indexing may take longer. The `BatchGetTraces` API cannot distinguish "not yet indexed" from "trace permanently lost" — both result in the trace ID appearing in `UnprocessedTraceIds` or being absent from the `Traces` response array. The canary MUST treat these cases identically and only report failure after the final retry. The 2-consecutive-interval alarm threshold (FR-020) provides the outer detection boundary. Source: https://aws.amazon.com/xray/faqs/
- **FR-079**: The canary (FR-049) MUST classify CloudWatch `put_metric_data` errors into two categories and handle them differently: (a) **Transient errors** (`ThrottlingException`, `InternalFailure`, `ServiceUnavailable`, `InternalServiceError`) — retry with exponential backoff (max 3 retries), then report failure if all retries exhausted, (b) **Permanent IAM/credential errors** (`AccessDeniedException`, `NotAuthorized`, `InvalidClientTokenId`) — trigger IMMEDIATE out-of-band alert via FR-050 with zero retries. IAM revocation is the EXACT failure mode that FR-051's separate IAM role is designed to detect. Without error classification, an IAM revocation on application Lambda roles is silently retried for multiple canary intervals, losing critical response time. The canary's own metric emission uses its separate IAM role (FR-051) and is unaffected by application role changes — but the canary's CloudWatch HEALTH CHECK (which verifies application metrics are flowing) must detect when application roles lose permissions. Source: https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_PutMetricData.html
- **FR-080**: The canary SHOULD support a secondary out-of-band alerting channel (external webhook endpoint such as PagerDuty or Opsgenie HTTPS API) in addition to SNS direct publish (FR-050). During simultaneous AWS regional failures affecting both CloudWatch AND SNS, the canary has no alerting path with SNS alone. Industry best practice from AWS outage post-mortems (e.g., 2017 S3 outage) is to use at least two independent alerting channels. This is a SHOULD (not MUST) because simultaneous CloudWatch + SNS regional failure is a low-probability event, but production deployments handling critical workloads benefit from this redundancy.

**Frontend SSE Server-Side Requirements (Round 9 — New)**

- **FR-081**: The SSE Lambda server MUST check the `Last-Event-ID` request header on connection establishment and resume the event stream from the corresponding position. If the client sends `Last-Event-ID: {id}` (per FR-033's client-side requirement), the server MUST NOT replay events already delivered (events with id <= Last-Event-ID). Without server-side support, FR-033's client-side `Last-Event-ID` propagation is hollow — the client sends the header but the server ignores it, causing duplicate events on reconnection. If server-side stream resumption is not feasible for the current SSE Lambda architecture (e.g., events are ephemeral and not stored), the server MUST respond with a `X-SSE-Resume-Supported: false` header so the client can implement idempotent deduplication.
- **FR-082**: The SSE Lambda Function URL's CORS configuration MUST include `x-amzn-trace-id` in `Access-Control-Expose-Headers`. Per the CORS specification, browsers can only read response headers explicitly listed in `Access-Control-Expose-Headers`. FR-048 requires the client to capture the response `X-Amzn-Trace-Id` header value as `previous_trace_id` for reconnection trace correlation. Without `ExposeHeaders` including this header, the browser JavaScript cannot access it, making FR-048's reconnection trace correlation impossible. This is a hard prerequisite for FR-048, not optional.
- **FR-083**: The frontend SSE client's reconnection logic (FR-033) MUST distinguish between graceful server close and network error using the `ReadableStreamDefaultReader.closed` Promise (fulfills on graceful close, rejects on error). On graceful server close (expected — e.g., Lambda timeout, stream complete): reconnect with the SSE `retry:` field value or a short fixed delay (1-3 seconds). On network error (unexpected — e.g., WiFi drop, server crash): reconnect with exponential backoff plus jitter per FR-033's existing specification. Uniform backoff on all close types unnecessarily delays reconnection after expected server-initiated closes, degrading user experience.

**BSP Default Correction (Round 9 — Assumption Fix)**

- **FR-084**: The spec's Round 5 assumption regarding "default batch timeout of 200ms" is INCORRECT and MUST be corrected. The actual `BatchSpanProcessor` `schedule_delay_millis` default is **5000ms** (5 seconds), confirmed by the OTel Python SDK source code and the OTel Specification. FR-073 correctly overrides this to 1000ms, so runtime behavior is unaffected. The incorrect 200ms figure in the assumption and edge case text (originally added in Round 5) caused downstream reasoning errors about the flush window size — the actual default means spans buffer 25x longer than documented. This FR corrects the assumption; no runtime change is required.

**Client Disconnect Handling (Round 10 — New)**

- **FR-085**: The SSE Lambda's streaming generator MUST catch `BrokenPipeError` (Python's signal that the client has disconnected mid-write) in its event dispatch loop and set the OTel span attribute `client.disconnected=true` on the active span before propagating the exception. Python custom runtimes have NO standard mechanism to detect client disconnect (unlike Node.js `StreamifyResponse` which provides `metadata.onAbort`). `BrokenPipeError` on `response.write()` is the ONLY available signal. When caught: (a) set `client.disconnected=true` annotation on the current streaming-phase span, (b) set span status to `OK` (not `ERROR` — client disconnect is expected behavior, not a server fault), (c) skip remaining DynamoDB polls, (d) proceed to `finally` block for `force_flush()`. Without this handling, post-disconnect spans report successful operations that no client received, inflating success metrics. Source: https://docs.aws.amazon.com/lambda/latest/dg/configuration-response-streaming.html — "Streamed responses are not interrupted or stopped when the invoking client connection is broken."

**BSP Queue Size Correction (Round 10 — Amends FR-073)**

- **FR-086**: FR-073's `max_queue_size` MUST be recalculated based on expected span throughput per invocation and set to >= 2x the expected maximum. Calculation: 15-second streaming session × ~15 DynamoDB polls/second × ~3 spans per poll cycle (db query span + event dispatch span + metric emit span) = ~675 spans per invocation. 2x safety margin = 1,350 minimum. **FR-073 is hereby AMENDED**: `max_queue_size` changes from `512` to `1500`. The `max_export_batch_size` remains at `64` (unchanged) and `schedule_delay_millis` remains at `1000` (unchanged). The 2x margin accounts for: (a) variable poll frequency under load, (b) additional spans from error handling paths, (c) spans created between `force_flush()` calls during batch intervals. The silent drop behavior of `BatchSpanProcessor` (logs WARNING only, no metric, no exception) makes undersized queues particularly dangerous — span loss is invisible without explicit log monitoring. Source: OTel Python SDK — `BatchSpanProcessor.on_end()` uses `queue.put_nowait()`, catches `queue.Full`, logs warning.

**Canary IsPartial Trace Check (Round 10 — New)**

- **FR-087**: The X-Ray canary (FR-078) MUST check the `IsPartial` flag on `TraceSummary` objects returned by `GetTraceSummaries` or `BatchGetTraces` for its test traces. If `IsPartial=true` after the final retry attempt (per FR-078's retry-with-backoff): (a) emit a dedicated CloudWatch metric `canary.trace.partial` with value 1, (b) report the trace verification as **degraded** (not complete success AND not complete failure), (c) include `is_partial=true` in the canary's OTel span attributes. A trace with `IsPartial=true` means not all segments have been received by X-Ray — critical streaming-phase spans from ADOT may be missing while the trace "exists" from the canary's perspective. Without this check, the canary reports false-green when ADOT span export is failing but the Lambda runtime's facade segment arrived successfully. Source: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html — "X-Ray tags traces as incomplete or partial."

**SSE Reconnection Control (Round 10 — New)**

- **FR-088**: The SSE Lambda MUST emit a `retry:` field in the initial SSE frame of each connection with a recommended reconnection interval in milliseconds (e.g., `retry: 3000\n`). The SSE specification (WHATWG HTML Living Standard, Section 9.2) defines this field as the server-controlled mechanism for client reconnection timing. Without it, clients use their own implementation-defined default (varies by browser/library). The `retry:` value MUST be: (a) emitted as the FIRST field in the SSE stream (before any `data:` events), (b) set to 3000ms (3 seconds) as default, (c) configurable via environment variable `SSE_RETRY_MS` for per-environment tuning. Note: since the frontend uses `fetch()+ReadableStream` (not `EventSource`), the `retry:` field is NOT auto-parsed — the frontend must explicitly parse it per FR-089. Source: https://html.spec.whatwg.org/multipage/server-sent-events.html — Section 9.2, "retry" field.
- **FR-089**: The frontend SSE client (FR-033/FR-083) MUST parse the `retry:` field from the server's SSE stream and use its value as the reconnection delay for graceful server closes (overriding FR-083's default 1-3 second range with the server-specified value). Since the frontend uses `fetch()+ReadableStream` (NOT `EventSource`), the SSE protocol fields (`retry:`, `id:`, `event:`, `data:`) are NOT automatically parsed. The client MUST implement SSE field parsing for at least `retry:` and `data:` fields. If no `retry:` field is received, fall back to FR-083's default behavior. This ensures server-controlled reconnection timing without requiring `EventSource` (which cannot send custom headers per FR-032's CORS requirements).

**Explicit Decouple Processor in Custom Config (Round 10 — Strengthens FR-075)**

- **FR-090**: The custom ADOT collector YAML (created per FR-069 for the container-based SSE Lambda) MUST explicitly include the `decouple` processor in the `service.pipelines.traces.processors` list. The ADOT Lambda Layer's automatic decouple processor injection (documented in the decouple processor README: "the OpenTelemetry Lambda Layer automatically configures the decouple processor when the batch processor is used") ONLY applies to Lambda Layer deployments. Container-based deployments with custom collector configurations (FR-064) receive NO automatic processor injection. Without explicit inclusion, the SSE Lambda's span export reverts to the known ~30% drop rate (aws-otel-lambda#886). This FR strengthens FR-075 by making the container-specific requirement explicit. The pipeline order MUST be: `processors: [decouple, batch]` (decouple before batch). Source: https://github.com/open-telemetry/opentelemetry-lambda/blob/main/collector/processor/decoupleprocessor/README.md

**BSP Queue Overflow Observability (Round 10 — New)**

- **FR-091**: The SSE Lambda MUST configure Python logging to capture `BatchSpanProcessor` WARNING-level log messages and emit a structured log entry when span queue overflow occurs. The OTel SDK logs `"Queue is full, dropping span"` at WARNING level when `queue.put_nowait()` raises `Full`. This log line is the ONLY observable signal of span loss (no metric, no exception, no span attribute). The structured log entry MUST include: (a) `span_queue_overflow=true` field for CloudWatch Logs Insights filtering, (b) current queue size, (c) span name being dropped. A CloudWatch Logs metric filter on `span_queue_overflow=true` SHOULD be created to provide alarming on span loss. This is defense-in-depth — FR-086's queue sizing should prevent overflow, but silent data loss requires independent detection.

### Key Entities

- **X-Ray Trace**: A distributed trace spanning one or more service components, identified by a trace ID in the format `1-{hex_timestamp}-{24_hex_digits}`. Propagated via the `X-Amzn-Trace-Id` header.
- **X-Ray Subsegment**: A named, timed span within a trace representing a discrete operation (e.g., a DynamoDB query, a connection acquisition). Can carry annotations (indexed, searchable key-value pairs) and metadata (non-indexed structured data).
- **X-Ray Annotation**: An indexed key-value pair attached to a subsegment. Annotations are searchable in the X-Ray console and API, making them the primary mechanism for filtering traces by business attributes (e.g., `event_type = "bucket_update"`, `cache_hit_rate = 0.85`).
- **X-Ray Canary**: A scheduled probe that validates X-Ray ingestion health and data integrity by submitting and querying test traces, reporting results via a separate (non-X-Ray) channel.
- **X-Ray Group**: A filtered view of traces defined by a filter expression (e.g., `fault = true`). Groups automatically generate CloudWatch metrics, enabling alarm-based monitoring from trace data without manual metric emission.
- **X-Ray Sampling Rule**: A server-side configuration that controls which requests are traced. Rules specify a reservoir (guaranteed minimum per second) and a fixed rate (percentage of additional requests). Rules are evaluated by priority (lowest number first).

## Assumptions

- ~~The X-Ray SDK auto-patches HTTP client libraries including the one used by the SendGrid SDK.~~ **INVALIDATED in Round 2**: The SendGrid SDK uses `python-http-client` which uses Python's stdlib `urllib.request`. The X-Ray SDK does NOT auto-patch `urllib`. SendGrid HTTP calls require explicit X-Ray subsegments (see FR-028).
- The existing CloudWatch RUM configuration (`enable_xray = true`) already generates browser-side X-Ray trace contexts. The frontend needs to extract and propagate these, not generate new ones. CloudWatch RUM's web client auto-injects `X-Amzn-Trace-Id` headers on `fetch()` calls when `addXRayTraceIdHeader: true` is configured in the http telemetry options.
- When both the publishing and subscribing Lambda functions have Active X-Ray tracing enabled, AWS automatically propagates trace context through SNS via the `AWSTraceHeader` system attribute. No manual MessageAttributes configuration is needed for cross-Lambda trace linking.
- Auto-instrumentation patches all supported AWS SDK calls (DynamoDB, SNS, CloudWatch, etc.) transparently. Explicit subsegments are needed only for business logic operations not covered by auto-patching.
- The X-Ray canary's query latency window (time between trace submission and retrievability) is typically 5-30 seconds due to X-Ray's eventual consistency model.
- Standard AWS managed policies for X-Ray provide sufficient permissions for Lambda trace emission and telemetry reporting.
- ~~The X-Ray SDK's `xray_recorder.begin_segment()` API can create independent segments outside the Lambda runtime's auto-created segment. These segments can carry a specified trace ID to link them to the original invocation, appearing as part of the same distributed trace.~~ **INVALIDATED in Round 3**: `begin_segment()` is a documented no-op in Lambda. The SDK's `LambdaContext.put_segment()` silently discards segments with a log warning (source: `aws_xray_sdk/core/lambda_launcher.py:55-59`). The `FacadeSegment` raises `FacadeSegmentMutationException` on all mutation operations including `close()`, `put_annotation()`, `put_metadata()`, `set_aws()`, `add_exception()`, and `serialize()`. Additionally, the X-Ray daemon within the Lambda execution environment may begin shutting down after the handler returns in `RESPONSE_STREAM` mode, making even raw UDP emission unreliable. **Replacement**: The SSE Lambda uses a Lambda Extension-based tracing mechanism (e.g., ADOT — AWS Distro for OpenTelemetry) that runs as a separate process with its own lifecycle. Lambda Extensions receive INIT, INVOKE, and SHUTDOWN lifecycle events and continue running after the handler returns. The extension accepts trace spans via local OTLP endpoint and exports to X-Ray, providing trace data emission during response streaming without depending on the X-Ray SDK's LambdaContext.
- **(Round 2)** The X-Ray SDK supports `str`, `int`, `float`, and `bool` annotation types. All annotations defined in this spec use these types. `None` values are silently dropped, so annotations MUST use explicit sentinel values or be omitted when the value is unavailable.
- **(Round 2)** Adding Powertools Tracer as a dependency to all non-streaming Lambda containers adds approximately 5-10MB to the deployment package and 50-100ms to cold start. This is acceptable given that the Lambdas already include the aws-xray-sdk dependency.
- **(Round 3)** The `EventSource` browser API does not support custom HTTP headers. This is defined in the WHATWG HTML Living Standard, Section 9.2. The constructor signature is `EventSource(url, eventSourceInitDict)` where `eventSourceInitDict` only supports `withCredentials: boolean`. CloudWatch RUM's automatic trace header injection patches `window.fetch()` and `XMLHttpRequest.prototype.open` but does NOT patch `EventSource`. Trace propagation for SSE connections requires using `fetch()` + `ReadableStream` instead.
- **(Round 3)** Adding the ADOT Lambda Extension to the SSE Lambda adds approximately 40-60MB to the execution environment and 50-200ms to cold start (runs concurrently with Lambda INIT, partially masking overhead). The extension provides an OTLP receiver on localhost that accepts trace spans independently of the X-Ray SDK lifecycle.
- **(Round 3)** X-Ray's region-level throughput limit is 2,600 segments per second. Beyond this, `PutTraceSegments` returns `ThrottledException` and affected segments are listed in `UnprocessedTraceSegments` — data is permanently lost, not queued. The X-Ray daemon provides batching and burst capacity but cannot sustain above-limit throughput.
- **(Round 3)** The default `threading.local()` context storage used by the X-Ray SDK correctly propagates across `asyncio.new_event_loop().run_until_complete()` boundaries when called from the same thread. This is because `run_until_complete()` executes coroutines on the calling thread. The X-Ray SDK's `AsyncContext` alternative uses `TaskLocalStorage`, which is scoped to `asyncio.Task` objects and does NOT propagate across event loop boundaries.
- **(Round 3)** At current traffic levels (<1M requests/month), 100% X-Ray sampling costs less than $5/month, well within the cost guard thresholds. The free tier covers the first 100,000 traces recorded per month.
- **(Round 4)** CloudFront is not in the current architecture — removed in Features 1203-1207. Browser-to-backend trace propagation via `X-Amzn-Trace-Id` headers reaches API Gateway and Lambda Function URLs directly without intermediary header manipulation. CloudFront treats `X-Amzn-Trace-Id` as a restricted header and replaces client-sent values with its own; re-introducing CloudFront would break browser-originated trace propagation (see edge case).
- **(Round 4)** ADOT Lambda Extension in "sidecar-only" mode (OTLP receiver without auto-instrumentation) does not conflict with Powertools Tracer or the X-Ray SDK. Conflicts arise only when ADOT auto-instrumentation is enabled via the `AWS_LAMBDA_EXEC_WRAPPER` environment variable, which activates handler wrapping (`AwsLambdaInstrumentor`) and botocore patching (`BotocoreInstrumentor`).
- **(Round 4)** CloudWatch metric ingestion delay (time between `put_metric_data` and metric availability via `get_metric_statistics`) is typically 60-120 seconds. The canary's CloudWatch health check query window must account for this delay. Source: AWS CloudWatch API documentation.
- **(Round 4)** At current traffic levels with 100% X-Ray sampling, the gap between X-Ray trace availability and CloudWatch metric availability for error detection is effectively zero — every error has both an X-Ray trace and a CloudWatch metric. The dual-instrumentation requirement for silent failure paths (FR-043) provides insurance for the future when production sampling may be reduced below 100%.
- **(Round 5)** The OTel `AwsXRayLambdaPropagator` reads the `_X_AMZN_TRACE_ID` Lambda environment variable and extracts `Root` (trace ID), `Parent` (span ID), and `Sampled` (sampling decision) into the OTel context. This provides the bridge between the Lambda runtime's X-Ray trace and the OTel SDK's span hierarchy. When combined with `parentbased_always_on` sampler, the OTel SDK honors the Lambda runtime's sampling decision. Source: `opentelemetry-propagator-aws-xray` package, PyPI.
- ~~**(Round 5)** The OTel `BatchSpanProcessor`'s default batch timeout of 200ms means streaming-phase spans are exported to the ADOT Extension incrementally during the 15-second streaming lifecycle. The `force_flush()` call at end of generator handles only the final partial batch (spans created in the last <200ms before generator exhaustion). The flush target is localhost (ADOT Extension), not remote X-Ray API.~~ **CORRECTED in Round 9**: The actual `BatchSpanProcessor` `schedule_delay_millis` default is **5000ms** (5 seconds), NOT 200ms. FR-073 overrides to 1000ms, making the corrected statement: "With FR-073's tuned 1000ms schedule delay, streaming-phase spans are exported approximately every 1 second. The `force_flush()` call at end of generator handles the final partial batch (spans created in the last <1000ms before generator exhaustion)." The original 200ms figure was incorrect and caused downstream reasoning errors about the flush window. Source: OTel Python SDK v1.39.1 — `BatchSpanProcessor` default `schedule_delay_millis=5000`; OTel Specification, BatchSpanProcessor section.
- **(Round 5)** The ADOT Extension layer in sidecar-only mode (no `AWS_LAMBDA_EXEC_WRAPPER`, OTLP receiver only) is approximately 35-45MB unzipped. The additional OTel Python SDK packages (FR-057) add approximately 5-10MB. Combined with the SSE Lambda's existing deployment package, the total remains well within the 250MB Lambda deployment limit.
- **(Round 5)** The X-Ray `PutTraceSegments` API rejects segment documents exceeding 64KB. The X-Ray SDK and OTel SDK both mitigate this by emitting completed subsegments/spans as independent documents, but individual documents with large metadata can still exceed the limit. The 50-annotation-per-subsegment limit (2,048 chars each) produces ~100KB of annotation data at maximum, which exceeds 64KB — in practice, annotations are small (typically <50 chars each), keeping documents well under the limit.
- **(Round 6)** The `_X_AMZN_TRACE_ID` environment variable is updated by the Lambda runtime at the start of each invocation and remains stable for the duration of that invocation. `AwsXRayLambdaPropagator.extract()` reads `os.environ.get("_X_AMZN_TRACE_ID")` on each call with no internal caching. Module-level extraction captures only the INIT-phase value, which becomes stale on warm invocations.
- **(Round 6)** The SSE Lambda's current memory allocation of 512MB provides sufficient headroom for the ADOT Extension (~40-60MB) plus OTel SDK packages (~5-10MB). No memory increase is required. Source: Terraform `infrastructure/terraform/main.tf` — `memory_size = 512`.
- **(Round 6)** The Lambda RESPONSE_STREAM runtime calls the generator's `.close()` method or propagates `BrokenPipeError` when the client disconnects mid-stream, ensuring the generator's `finally` block executes reliably. The `finally` block is NOT deferred to garbage collection.
- **(Round 6)** Powertools Tracer's `auto_patch=True` patches `botocore.client.BaseClient._make_api_call` at module import time. These patches are global (not scoped to the handler phase) and remain active during the streaming phase. The X-Ray daemon's ability to accept subsegments created by auto-patched calls during RESPONSE_STREAM streaming is undocumented by AWS and may be unreliable — the daemon process lifecycle after handler return is not guaranteed.
- **(Round 7)** The ADOT Lambda Extension's default collector configuration for Lambda includes an OTLP HTTP receiver on port 4318 and an AWS X-Ray exporter pipeline. In sidecar-only mode (no `AWS_LAMBDA_EXEC_WRAPPER`), the collector receives OTel spans via OTLP and exports them to X-Ray without any auto-instrumentation. If the default config includes extraneous components, a custom `collector.yaml` is required. Source: ADOT Lambda Extension GitHub repository — `adot/collector/config.yaml`.
- ~~**(Round 7)** The SSE Lambda currently uses zero Lambda layers (S3-packaged deployment). Adding 1 ADOT Extension layer is well within the 5-layer Lambda limit. The combined unzipped size (Lambda package + ADOT collector-only layer ~35MB) remains within the 250MB Lambda deployment limit. Source: AWS Lambda quotas — 5 layers per function, 250MB unzipped deployment limit.~~ **INVALIDATED in Round 8**: The SSE Lambda is container-based (ECR image with `python:3.13-slim` + custom bootstrap), NOT ZIP-deployed. Lambda container images CANNOT use Lambda Layers (source: AWS Lambda Container Image docs — "You cannot add Lambda layers to container images"). ADOT is embedded in the container image via multi-stage Dockerfile build. Container images have a 10GB size limit, not 250MB. The 5-layer limit is irrelevant.
- **(Round 7)** The X-Ray daemon process (~16MB) and ADOT Lambda Extension (~40-60MB) coexist within the SSE Lambda's 512MB memory allocation. Combined tracing infrastructure overhead is ~76MB (15% of total), leaving ~436MB for the Python runtime and application code. This is validated against the current SSE Lambda memory usage. Source: Terraform `infrastructure/terraform/main.tf` — SSE Lambda `memory_size = 512`, zero layers currently.
- **(Round 7)** The OTel Python SDK's `BatchSpanProcessor` spawns one daemon thread per instantiation. Module-level `TracerProvider` instantiation creates exactly one thread for the lifetime of the execution environment. Repeated instantiation without `shutdown()` orphans daemon threads (~1MB each), causing monotonic memory growth. Source: OTel Python SDK — `BatchSpanProcessor.__init__()` starts `threading.Thread(daemon=True)`.
- **(Round 7)** The `set_global_textmap()` function in the OTel Python SDK unconditionally overwrites the global propagator, including any propagator loaded from the `OTEL_PROPAGATORS` environment variable at import time. The env var sets the initial default; `set_global_textmap()` is effective at any subsequent call. Source: OTel Python SDK v1.39.1 — `opentelemetry/propagate/__init__.py`.
- **(Round 8)** The SSE Lambda is container-based, using `python:3.13-slim` as its base image with a custom `bootstrap` script that implements the Lambda Runtime API for RESPONSE_STREAM mode. Lambda Extensions (including ADOT) are Lambda service-level features, not runtime-level — they are discovered by scanning `/opt/extensions/` during INIT and launched as separate processes. The ADOT Extension registers with the Extensions API (`/2020-01-01/extension/register`) independently of the custom runtime. No bootstrap modifications are needed. Source: AWS Lambda Extensions API documentation.
- **(Round 8)** AWS publishes ADOT Lambda Extension images on the public ECR gallery at `public.ecr.aws/aws-observability/`. The multi-stage Dockerfile pattern `COPY --from=adot-image /opt/extensions/ /opt/extensions/` extracts the collector binary. Image tags must be digest-pinned (`@sha256:...`) for reproducible builds. Source: ECR Public Gallery — AWS Observability.
- **(Round 8)** The OTel `AwsLambdaResourceDetector` (from `opentelemetry-sdk-extension-aws`) reads `AWS_LAMBDA_FUNCTION_NAME`, `AWS_LAMBDA_FUNCTION_VERSION`, `AWS_REGION`, `AWS_LAMBDA_LOG_STREAM_NAME`, and `AWS_LAMBDA_FUNCTION_MEMORY_SIZE` environment variables. These are set by the Lambda service regardless of base image. The detector sets `cloud.provider`, `cloud.platform`, `cloud.region`, `faas.name`, `faas.version`, `faas.instance`, and `faas.max_memory` — but does NOT set `service.name`. Without explicit `OTEL_SERVICE_NAME`, the default is `unknown_service`. Source: `opentelemetry-python-contrib` — `sdk-extension/opentelemetry-sdk-extension-aws/src/opentelemetry/sdk/extension/aws/resource/_lambda.py`.
- **(Round 8)** The OTel `BatchSpanProcessor` default configuration (`schedule_delay_millis=5000`, `max_queue_size=2048`, `max_export_batch_size=512`) is designed for long-running server processes, not Lambda. For the SSE Lambda (900s streaming), the 5s delay is functionally acceptable since `force_flush()` drains the entire queue synchronously. However, reduced defaults (`schedule_delay_millis=1000`, `max_queue_size=256`, `max_export_batch_size=64`) consume less memory and export more frequently, reducing the risk of data loss if `force_flush()` is not reached. Source: OTel Python SDK v1.39.1 — `BatchSpanProcessor` defaults and `OTEL_BSP_*` env vars.
- **(Round 9)** The ADOT Lambda Extension's decouple processor (Lambda-specific batch processor) decouples span reception from export, preventing the Extension from signaling invocation completion until all queued spans are exported to the X-Ray backend. This is the accepted mitigation for the known span drop race condition (aws-otel-lambda#886). The decouple processor is included in the ADOT Extension's default Lambda configuration. Source: https://github.com/open-telemetry/opentelemetry-lambda/blob/main/collector/processor/decoupleprocessor/README.md
- **(Round 9)** The `TracerProvider(shutdown_on_exit=False)` parameter prevents Python `atexit` handler registration. In Lambda, the `atexit` handler is redundant because FR-055 mandates `force_flush()` at end of each streaming session, and the ADOT Extension has its own SHUTDOWN phase for final export. Disabling `atexit` eliminates the thread pool shutdown race (opentelemetry-python#4461) with zero functional impact.
- **(Round 9)** X-Ray trace propagation delay (time between `PutTraceSegments` and availability via `BatchGetTraces`) is documented as "generally 30 seconds" in the AWS FAQ. This is eventual consistency, NOT a deterministic SLA. Under regional load or during X-Ray service degradation, delay may exceed 30 seconds. The canary's retry-with-backoff (30s then 60s per FR-078) accounts for this variability.
- **(Round 9)** CloudWatch `put_metric_data` error codes fall into two categories: (a) transient/retryable: `ThrottlingException`, `InternalFailure`, `ServiceUnavailable`, `InternalServiceError`; (b) permanent/IAM: `AccessDeniedException`, `NotAuthorized`, `InvalidClientTokenId`. The boto3 SDK raises `botocore.exceptions.ClientError` with error code accessible via `error.response['Error']['Code']`. Source: https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/CommonErrors.html
- **(Round 9)** The SSE Lambda's event stream format uses `id:` fields in SSE frames, enabling `Last-Event-ID` based resumption per the SSE protocol specification. If the current SSE Lambda does not emit `id:` fields, FR-081's server-side support requires adding them. The `id:` value must be monotonically increasing or otherwise orderable to support "resume from position" semantics.
- **(Round 9)** The `ReadableStreamDefaultReader.closed` Promise is supported in all modern browsers (Chrome 78+, Firefox 65+, Safari 10.1+) and is the canonical mechanism for detecting stream close type. Source: MDN Web Docs — ReadableStreamDefaultReader.closed.
- ~~**(Round 5)** The OTel `BatchSpanProcessor`'s default batch timeout is 200ms.~~ **CORRECTED in Round 9 (FR-084)**: The actual default `schedule_delay_millis` is 5000ms (5 seconds). FR-073 overrides to 1000ms. The original 200ms figure was incorrect. Source: OTel Python SDK v1.39.1 source code; OTel Specification, BatchSpanProcessor defaults.
- **(Round 10)** AWS is rolling out a new Lambda X-Ray segment format that eliminates the Invocation subsegment and uses `aws.responseLatency`/`aws.responseDuration` annotations on the function segment. Customer subsegments attach directly to `AWS::Lambda::Function` segment instead of the Invocation subsegment. OTel span parenting via `AwsXRayLambdaPropagator` is format-independent because it reads `_X_AMZN_TRACE_ID` which provides trace context regardless of segment format. Powertools Tracer subsegments may attach to a different parent under the new format, but this affects only handler-phase cosmetics in the X-Ray console — not trace correctness. No spec changes required; this assumption documents awareness. Source: https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html
- **(Round 10)** The SSE Lambda MUST NOT be deployed in a VPC when using Function URL response streaming. AWS docs confirm: "Function URLs do not support response streaming in VPC environments, but invoking functions using the InvokeWithResponseStream API is supported." If deployed in a VPC, responses are silently buffered instead of streamed — no error, no warning, just degraded behavior. The current architecture uses DynamoDB (public endpoint via AWS SDK), so VPC is not needed. If VPC access is required in the future (e.g., VPC-only RDS or ElastiCache), the SSE Lambda must either (a) remain non-VPC and access VPC resources via VPC endpoints or PrivateLink, or (b) be invoked via SDK `InvokeWithResponseStream` from a non-VPC intermediary Lambda with a Function URL. Source: https://docs.aws.amazon.com/lambda/latest/dg/configuration-response-streaming.html
- **(Round 10)** The project's span-per-poll-cycle convention for SSE streaming instrumentation (FR-025/FR-026: one parent span per DynamoDB poll cycle, child spans for db query + event dispatch + metric emit) is project-specific and NOT aligned with any OTel semantic convention — because none exists. The OTel specification's section on streaming instrumentation for GenAI spans explicitly says "TODO" (no span-per-event vs span-per-connection convention defined). When OTel publishes streaming semantic conventions, the project SHOULD evaluate alignment and migrate if feasible. Until then, the current convention is the best-available approach. Source: https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/
- **(Round 10)** AWS Lambda response streaming supports only Node.js managed runtimes and custom runtimes. The Python managed runtime (`python3.x`) does NOT support response streaming — `InvokeMode: RESPONSE_STREAM` returns buffered responses identical to `BUFFERED` mode. The SSE Lambda uses `python:3.13-slim` with a custom `bootstrap` script, which IS a custom runtime. This is the architectural reason the SSE Lambda requires container-based deployment with a custom bootstrap (not just for ADOT embedding). Source: https://docs.aws.amazon.com/lambda/latest/dg/configuration-response-streaming.html — "Response streaming currently supports the Node.js managed runtimes and custom runtimes."
- **(Round 8 AMENDED in Round 10)** ~~FR-073's `max_queue_size=256` provides sufficient headroom for the SSE Lambda's span throughput.~~ **CORRECTED in Round 10 (FR-086)**: The original `max_queue_size=512` (updated from 256 in an earlier round) is INSUFFICIENT. Recalculation: 15s stream × 15 polls/s × 3 spans/poll = ~675 spans/invocation. FR-086 amends to `max_queue_size=1500` (>= 2x expected throughput). Source: OTel Python SDK — `BatchSpanProcessor.on_end()` silent drop on queue full.

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: For any sampled user request, an operator can trace it from browser through API Gateway through Lambda through downstream services (DynamoDB, SNS, SendGrid) as a single continuous trace — with zero tool-switching required. Dev/preprod environments sample at 100%; production sampling is configured to balance cost with debuggability.
- **SC-002**: An operator can filter traces by error/fault status and immediately identify which specific operation failed (circuit breaker, audit trail, notification publish, fanout, self-healing) — without grepping logs.
- **SC-003**: 100% of Lambda functions (6 of 6) have auto-instrumented service call subsegments, up from current 5 of 6 (Metrics Lambda missing).
- **SC-004**: 100% of silent failure paths have X-Ray subsegments marked as error when the failure occurs. The 7 paths are: (1) circuit breaker load, (2) circuit breaker save, (3) audit trail persistence, (4) downstream notification SNS publish, (5) time-series fanout partial batch write, (6) self-healing item fetch, (7) parallel fetcher error aggregation.
- **SC-005**: SSE streaming latency and cache performance data is queryable via trace annotations — operators can filter traces by latency thresholds or cache hit rate directly, without separate log query tools.
- **SC-006**: The custom correlation ID system (`get_correlation_id`, `generate_correlation_id`) is fully removed, with zero remaining call sites.
- **SC-007**: The X-Ray canary detects simulated tracing ingestion failure within 2 consecutive canary intervals and fires a non-X-Ray alarm.
- **SC-008**: Cross-Lambda traces through SNS (Ingestion -> Analysis) appear under a single trace ID with the SNS hop visible as a connecting segment, enabling cross-service debugging without log correlation.
- **SC-009**: The system contains exactly one non-X-Ray tracing mechanism: the X-Ray canary. All custom tracing logs and custom correlation IDs previously identified in audit section 9.1 are consolidated onto X-Ray. Standard operational logging and CloudWatch metrics remain unchanged.
- **SC-010** [Round 3 — Revised]: SSE streaming operations (DynamoDB polls, event dispatch, CloudWatch metrics) that occur AFTER the Lambda handler returns the generator are captured in traced spans that appear in X-Ray linked to the original invocation trace ID. Zero orphaned subsegments, zero `SegmentNotFoundException` errors, zero silently discarded segments from `LambdaContext.put_segment()`. The tracing mechanism has an independent lifecycle that survives handler return.
- **SC-011** (Round 2): SendGrid email API calls in the Notification Lambda appear as explicitly instrumented subsegments with HTTP status and duration, not as untraced gaps in the service map.
- **SC-012** (Round 2): All non-streaming Lambda functions use a single, consistent tracing approach. Exceptions raised inside traced functions are automatically captured as subsegment errors. Zero Lambdas use dual/conflicting patching mechanisms. The SSE Lambda uses a distinct approach necessitated by the RESPONSE_STREAM lifecycle constraint, but its traces appear in the same X-Ray console.
- **SC-013** (Round 3): An X-Ray Group with error/fault filter generates CloudWatch metrics for errored traces. Operators can set alarms on error rates derived directly from trace data.
- **SC-014** (Round 3): The X-Ray canary detects partial trace data loss (throttling, daemon failure) by tracking the ratio of submitted-to-retrieved test traces. When data loss exceeds the configured threshold, a dedicated CloudWatch alarm fires.
- **SC-015** (Round 3): The frontend SSE client uses `fetch()` + `ReadableStream` with automatic reconnection logic, propagating `X-Amzn-Trace-Id` headers on every connection — matching or exceeding the reliability of the replaced `EventSource` implementation.
- **SC-016** (Round 3): X-Ray billing alarms fire when monthly costs exceed $10, $25, or $50 thresholds, preventing cost surprises from sampling configuration changes or traffic growth.
- **SC-017** (Round 4): 6/6 Lambda functions have both CloudWatch error alarms and CloudWatch latency alarms configured. Zero Lambda functions exist without operational alerting.
- **SC-018** (Round 4): All custom metrics previously emitted without alarms (7 metrics from audit Section 4.2) now have CloudWatch alarms with documented thresholds. Zero metrics exist in "emitted but unalarmed" state.
- **SC-019** (Round 4): All 7 silent failure paths emit both X-Ray subsegments (for trace context, per SC-004) and CloudWatch metrics (for 100% alarm coverage). An error on any silent failure path triggers both a trace annotation and a CloudWatch alarm — no silent failure is invisible to operators regardless of X-Ray sampling configuration.
- **SC-020** (Round 4): SSE reconnection traces are queryable by `session_id` annotation, returning all traces for a logical SSE session across reconnections. Operators can follow the reconnection chain via `previous_trace_id` annotations.
- **SC-021** (Round 4): The canary detects both X-Ray ingestion failure AND CloudWatch metric emission pipeline failure within 2 consecutive canary intervals. Meta-observability failures are reported via an out-of-band channel independent of CloudWatch alarms.
- **SC-022** (Round 4): The CloudWatch dashboard alarm status widget displays ALL configured alarms. Operators viewing the dashboard see complete alarm status without hidden blind spots.
- **SC-023** (Round 5): OTel spans created during SSE streaming phase carry the SAME trace ID as the Lambda runtime's X-Ray facade segment. Zero disconnected traces from trace ID mismatch. Zero orphaned OTel spans from sampling decision mismatch. Handler-phase subsegments (Powertools) and streaming-phase spans (OTel/ADOT) appear as a unified trace in the X-Ray console.
- **SC-024** (Round 5): The SSE Lambda's streaming generator calls `force_flush()` before the execution environment freezes. Zero spans are lost between invocations due to unflushed `BatchSpanProcessor` buffers. Zero stale spans from previous invocations appear in current invocation traces.
- **SC-025** (Round 5): All X-Ray subsegment/span metadata payloads are bounded. Zero segment documents are rejected by the 64KB `PutTraceSegments` limit. Error messages are truncated to 2,048 characters. No HTTP response bodies are attached as metadata.
- **SC-026** (Round 6): On warm invocations, all OTel streaming-phase spans link to the CURRENT invocation's trace ID — not a stale trace ID from a previous invocation. Verified by triggering 2+ sequential invocations on the same warm execution environment and confirming each invocation's spans carry distinct, correct trace IDs. Zero trace ID carryover between invocations.
- **SC-027** (Round 7): The SSE Lambda's OTel `TracerProvider` is instantiated exactly once per execution environment (at INIT). On 10+ consecutive warm invocations, no new `BatchSpanProcessor` daemon threads are created and memory usage remains stable (no monotonic growth). Zero `TracerProvider` instances are created inside the handler function.
- **SC-028** (Round 7): Streaming-phase DynamoDB and CloudWatch spans appear as correctly-typed subsegments in the X-Ray service map — DynamoDB calls render as DynamoDB nodes, CloudWatch calls render as CloudWatch nodes. Zero "unknown remote" or generic nodes from streaming-phase operations. Verified by querying the X-Ray service map API after a traced SSE streaming invocation with at least one DynamoDB query and one CloudWatch `put_metric_data` call.
- **SC-029** (Round 8): The SSE Lambda's container image includes the ADOT collector binary at `/opt/extensions/collector` and the default collector configuration at `/opt/collector-config/config.yaml`. The ADOT Extension process starts during Lambda INIT, registers with the Extensions API, and accepts OTLP spans on `localhost:4318`. Verified by inspecting the container image layers and confirming the Extension appears in Lambda INIT logs.
- **SC-030** (Round 8): The SSE Lambda appears as a correctly-named node in the X-Ray service map (matching `POWERTOOLS_SERVICE_NAME`). Zero `unknown_service` nodes exist in the service map for any environment where the SSE Lambda has been invoked. Verified by querying the X-Ray service map API and confirming the SSE Lambda node name matches the configured `OTEL_SERVICE_NAME`.
- **SC-031** (Round 9): The ADOT Extension's collector pipeline includes the decouple processor. Verified by inspecting the collector configuration and confirming the pipeline order includes `decouple` as the last processor before the exporter.
- **SC-032** (Round 9): The SSE Lambda's `TracerProvider` is instantiated with `shutdown_on_exit=False`. Zero `RuntimeError` exceptions from `ConcurrentMultiSpanProcessor` shutdown race conditions on execution environment recycling. Verified by running 100+ invocations across multiple execution environment recycling events.
- **SC-033** (Round 9): Zero `BotocoreInstrumentor` imports or `.instrument()` calls exist in the SSE Lambda's codebase. Verified by code search.
- **SC-034** (Round 9): The X-Ray canary retries trace retrieval at least once with backoff (30s then 60s) before reporting trace loss. Verified by simulating X-Ray propagation delay >30s and confirming the canary does not false-alarm on the first query.
- **SC-035** (Round 9): The canary immediately escalates IAM/credential errors (`AccessDeniedException`, `NotAuthorized`) via out-of-band channel (FR-050) with zero retries. Transient errors (`ThrottlingException`, `InternalFailure`) are retried up to 3 times before escalation. Verified by simulating both error types and confirming differential handling.
- **SC-036** (Round 9): The SSE Lambda server either (a) checks `Last-Event-ID` header and resumes from that position, or (b) responds with `X-SSE-Resume-Supported: false` header. Zero scenarios exist where the client sends `Last-Event-ID` and the server silently ignores it.
- **SC-037** (Round 9): The SSE Lambda Function URL CORS configuration includes `x-amzn-trace-id` in `Access-Control-Expose-Headers`. Browser JavaScript can read the `X-Amzn-Trace-Id` response header for FR-048's `previous_trace_id` capture.
- **SC-038** (Round 9): The frontend SSE client reconnects with short delay (1-3s) on graceful server close and exponential backoff on network error. Zero instances of 30s+ reconnection delays after expected server-initiated closes (e.g., Lambda timeout).
- **SC-039** (Round 10): When an SSE Lambda streaming client disconnects mid-stream, OTel spans emitted after the disconnect carry `client.disconnected=true` annotation. Verified by disconnecting a client mid-stream and querying X-Ray for traces with the `client.disconnected` annotation. The span status is `OK` (not `ERROR`), confirming client disconnect is not misclassified as a server fault.
- **SC-040** (Round 10): The SSE Lambda's `BatchSpanProcessor` `max_queue_size` is >= 1,350 (2x expected span throughput of ~675 spans per 15-second invocation). Zero spans are silently dropped due to queue overflow during normal streaming operations (15 polls/second, 3 spans/poll). Verified by running a 15-second streaming session with `OTEL_LOG_LEVEL=WARNING` and confirming zero "Queue is full" log entries.
- **SC-041** (Round 10): The X-Ray canary reports partial traces (`IsPartial=true`) as degraded rather than successful. When a test trace has `IsPartial=true` after all retry attempts, the canary emits `canary.trace.partial` CloudWatch metric with value 1 and does NOT count the trace as a fully successful verification. Verified by simulating delayed ADOT span delivery and confirming the canary reports degraded status.
- **SC-042** (Round 10): The SSE Lambda emits `retry: 3000\n` as the first field in each SSE connection. The frontend client parses this value and uses 3000ms as the reconnection delay on graceful server close. Verified by inspecting SSE stream output and measuring client reconnection timing after Lambda timeout.
- **SC-043** (Round 10): The SSE Lambda is deployed WITHOUT VPC configuration — no `vpc_config` block in the Terraform `aws_lambda_function` resource. Verified by inspecting Terraform configuration and confirming zero `subnet_ids` or `security_group_ids` on the SSE Lambda.
- **SC-044** (Round 10): The custom ADOT collector YAML explicitly includes `decouple` in the `service.pipelines.traces.processors` list, in the order `[decouple, batch]`. Verified by inspecting the collector configuration file in the container image at `/opt/collector-config/config.yaml`.
